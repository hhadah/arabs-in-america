{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# !pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_urls = [\n",
    "\"https://economics.harvard.edu/faculty\",\n",
    "\"https://economics.mit.edu/people/faculty\",\n",
    "\"https://econ.berkeley.edu/people/faculty\",\n",
    "\"https://economics.uchicago.edu/people/faculty\",\n",
    "\"https://economics.princeton.edu/people/\",\n",
    "\"https://economics.stanford.edu/people/faculty\",\n",
    "\"https://economics.yale.edu/people\",\n",
    "\"https://econ.columbia.edu/faculty/\",\n",
    "\"https://as.nyu.edu/departments/econ/faculty.html\",\n",
    "\"https://economics.sas.upenn.edu/people/faculty\",\n",
    "\"https://economics.brown.edu/people/faculty\",\n",
    "\"https://www.bu.edu/econ/people/faculty/\",\n",
    "\"https://dornsife.usc.edu/econ/faculty/\",\n",
    "\"https://lsa.umich.edu/econ/people/faculty.html\",\n",
    "\"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\",\n",
    "\"https://economics.dartmouth.edu/people\",\n",
    "\"https://economics.northwestern.edu/people/faculty/\",\n",
    "\"https://economics.ucla.edu/faculty/ladder\",\n",
    "\"https://business.columbia.edu/faculty/divisions/finance/faculty\",\n",
    "\"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\",\n",
    "\"https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=\",\n",
    "\"https://economics.cornell.edu/faculty\",\n",
    "\"https://econ.wisc.edu/faculty/\",\n",
    "\"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\",\n",
    "\"https://economics.ucdavis.edu/people/faculty\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print names in a formatted way\n",
    "def print_faculty_names(names):\n",
    "    print(f\"Total faculty members found: {len(names)}\")\n",
    "    print(\"\\nFaculty Names:\")\n",
    "    for i, name in enumerate(names, 1):\n",
    "        print(f\"{i}. {name}\")\n",
    "\n",
    "# Extract faculty names\n",
    "faculty_names = extract_faculty_names(html_content)\n",
    "print_faculty_names(faculty_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping faculty from: https://economics.harvard.edu/faculty\n",
      "Successfully scraped 64 faculty names from economics.harvard.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.mit.edu/people/faculty\n",
      "Successfully scraped 52 faculty names from economics.mit.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.berkeley.edu/people/faculty\n",
      "UC Berkeley - Found 53 faculty names.\n",
      "Successfully scraped 53 faculty names from econ.berkeley.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.uchicago.edu/people/faculty\n",
      "Successfully scraped 34 faculty names from economics.uchicago.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.princeton.edu/people/\n",
      "Successfully scraped 82 faculty names from economics.princeton.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.stanford.edu/people/faculty\n",
      "Successfully scraped 39 faculty names from economics.stanford.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.yale.edu/people\n",
      "Successfully scraped 49 faculty names from economics.yale.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.columbia.edu/faculty/\n",
      "Successfully scraped 84 faculty names from econ.columbia.edu.\n",
      "\n",
      "Scraping faculty from: https://as.nyu.edu/departments/econ/faculty.html\n",
      "Successfully scraped 50 faculty names from as.nyu.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.sas.upenn.edu/people/faculty\n",
      "Successfully scraped 34 faculty names from economics.sas.upenn.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.brown.edu/people/faculty\n",
      "Successfully scraped 39 faculty names from economics.brown.edu.\n",
      "\n",
      "Scraping faculty from: https://www.bu.edu/econ/people/faculty/\n",
      "Successfully scraped 41 faculty names from bu.edu.\n",
      "\n",
      "Scraping faculty from: https://dornsife.usc.edu/econ/faculty/\n",
      "Scraping page 1: https://dornsife.usc.edu/econ/faculty/\n",
      "Found 10 total faculty names after page 1\n",
      "Scraping page 2: https://dornsife.usc.edu/econ/faculty/page/2/\n",
      "Found 18 total faculty names after page 2\n",
      "Scraping page 3: https://dornsife.usc.edu/econ/faculty/page/3/\n",
      "Found 25 total faculty names after page 3\n",
      "Scraping page 4: https://dornsife.usc.edu/econ/faculty/page/4/\n",
      "Found 34 total faculty names after page 4\n",
      "Scraping page 5: https://dornsife.usc.edu/econ/faculty/page/5/\n",
      "Found 40 total faculty names after page 5\n",
      "Scraping page 6: https://dornsife.usc.edu/econ/faculty/page/6/\n",
      "Found 45 total faculty names after page 6\n",
      "Scraping page 7: https://dornsife.usc.edu/econ/faculty/page/7/\n",
      "Found 51 total faculty names after page 7\n",
      "Scraping page 8: https://dornsife.usc.edu/econ/faculty/page/8/\n",
      "\n",
      "Found total of 51 faculty members across 7 pages\n",
      "Successfully scraped 51 faculty names from dornsife.usc.edu.\n",
      "\n",
      "Scraping faculty from: https://lsa.umich.edu/econ/people/faculty.html\n",
      "Michigan - Found 0 faculty names.\n",
      "Successfully scraped 0 faculty names from lsa.umich.edu.\n",
      "Warning: No faculty names found for lsa.umich.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\n",
      "UCSD - Found 44 faculty names.\n",
      "Successfully scraped 44 faculty names from economics.ucsd.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.dartmouth.edu/people\n",
      "Dartmouth - Found 63 faculty names.\n",
      "Successfully scraped 63 faculty names from economics.dartmouth.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.northwestern.edu/people/faculty/\n",
      "Northwestern - Found 52 faculty names.\n",
      "Successfully scraped 52 faculty names from economics.northwestern.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.ucla.edu/faculty/ladder\n",
      "UCLA - Found 44 faculty names.\n",
      "Successfully scraped 44 faculty names from economics.ucla.edu.\n",
      "\n",
      "Scraping faculty from: https://business.columbia.edu/faculty/divisions/finance/faculty\n",
      "Access to https://business.columbia.edu/faculty/divisions/finance/faculty is forbidden (403).\n",
      "Attempting to bypass with additional headers or methods...\n",
      "Successfully scraped 0 faculty names from business.columbia.edu.\n",
      "Warning: No faculty names found for business.columbia.edu.\n",
      "\n",
      "Scraping faculty from: https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\n",
      "Successfully scraped 0 faculty names from bc.edu.\n",
      "Warning: No faculty names found for bc.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=\n",
      "Scraping page: https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=\n",
      "Found faculty: Jeff Ackermann - assistant professor of instruction\n",
      "Found faculty: Christian Ahlin - professor\n",
      "Found faculty: Soren Anderson - associate professor\n",
      "Found faculty: Luis Araujo - professor\n",
      "Found faculty: Richard Baillie - a.j. pasant professor of economics\n",
      "Found faculty: Prabhat Barnwal - associate professor\n",
      "Found faculty: Janice Beecher - adjunct professor\n",
      "Found faculty: Hannah Bolder - adjunct assistant professor\n",
      "Scraping page: https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=&page=2&letter=\n",
      "Found faculty: Jeff Ackermann - assistant professor of instruction\n",
      "Found faculty: Christian Ahlin - professor\n",
      "Found faculty: Soren Anderson - associate professor\n",
      "Found faculty: Luis Araujo - professor\n",
      "Found faculty: Richard Baillie - a.j. pasant professor of economics\n",
      "Found faculty: Prabhat Barnwal - associate professor\n",
      "Found faculty: Janice Beecher - adjunct professor\n",
      "Found faculty: Hannah Bolder - adjunct assistant professor\n",
      "Scraping page: https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=&page=3&letter=\n",
      "Found faculty: Jeff Ackermann - assistant professor of instruction\n",
      "Found faculty: Christian Ahlin - professor\n",
      "Found faculty: Soren Anderson - associate professor\n",
      "Found faculty: Luis Araujo - professor\n",
      "Found faculty: Richard Baillie - a.j. pasant professor of economics\n",
      "Found faculty: Prabhat Barnwal - associate professor\n",
      "Found faculty: Janice Beecher - adjunct professor\n",
      "Found faculty: Hannah Bolder - adjunct assistant professor\n",
      "Scraping page: https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=&page=4&letter=\n",
      "Found faculty: Jeff Ackermann - assistant professor of instruction\n",
      "Found faculty: Christian Ahlin - professor\n",
      "Found faculty: Soren Anderson - associate professor\n",
      "Found faculty: Luis Araujo - professor\n",
      "Found faculty: Richard Baillie - a.j. pasant professor of economics\n",
      "Found faculty: Prabhat Barnwal - associate professor\n",
      "Found faculty: Janice Beecher - adjunct professor\n",
      "Found faculty: Hannah Bolder - adjunct assistant professor\n",
      "MSU - Found 32 faculty names.\n",
      "Successfully scraped 32 faculty names from econ.msu.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.cornell.edu/faculty\n",
      "Found faculty: Amanda Agan - associate professor\n",
      "Found faculty: Sule Alan - professor\n",
      "Found faculty: Levon Barseghyan - robert julius thorne professor of economics\n",
      "Found faculty: Kaushik Basu - carl marks professor of international studies\n",
      "Found faculty: Marco Battaglini - edward h. meyer professor of economics\n",
      "Found faculty: Justin Bloesch - assistant professor\n",
      "Found faculty: Lawrence Blume - distinguished professor of arts and sciences in economics\n",
      "Found faculty: Nicolas Bottan - assistant professor\n",
      "Found faculty: George Boyer - martin p. catherwood professor of industrial and labor relations\n",
      "Found faculty: Colleen Carey - associate professor\n",
      "Found faculty: Julieta Caunedo - associate professor\n",
      "Found faculty: John Cawley - professor\n",
      "Found faculty: Ryan Chahrour - ernest s. liu professor of economics and international studies\n",
      "Found faculty: Stephen Coate - kiplinger professor of public policy\n",
      "Found faculty: Milena Djourelova - assistant professor\n",
      "Found faculty: David Easley - henry scarborough professor of social science\n",
      "Found faculty: Maria Fitzpatrick - professor\n",
      "Found faculty: Rick Geddes - professor founding director, cornell program in infrastructure policy\n",
      "Found faculty: Adam Harris - assistant professor\n",
      "Found faculty: George Jakubson - associate professor\n",
      "Found faculty: Ravi Kanbur - t.h. lee professor of world affairs\n",
      "Found faculty: Max Kapustin - assistant professor\n",
      "Found faculty: Andrew Karolyi - harold bierman, jr. distinguished professor of management\n",
      "Found faculty: Don Kenkel - andrew dickson white professor\n",
      "Found faculty: Philipp Kircher - irving m. ives professor of industrial and labor relations\n",
      "Found faculty: Rohit Lamba - assistant professor\n",
      "Found faculty: Pauline Leung - assistant professor\n",
      "Found faculty: Anran Li - assistant professor\n",
      "Found faculty: Jeremy Lise - professor\n",
      "Found faculty: Michael Lovenheim - professor\n",
      "Found faculty: Alan Mathios - professor\n",
      "Found faculty: Douglas Miller - professor\n",
      "Found faculty: Francesca Molinari - h.t. warshow and robert irving warshow professor of economics\n",
      "Found faculty: Jose Luis Montiel Olea - associate professor and louis salvatore ‘92 faculty leadership fellow\n",
      "Found faculty: Sean Nicholson - professor\n",
      "Found faculty: Kristoffer Nimark - associate professor\n",
      "Found faculty: Elio Nimier-David - assistant professor\n",
      "Found faculty: Ezra Oberfield - professor\n",
      "Found faculty: Eleonora Patacchini - stephen and barbara friedman professor of economics\n",
      "Found faculty: Zhuan Pei - associate professor\n",
      "Found faculty: Eswar Prasad - nandlal p. tolani senior professor of international trade policy\n",
      "Found faculty: Chen Qiu - assistant professor\n",
      "Found faculty: Evan Riehl - associate professor\n",
      "Found faculty: Nicholas Sanders - associate professor\n",
      "Found faculty: Seth Sanders - ronald g. ehrenberg professor of economics, the donald c. opatrny ’74 chair\n",
      "Found faculty: Robert Smith - professor\n",
      "Found faculty: Jason Sockin - assistant professor\n",
      "Found faculty: Mathieu Taschereau-Dumouchel - assistant professor robert jain faculty fellow\n",
      "Found faculty: Sharon Tennyson - professor\n",
      "Cornell - Found 49 faculty names.\n",
      "Successfully scraped 49 faculty names from economics.cornell.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.wisc.edu/faculty/\n",
      "Found faculty: Miguel Acosta\n",
      "Found faculty: Naoki Aizawa\n",
      "Found faculty: Simeon Alder\n",
      "Found faculty: Panle Barwick\n",
      "Found faculty: Benjamin Bernard\n",
      "Found faculty: Job Boerma\n",
      "Found faculty: John Brauer\n",
      "Found faculty: Yong Cai\n",
      "Found faculty: Matteo Camboni\n",
      "Found faculty: Stella Chan\n",
      "Found faculty: Harold Chiang\n",
      "Found faculty: Menzie Chinn\n",
      "Found faculty: Dean Corbae\n",
      "Found faculty: Louphou Coulibaly\n",
      "Found faculty: Lydia Cox\n",
      "Found faculty: Raymond Deneckere\n",
      "Found faculty: Charles Engel\n",
      "Found faculty: Gwen Eudey\n",
      "Found faculty: Fran Flanagan\n",
      "Found faculty: Matt Friedman\n",
      "Found faculty: Chao Fu\n",
      "Found faculty: Rebecca Glawtschew\n",
      "Found faculty: Jesse Gregory\n",
      "Found faculty: Agustin Gutierrez\n",
      "Found faculty: Bruce Hansen\n",
      "Found faculty: David Hansen\n",
      "Found faculty: Korinna Hansen\n",
      "Found faculty: Kenneth Hendricks\n",
      "Found faculty: Jean-Francois Houde\n",
      "Found faculty: David Johnson\n",
      "Found faculty: Karam Kang\n",
      "Found faculty: John Kennan\n",
      "Found faculty: Rishabh Kirpalani\n",
      "Found faculty: Rasmus Lentz\n",
      "Found faculty: Lorenzo Magnolfi\n",
      "Found faculty: Christopher McKelvey\n",
      "Found faculty: Corina Mommaerts\n",
      "Found faculty: Martin O'Connell\n",
      "Found faculty: Jack Porter\n",
      "Found faculty: Daniel Quint\n",
      "Found faculty: Steven Rick\n",
      "Found faculty: Fernanda Rojas-Ampuero\n",
      "Found faculty: Marzena Rostek\n",
      "Found faculty: Kim Ruhl\n",
      "Found faculty: Laura Schechter\n",
      "Found faculty: Ananth Seshadri\n",
      "Found faculty: Xiaoxia Shi\n",
      "Found faculty: Jeffrey Smith\n",
      "Found faculty: Lones Smith\n",
      "Found faculty: Alan Sorensen\n",
      "Found faculty: Christopher Sullivan\n",
      "Found faculty: Ashley Swanson\n",
      "Found faculty: Christopher Taber\n",
      "Found faculty: Steve Trost\n",
      "Found faculty: Marek Weretka\n",
      "Found faculty: Kenneth West\n",
      "Found faculty: Randall Wright\n",
      "Found faculty: Alice Wu\n",
      "Found faculty: Kohei Yata\n",
      "Wisconsin - Found 59 faculty names.\n",
      "Successfully scraped 59 faculty names from econ.wisc.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.duke.edu/people/other-faculty/regular-rank-faculty\n",
      "Duke - Found 0 faculty names.\n",
      "Successfully scraped 0 faculty names from econ.duke.edu.\n",
      "Warning: No faculty names found for econ.duke.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.ucdavis.edu/people/faculty\n",
      "Found faculty: Paul Bergin\n",
      "Found faculty: Marianne Bitler\n",
      "Found faculty: Giacomo Bonanno\n",
      "Found faculty: James Bushnell\n",
      "Found faculty: A. Colin Cameron\n",
      "Found faculty: Nicolas Caramp\n",
      "Found faculty: Andres Carvajal\n",
      "Found faculty: Anujit Chakraborty\n",
      "Found faculty: James Cloyne\n",
      "Found faculty: Katherine Eriksson\n",
      "Found faculty: Athanasios Geromichalos\n",
      "Found faculty: Xian Jiang\n",
      "Found faculty: Oscar Jorda\n",
      "Found faculty: Emile Marin\n",
      "Found faculty: Christopher Meissner\n",
      "Found faculty: Diana Moreira\n",
      "Found faculty: Erich Muehlegger\n",
      "Found faculty: Marianne Page\n",
      "Found faculty: Giovanni Peri\n",
      "Found faculty: Dave Rapson\n",
      "Found faculty: Arman Rezaee\n",
      "Found faculty: Katheryn Russ\n",
      "Found faculty: Burkhard Schipper\n",
      "Found faculty: Christoph Schlom\n",
      "Found faculty: Shu Shen\n",
      "Found faculty: Ina Simonovska\n",
      "Found faculty: Sanjay Singh\n",
      "Found faculty: Monica Singhal\n",
      "Found faculty: Jenna Stearns\n",
      "Found faculty: Derek Stimel\n",
      "Found faculty: Deborah Swenson\n",
      "Found faculty: Takuya Ura\n",
      "Found faculty: Janine Wilson\n",
      "UC Davis - Found 33 faculty names.\n",
      "Successfully scraped 33 faculty names from economics.ucdavis.edu.\n",
      "\n",
      "All Faculty Names Collected:\n",
      "                 University       Faculty Name\n",
      "0     economics.harvard.edu         Pol Antràs\n",
      "1     economics.harvard.edu       Mack Carroll\n",
      "2     economics.harvard.edu       Robert Barro\n",
      "3     economics.harvard.edu  Augustin Bergeron\n",
      "4     economics.harvard.edu        Emily Breza\n",
      "...                     ...                ...\n",
      "1043  economics.ucdavis.edu      Jenna Stearns\n",
      "1044  economics.ucdavis.edu       Derek Stimel\n",
      "1045  economics.ucdavis.edu    Deborah Swenson\n",
      "1046  economics.ucdavis.edu         Takuya Ura\n",
      "1047  economics.ucdavis.edu      Janine Wilson\n",
      "\n",
      "[1048 rows x 2 columns]\n",
      "\n",
      "Scraping Summary Report:\n",
      "                    University  \\\n",
      "0        economics.harvard.edu   \n",
      "1            economics.mit.edu   \n",
      "2            econ.berkeley.edu   \n",
      "3       economics.uchicago.edu   \n",
      "4      economics.princeton.edu   \n",
      "5       economics.stanford.edu   \n",
      "6           economics.yale.edu   \n",
      "7            econ.columbia.edu   \n",
      "8                   as.nyu.edu   \n",
      "9      economics.sas.upenn.edu   \n",
      "10         economics.brown.edu   \n",
      "11                      bu.edu   \n",
      "12            dornsife.usc.edu   \n",
      "13               lsa.umich.edu   \n",
      "14          economics.ucsd.edu   \n",
      "15     economics.dartmouth.edu   \n",
      "16  economics.northwestern.edu   \n",
      "17          economics.ucla.edu   \n",
      "18       business.columbia.edu   \n",
      "19                      bc.edu   \n",
      "20                econ.msu.edu   \n",
      "21       economics.cornell.edu   \n",
      "22               econ.wisc.edu   \n",
      "23               econ.duke.edu   \n",
      "24       economics.ucdavis.edu   \n",
      "\n",
      "                                                  URL          Status  \n",
      "0               https://economics.harvard.edu/faculty  64 names found  \n",
      "1            https://economics.mit.edu/people/faculty  52 names found  \n",
      "2            https://econ.berkeley.edu/people/faculty  53 names found  \n",
      "3       https://economics.uchicago.edu/people/faculty  34 names found  \n",
      "4             https://economics.princeton.edu/people/  82 names found  \n",
      "5       https://economics.stanford.edu/people/faculty  39 names found  \n",
      "6                   https://economics.yale.edu/people  49 names found  \n",
      "7                  https://econ.columbia.edu/faculty/  84 names found  \n",
      "8    https://as.nyu.edu/departments/econ/faculty.html  50 names found  \n",
      "9      https://economics.sas.upenn.edu/people/faculty  34 names found  \n",
      "10         https://economics.brown.edu/people/faculty  39 names found  \n",
      "11            https://www.bu.edu/econ/people/faculty/  41 names found  \n",
      "12             https://dornsife.usc.edu/econ/faculty/  51 names found  \n",
      "13     https://lsa.umich.edu/econ/people/faculty.html  No names found  \n",
      "14  https://economics.ucsd.edu/faculty-and-researc...  44 names found  \n",
      "15             https://economics.dartmouth.edu/people  63 names found  \n",
      "16  https://economics.northwestern.edu/people/facu...  52 names found  \n",
      "17          https://economics.ucla.edu/faculty/ladder  44 names found  \n",
      "18  https://business.columbia.edu/faculty/division...  No names found  \n",
      "19  https://www.bc.edu/content/bc-web/schools/morr...  No names found  \n",
      "20  https://econ.msu.edu/about/directory?employmen...  32 names found  \n",
      "21              https://economics.cornell.edu/faculty  49 names found  \n",
      "22                     https://econ.wisc.edu/faculty/  59 names found  \n",
      "23  https://econ.duke.edu/people/other-faculty/reg...  No names found  \n",
      "24       https://economics.ucdavis.edu/people/faculty  33 names found  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# List of faculty URLs\n",
    "faculty_urls = [\n",
    "    \"https://economics.harvard.edu/faculty\",\n",
    "    \"https://economics.mit.edu/people/faculty\",\n",
    "    \"https://econ.berkeley.edu/people/faculty\",\n",
    "    \"https://economics.uchicago.edu/people/faculty\",\n",
    "    \"https://economics.princeton.edu/people/\",\n",
    "    \"https://economics.stanford.edu/people/faculty\",\n",
    "    \"https://economics.yale.edu/people\",\n",
    "    \"https://econ.columbia.edu/faculty/\",\n",
    "    \"https://as.nyu.edu/departments/econ/faculty.html\",\n",
    "    \"https://economics.sas.upenn.edu/people/faculty\",\n",
    "    \"https://economics.brown.edu/people/faculty\",\n",
    "    \"https://www.bu.edu/econ/people/faculty/\",\n",
    "    \"https://dornsife.usc.edu/econ/faculty/\",\n",
    "    \"https://lsa.umich.edu/econ/people/faculty.html\",\n",
    "    \"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\",\n",
    "    \"https://economics.dartmouth.edu/people\",\n",
    "    \"https://economics.northwestern.edu/people/faculty/\",\n",
    "    \"https://economics.ucla.edu/faculty/ladder\",\n",
    "    \"https://business.columbia.edu/faculty/divisions/finance/faculty\",\n",
    "    \"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\",\n",
    "    \"https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=\",\n",
    "    \"https://economics.cornell.edu/faculty\",\n",
    "    \"https://econ.wisc.edu/faculty/\",\n",
    "    \"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\",\n",
    "    \"https://economics.ucdavis.edu/people/faculty\"\n",
    "]\n",
    "\n",
    "# Function to validate names\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, starting with uppercase letters\n",
    "    if re.match(r\"^[A-Z][a-zA-Z\\-\\'\\.]+\\s[A-Z][a-zA-Z\\-\\'\\.]+\", text):\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\n",
    "            \"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\",\n",
    "            \"Bureau\", \"Research\", \"Department\", \"Faculty\", \"Staff\",\n",
    "            \"Emeriti\", \"Adjunct\", \"Visiting\", \"Professor\", \"Lecturer\",\n",
    "            \"Assistant\", \"Associate\", \"Director\", \"Chair\", \"Affiliated\"\n",
    "        ]\n",
    "        if not any(keyword.lower() in text.lower() for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scraping functions for each university\n",
    "\n",
    "def scrape_harvard_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for widget_id in [\"widget-1\", \"widget-2\"]:\n",
    "            tab = soup.find(\"div\", id=widget_id)\n",
    "            if tab:\n",
    "                for name in tab.find_all(\"a\"):\n",
    "                    text = name.get_text(strip=True)\n",
    "                    if is_valid_name(text):\n",
    "                        faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_mit_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        container = soup.find(\"div\", class_=\"view-content faculty-landing__items\")\n",
    "        if container:\n",
    "            for name in container.find_all(\"h3\", class_=\"profile-teaser__name\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_berkeley_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Locate all faculty profiles\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"display-name\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UC Berkeley - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_uchicago_faculty(url):\n",
    "    # Same as before\n",
    "    faculty_names = []\n",
    "    page = 0\n",
    "    while True:\n",
    "        page_url = f\"{url}?title=all&name=all&page={page}\"\n",
    "        response = requests.get(page_url)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        rows = soup.select(\".views-row\")\n",
    "        if not rows:\n",
    "            break\n",
    "        for row in rows:\n",
    "            name_tag = row.select_one(\"h2.no-tags a\")\n",
    "            if name_tag:\n",
    "                name = name_tag.text.strip()\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "        page += 1\n",
    "        time.sleep(1)  # Respectful scraping\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_princeton_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all person divs\n",
    "        faculty_divs = soup.find_all('div', class_='person')\n",
    "        for faculty in faculty_divs:\n",
    "            name_tag = faculty.find('h3')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_stanford_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all faculty cards\n",
    "        faculty_cards = soup.find_all('div', class_='views-row')\n",
    "        for card in faculty_cards:\n",
    "            # Find the name element within each card\n",
    "            name_element = card.find('div', class_='views-field-title')\n",
    "            if name_element:\n",
    "                name_span = name_element.find('span', class_='field-content')\n",
    "                if name_span:\n",
    "                    name = name_span.find('a').get_text(strip=True)\n",
    "                    if is_valid_name(name):\n",
    "                        faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_yale_faculty(url):\n",
    "    response = requests.get(url + \"?person_type=2\")  # Add faculty filter parameter\n",
    "    faculty_names = []\n",
    "    page = 0\n",
    "    \n",
    "    while True:\n",
    "        page_url = f\"{url}?person_type=2&page={page}\"  # Add page parameter\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all person articles\n",
    "            faculty_articles = soup.find_all('article', class_='node-teaser--person')\n",
    "            \n",
    "            # If no more faculty articles found, break the loop\n",
    "            if not faculty_articles:\n",
    "                break\n",
    "                \n",
    "            for article in faculty_articles:\n",
    "                # Get the name from the heading\n",
    "                name_div = article.find('div', class_='node-teaser__heading')\n",
    "                if name_div:\n",
    "                    name_link = name_div.find('a')\n",
    "                    if name_link:\n",
    "                        name = name_link.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "            \n",
    "            # Look for \"Next\" link to determine if there are more pages\n",
    "            next_link = soup.find('li', class_='views-mini-pager__item--next')\n",
    "            if not next_link:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            time.sleep(1)  # Add a delay between requests to be polite\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_columbia_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty boxes\n",
    "        faculty_boxes = soup.find_all('div', class_='tshowcase-box')\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Get title/position to check if they're faculty\n",
    "            position_div = box.find('div', class_='tshowcase-single-position')\n",
    "            if position_div:\n",
    "                position = position_div.get_text(strip=True).lower()\n",
    "                \n",
    "                # Check if they're a professor (includes assistant, associate, full)\n",
    "                if 'professor' in position and not any(x in position for x in ['visiting', 'adjunct', 'emeritus']):\n",
    "                    # Get the name from the title element\n",
    "                    name_div = box.find('div', class_='tshowcase-box-title')\n",
    "                    if name_div:\n",
    "                        name = name_div.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "                            \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_nyu_faculty(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty containers in the undergraduate tab (main faculty)\n",
    "        faculty_boxes = soup.find_all('div', class_='book-container section')\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Get the role/title\n",
    "            author_div = box.find('div', class_='book-box__author')\n",
    "            if author_div:\n",
    "                role = author_div.get_text(strip=True).lower()\n",
    "                \n",
    "                # Filter for professors (including assistant/associate) but exclude clinical, visiting, emeritus\n",
    "                if ('professor' in role and \n",
    "                    'clinical' not in role and \n",
    "                    'visiting' not in role and \n",
    "                    'emeritus' not in role and\n",
    "                    'courtesy' not in role):\n",
    "                    \n",
    "                    # Get the faculty name\n",
    "                    name_div = box.find('h2', class_='book-box__title')\n",
    "                    if name_div:\n",
    "                        name = name_div.get_text(strip=True)\n",
    "                        # Remove \"Associated appointment\" prefix if present\n",
    "                        if \"Associated appointment\" in name:\n",
    "                            continue\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_penn_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty rows\n",
    "        faculty_rows = soup.find_all('li', class_='row')\n",
    "        \n",
    "        for row in faculty_rows:\n",
    "            # Get the name from h3 tag\n",
    "            name_tag = row.find('h3')\n",
    "            # Get the title/position from h4 tag\n",
    "            title_tag = row.find('h4')\n",
    "            \n",
    "            if name_tag and title_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                title = title_tag.get_text(strip=True).lower()\n",
    "                \n",
    "                # Exclude certain positions\n",
    "                excluded_titles = ['lecturer', 'adjunct', 'visiting', 'emeritus', 'scholar']\n",
    "                if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                    # Clean up the name by removing any additional text after the name\n",
    "                    name = name.split('(')[0].strip()  # Remove anything in parentheses\n",
    "                    if is_valid_name(name):\n",
    "                        faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_brown_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty list items in the main faculty section\n",
    "        # (Excluding the \"Affiliates\" section that comes after)\n",
    "        faculty_section = soup.find('div', class_='component_block_3999')\n",
    "        if faculty_section:\n",
    "            faculty_items = faculty_section.find_all('li', class_='people_item')\n",
    "            \n",
    "            for item in faculty_items:\n",
    "                # Get name and title\n",
    "                name_tag = item.find('h3', class_='people_item_name')\n",
    "                title_tag = item.find('div', class_='people_item_title')\n",
    "                \n",
    "                if name_tag and title_tag:\n",
    "                    name = name_tag.get_text(strip=True)\n",
    "                    title = title_tag.get_text(strip=True).lower()\n",
    "                    \n",
    "                    # Exclude non-faculty positions\n",
    "                    excluded_titles = ['visiting', 'adjunct', 'lecturer', 'affiliate', 'scholar', 'emeritus']\n",
    "                    if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                        # Clean up the name\n",
    "                        name = name.replace('\\n', ' ').strip()\n",
    "                        # Remove additional whitespace between words\n",
    "                        name = ' '.join(name.split())\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_bu_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the faculty list container\n",
    "        faculty_list = soup.find('ul', class_='profile-listing')\n",
    "        if faculty_list:\n",
    "            # Find all faculty items\n",
    "            faculty_items = faculty_list.find_all('li', class_='profile-item')\n",
    "            \n",
    "            for item in faculty_items:\n",
    "                # Get name and title\n",
    "                name_tag = item.find('h6', class_='profile-name')\n",
    "                title_tag = item.find('p', class_='profile-title')\n",
    "                \n",
    "                if name_tag and title_tag:\n",
    "                    name = name_tag.get_text(strip=True)\n",
    "                    title = title_tag.get_text(strip=True).lower()\n",
    "                    \n",
    "                    # Exclude non-faculty positions\n",
    "                    excluded_titles = ['lecturer', 'instructor', 'visiting', 'adjunct', 'emeritus']\n",
    "                    if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                        # Clean up any special characters or extra whitespace in name\n",
    "                        name = name.replace('\"', '').replace('\"', '')\n",
    "                        name = ' '.join(name.split())\n",
    "                        \n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_usc_faculty(url=\"https://dornsife.usc.edu/econ/faculty/\"):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    faculty_names = []\n",
    "    page = 1\n",
    "    more_pages = True\n",
    "    \n",
    "    while more_pages:\n",
    "        # Construct URL for current page\n",
    "        if page == 1:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = f\"{url}page/{page}/\"\n",
    "            \n",
    "        print(f\"Scraping page {page}: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find all the person cards on this page\n",
    "                faculty_cards = soup.find_all('div', class_='person-card')\n",
    "                \n",
    "                if not faculty_cards:  # If no faculty cards found, we've reached the end\n",
    "                    more_pages = False\n",
    "                    continue\n",
    "                \n",
    "                for card in faculty_cards:\n",
    "                    # Get name from the h3 within f--field f--cta-title div\n",
    "                    name_container = card.find('div', class_='f--field f--cta-title')\n",
    "                    if name_container:\n",
    "                        name_tag = name_container.find('h3')\n",
    "                        if name_tag and name_tag.find('a'):\n",
    "                            name = name_tag.find('a').get_text(strip=True)\n",
    "                            \n",
    "                            # Get title from the span with class person-title\n",
    "                            title_tag = card.find('span', class_='person-title')\n",
    "                            if title_tag:\n",
    "                                title = title_tag.get_text(strip=True).lower()\n",
    "                                \n",
    "                                # Filter faculty based on title\n",
    "                                excluded_titles = [\n",
    "                                    'lecturer',\n",
    "                                    'adjunct',\n",
    "                                    'visiting',\n",
    "                                    'emeritus',\n",
    "                                    'practice',\n",
    "                                    'teaching'\n",
    "                                ]\n",
    "                                \n",
    "                                # Include if contains 'professor' and not any excluded terms\n",
    "                                if ('professor' in title.lower() and \n",
    "                                    not any(exclude in title.lower() for exclude in excluded_titles)):\n",
    "                                    \n",
    "                                    # Clean up the name\n",
    "                                    name = ' '.join(name.split())\n",
    "                                    if is_valid_name(name) and name not in faculty_names:\n",
    "                                        faculty_names.append(name)\n",
    "                \n",
    "                print(f\"Found {len(faculty_names)} total faculty names after page {page}\")\n",
    "                \n",
    "                # Check if next page exists by looking for content\n",
    "                if not faculty_cards or len(faculty_cards) == 0:\n",
    "                    more_pages = False\n",
    "                else:\n",
    "                    page += 1\n",
    "                    \n",
    "            else:\n",
    "                # If we get a non-200 status code, we've likely reached the end\n",
    "                more_pages = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page}: {str(e)}\")\n",
    "            more_pages = False\n",
    "    \n",
    "    print(f\"\\nFound total of {len(faculty_names)} faculty members across {page-1} pages\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_michigan_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate the main div containing all faculty profiles\n",
    "        people_list = soup.find('div', class_='people row equalHeight clearfix', id='people-list')\n",
    "        if not people_list:\n",
    "            print(\"Could not find the people-list div.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        # Find all individual faculty containers\n",
    "        person_wraps = people_list.find_all('div', class_='person-wrap')\n",
    "        \n",
    "        for person in person_wraps:\n",
    "            info_div = person.find('div', class_='info')\n",
    "            if info_div:\n",
    "                name_h3 = info_div.find('h3', class_='name')\n",
    "                if name_h3:\n",
    "                    # Find the <a> tag with classes 'themeText themeLink' inside the <h3>\n",
    "                    name_a = name_h3.find('a', class_='themeText themeLink')\n",
    "                    if name_a and name_a.text:\n",
    "                        name = name_a.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"Michigan - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Michigan faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "# Function to validate names\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, starting with uppercase letters\n",
    "    if re.match(r\"^[A-Z][a-zA-Z\\-\\'\\.]+\\s[A-Z][a-zA-Z\\-\\'\\.]+\", text):\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\n",
    "            \"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\",\n",
    "            \"Bureau\", \"Research\", \"Department\", \"Faculty\", \"Staff\",\n",
    "            \"Emeriti\", \"Adjunct\", \"Visiting\", \"Professor\", \"Lecturer\",\n",
    "            \"Assistant\", \"Associate\", \"Director\", \"Chair\", \"Affiliated\"\n",
    "        ]\n",
    "        if not any(keyword.lower() in text.lower() for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def scrape_ucsd_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate the article containing the faculty listings\n",
    "        article = soup.find('article', class_='clearfix profile-listing')\n",
    "        if not article:\n",
    "            print(\"Could not find the faculty listing article.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        # Find all list items with class 'profile-listing-card'\n",
    "        profiles = article.find_all('li', class_='profile-listing-card')\n",
    "        \n",
    "        for profile in profiles:\n",
    "            # Locate the span containing the profile data\n",
    "            data_span = profile.find('span', class_='profile-listing-data')\n",
    "            if data_span:\n",
    "                # Find the h3 tag within the data span\n",
    "                h3_tag = data_span.find('h3')\n",
    "                if h3_tag:\n",
    "                    # Find the a tag within the h3 tag to get the faculty name\n",
    "                    a_tag = h3_tag.find('a')\n",
    "                    if a_tag and a_tag.text:\n",
    "                        name = a_tag.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UCSD - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve UCSD faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "\n",
    "def scrape_dartmouth_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate all <ul> elements with class 'people-list clearfix'\n",
    "        people_lists = soup.find_all('ul', class_='people-list clearfix')\n",
    "        if not people_lists:\n",
    "            print(\"Could not find any people-list elements.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        for people_list in people_lists:\n",
    "            # Find all <article> elements within the <ul>\n",
    "            articles = people_list.find_all('article', class_='node-person node-page-listing')\n",
    "            for article in articles:\n",
    "                # Find the <h3> tag within the <div class=\"content\">\n",
    "                content_div = article.find('div', class_='content')\n",
    "                if content_div:\n",
    "                    h3_tag = content_div.find('h3')\n",
    "                    if h3_tag:\n",
    "                        a_tag = h3_tag.find('a')\n",
    "                        if a_tag and a_tag.text:\n",
    "                            name = a_tag.get_text(strip=True)\n",
    "                            if is_valid_name(name):\n",
    "                                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"Dartmouth - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Dartmouth faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_northwestern_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate all <article> elements with class 'people'\n",
    "        articles = soup.find_all('article', class_='people')\n",
    "        if not articles:\n",
    "            print(\"Could not find any article elements with class 'people'.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        for article in articles:\n",
    "            # Find the <h3> tag within the <div class=\"people-content\">\n",
    "            people_content = article.find('div', class_='people-content')\n",
    "            if people_content:\n",
    "                h3_tag = people_content.find('h3')\n",
    "                if h3_tag:\n",
    "                    a_tag = h3_tag.find('a')\n",
    "                    if a_tag and a_tag.text:\n",
    "                        name = a_tag.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"Northwestern - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Northwestern faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucla_faculty(url):\n",
    "    \"\"\"\n",
    "    Scrapes faculty names from UCLA's Economics department page.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of UCLA's Economics faculty page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of faculty names.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate all <div> elements with class 'faculty-box'\n",
    "        faculty_boxes = soup.find_all('div', class_='faculty-box')\n",
    "        if not faculty_boxes:\n",
    "            print(\"Could not find any faculty-box elements.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Find the <h3> tag within the faculty-box\n",
    "            h3_tag = box.find('h3')\n",
    "            if h3_tag:\n",
    "                # Find all <a> tags within the <h3> tag\n",
    "                a_tags = h3_tag.find_all('a')\n",
    "                if a_tags:\n",
    "                    # Assume the second <a> tag contains the faculty name\n",
    "                    if len(a_tags) >= 2 and a_tags[1].text:\n",
    "                        name = a_tags[1].get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "                    elif len(a_tags) == 1 and a_tags[0].text:\n",
    "                        # Fallback in case there's only one <a> tag\n",
    "                        name = a_tags[0].get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UCLA - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve UCLA faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_columbiabusiness_faculty(url):\n",
    "    \"\"\"\n",
    "    Scrapes faculty names from Columbia Business School's Finance faculty page.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of Columbia Business School's Finance faculty page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of faculty names.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept\": (\n",
    "            \"text/html,application/xhtml+xml,application/xml;\"\n",
    "            \"q=0.9,image/webp,image/apng,*/*;q=0.8\"\n",
    "        ),\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error while accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    faculty_names = []\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Locate all <div> elements with class 'm-listing-faculty'\n",
    "        faculty_boxes = soup.find_all('div', class_='m-listing-faculty')\n",
    "        if not faculty_boxes:\n",
    "            print(\"Could not find any 'm-listing-faculty' elements.\")\n",
    "            return faculty_names\n",
    "\n",
    "        for box in faculty_boxes:\n",
    "            # Find the <h3> tag with class 'm-listing-faculty__title' within the faculty box\n",
    "            h3_tag = box.find('h3', class_='m-listing-faculty__title')\n",
    "            if h3_tag:\n",
    "                # Extract text from the <h3> tag, which contains the faculty name\n",
    "                # It may contain nested <a> tags, so get the text content\n",
    "                name = h3_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "\n",
    "        print(f\"Columbia Business School - Found {len(faculty_names)} faculty names.\")\n",
    "    elif response.status_code == 403:\n",
    "        print(f\"Access to {url} is forbidden (403).\")\n",
    "        print(\"Attempting to bypass with additional headers or methods...\")\n",
    "        # Optionally, implement further strategies here\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Columbia Business School faculty page. Status code: {response.status_code}\")\n",
    "\n",
    "    return faculty_names\n",
    "\n",
    "\n",
    "def scrape_bc_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h3', class_='person-card__name'):\n",
    "            name = person.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_msu_faculty(url):\n",
    "    faculty_names = []\n",
    "    \n",
    "    # Define URL pattern for each page\n",
    "    urls = [\n",
    "        f\"{url}\",  # Page 1\n",
    "        f\"{url}&page=2&letter=\",  # Page 2\n",
    "        f\"{url}&page=3&letter=\",  # Page 3\n",
    "        f\"{url}&page=4&letter=\"   # Page 4\n",
    "    ]\n",
    "    \n",
    "    for page_url in urls:\n",
    "        print(f\"Scraping page: {page_url}\")\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all faculty items on current page\n",
    "            faculty_items = soup.find_all('div', class_='faculty-item')\n",
    "            \n",
    "            for item in faculty_items:\n",
    "                # Get name and title\n",
    "                name_tag = item.find('h2', class_='line')\n",
    "                title_tag = item.find('p', class_='title')\n",
    "                \n",
    "                if name_tag and title_tag:\n",
    "                    name = name_tag.get_text(strip=True)\n",
    "                    title = title_tag.get_text(strip=True).lower()\n",
    "                    \n",
    "                    # Filter out non-faculty positions\n",
    "                    excluded_titles = ['student', 'emeritus', 'retired']\n",
    "                    if title and not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "                            print(f\"Found faculty: {name} - {title}\")\n",
    "        \n",
    "        time.sleep(1)  # Be polite to the server\n",
    "    \n",
    "    print(f\"MSU - Found {len(faculty_names)} faculty names.\")\n",
    "    return faculty_names\n",
    "\n",
    "\n",
    "def scrape_cornell_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty articles in the main content\n",
    "        faculty_articles = soup.find_all('article', class_='person--card')\n",
    "        \n",
    "        for article in faculty_articles:\n",
    "            # Get name and title\n",
    "            name_tag = article.find('h1', class_='person__name')\n",
    "            title_tag = article.find('span', class_='person__title')\n",
    "            \n",
    "            if name_tag and title_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                title = title_tag.get_text(strip=True).lower()\n",
    "                \n",
    "                # Filter out non-faculty positions\n",
    "                excluded_titles = ['visiting', 'adjunct', 'emeritus', 'lecturer', 'research associate']\n",
    "                if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                    if is_valid_name(name):\n",
    "                        faculty_names.append(name)\n",
    "                        print(f\"Found faculty: {name} - {title}\")\n",
    "    \n",
    "    print(f\"Cornell - Found {len(faculty_names)} faculty names.\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_wisconsin_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty divs\n",
    "        faculty_divs = soup.find_all('div', class_='faculty-member')\n",
    "        \n",
    "        for div in faculty_divs:\n",
    "            # Get name from h3 with class faculty-name\n",
    "            name_tag = div.find('h3', class_='faculty-name')\n",
    "            if name_tag:\n",
    "                # The name is inside an <a> tag\n",
    "                name_link = name_tag.find('a')\n",
    "                if name_link:\n",
    "                    name = name_link.get_text(strip=True)\n",
    "                    # Get position title to filter non-faculty\n",
    "                    position_tag = div.find('p', class_='position-title')\n",
    "                    if position_tag:\n",
    "                        title = position_tag.get_text(strip=True).lower()\n",
    "                        # Filter out non-faculty positions\n",
    "                        excluded_titles = ['emeritus', 'visiting', 'adjunct']\n",
    "                        if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                            if is_valid_name(name):\n",
    "                                faculty_names.append(name)\n",
    "                                print(f\"Found faculty: {name}\")\n",
    "    \n",
    "    print(f\"Wisconsin - Found {len(faculty_names)} faculty names.\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_duke_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty items in the grid\n",
    "        faculty_items = soup.find_all('div', class_='grid list-group-item')\n",
    "        \n",
    "        for item in faculty_items:\n",
    "            # Get name from h4 element within the article section\n",
    "            article = item.find('article')\n",
    "            if article:\n",
    "                name_div = article.find('div', class_='h4')\n",
    "                if name_div:\n",
    "                    name_link = name_div.find('a') \n",
    "                    if name_link:\n",
    "                        name = name_link.get_text(strip=True)\n",
    "                        \n",
    "                        # Get title to filter non-faculty\n",
    "                        title_div = article.find('div', class_='h6')\n",
    "                        if title_div:\n",
    "                            title = title_div.get_text(strip=True).lower()\n",
    "                            \n",
    "                            # Filter out non-faculty positions\n",
    "                            excluded_titles = ['emeritus', 'visiting', 'adjunct', 'lecturer', 'scholar']\n",
    "                            if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                                if is_valid_name(name):\n",
    "                                    faculty_names.append(name)\n",
    "                                    print(f\"Found faculty: {name}\")\n",
    "    \n",
    "    print(f\"Duke - Found {len(faculty_names)} faculty names.\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucdavis_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty articles\n",
    "        faculty_items = soup.find_all('article', class_='node--type-sf-person')\n",
    "        \n",
    "        for item in faculty_items:\n",
    "            # Get name from h3 within vm-teaser__title\n",
    "            name_tag = item.find('h3', class_='vm-teaser__title')\n",
    "            if name_tag:\n",
    "                name_link = name_tag.find('a')\n",
    "                if name_link:\n",
    "                    name = name_link.get_text(strip=True)\n",
    "                    # Get position to filter non-faculty\n",
    "                    position_list = item.find('ul', class_='vm-teaser__position')\n",
    "                    if position_list:\n",
    "                        position = position_list.get_text(strip=True).lower()\n",
    "                        # Filter out non-faculty positions\n",
    "                        excluded_titles = ['emeritus', 'visiting', 'adjunct', 'lecturer']\n",
    "                        if not any(exclude in position.lower() for exclude in excluded_titles):\n",
    "                            if is_valid_name(name):\n",
    "                                faculty_names.append(name)\n",
    "                                print(f\"Found faculty: {name}\")\n",
    "    \n",
    "    print(f\"UC Davis - Found {len(faculty_names)} faculty names.\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_generic_faculty(url):\n",
    "    # Generic scraping function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            text = link.get_text(strip=True)\n",
    "            if is_valid_name(text):\n",
    "                faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "# Mapping URLs to their respective scraping functions\n",
    "scraping_functions = {\n",
    "    \"https://economics.harvard.edu/faculty\": scrape_harvard_faculty,\n",
    "    \"https://economics.mit.edu/people/faculty\": scrape_mit_faculty,\n",
    "    \"https://econ.berkeley.edu/people/faculty\": scrape_berkeley_faculty,\n",
    "    \"https://economics.uchicago.edu/people/faculty\": scrape_uchicago_faculty,\n",
    "    \"https://economics.princeton.edu/people/\": scrape_princeton_faculty,\n",
    "    \"https://economics.stanford.edu/people/faculty\": scrape_stanford_faculty,\n",
    "    \"https://economics.yale.edu/people\": scrape_yale_faculty,\n",
    "    \"https://econ.columbia.edu/faculty/\": scrape_columbia_faculty,\n",
    "    \"https://as.nyu.edu/departments/econ/faculty.html\": scrape_nyu_faculty,\n",
    "    \"https://economics.sas.upenn.edu/people/faculty\": scrape_penn_faculty,\n",
    "    \"https://economics.brown.edu/people/faculty\": scrape_brown_faculty,\n",
    "    \"https://www.bu.edu/econ/people/faculty/\": scrape_bu_faculty,\n",
    "    \"https://dornsife.usc.edu/econ/faculty/\": scrape_usc_faculty,\n",
    "    \"https://lsa.umich.edu/econ/people/faculty.html\": scrape_michigan_faculty,\n",
    "    \"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\": scrape_ucsd_faculty,\n",
    "    \"https://economics.dartmouth.edu/people\": scrape_dartmouth_faculty,\n",
    "    \"https://economics.northwestern.edu/people/faculty/\": scrape_northwestern_faculty,\n",
    "    \"https://economics.ucla.edu/faculty/ladder\": scrape_ucla_faculty,\n",
    "    \"https://business.columbia.edu/faculty/divisions/finance/faculty\": scrape_columbiabusiness_faculty,\n",
    "    \"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\": scrape_bc_faculty,\n",
    "    \"https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=\": scrape_msu_faculty,\n",
    "    \"https://economics.cornell.edu/faculty\": scrape_cornell_faculty,\n",
    "    \"https://econ.wisc.edu/faculty/\": scrape_wisconsin_faculty,\n",
    "    \"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\": scrape_duke_faculty,\n",
    "    \"https://economics.ucdavis.edu/people/faculty\": scrape_ucdavis_faculty\n",
    "}\n",
    "\n",
    "# Collect all faculty names in a list\n",
    "all_faculty = []\n",
    "scraping_results = []\n",
    "\n",
    "for url in faculty_urls:\n",
    "    university_match = re.findall(r'https?://(?:www\\.)?([^/]+)/', url)\n",
    "    university_name = university_match[0].replace('www.', '') if university_match else 'Unknown'\n",
    "    print(f\"\\nScraping faculty from: {url}\")\n",
    "    try:\n",
    "        if url in scraping_functions:\n",
    "            scrape_function = scraping_functions[url]\n",
    "        else:\n",
    "            scrape_function = scrape_generic_faculty\n",
    "            print(\"Using generic scraping function.\")\n",
    "        faculty_names = scrape_function(url)\n",
    "        num_names = len(faculty_names)\n",
    "        print(f\"Successfully scraped {num_names} faculty names from {university_name}.\")\n",
    "        if num_names == 0:\n",
    "            print(f\"Warning: No faculty names found for {university_name}.\")\n",
    "            scraping_results.append({'University': university_name, 'URL': url, 'Status': 'No names found'})\n",
    "        else:\n",
    "            scraping_results.append({'University': university_name, 'URL': url, 'Status': f'{num_names} names found'})\n",
    "            for name in faculty_names:\n",
    "                all_faculty.append({'University': university_name, 'Faculty Name': name})\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {university_name}: {e}\")\n",
    "        scraping_results.append({'University': university_name, 'URL': url, 'Status': f'Error: {e}'})\n",
    "    time.sleep(1)  # Be polite to the servers\n",
    "\n",
    "# Create a DataFrame with all faculty names\n",
    "df_all_faculty = pd.DataFrame(all_faculty)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nAll Faculty Names Collected:\")\n",
    "print(df_all_faculty)\n",
    "\n",
    "# Display a summary report\n",
    "print(\"\\nScraping Summary Report:\")\n",
    "df_scraping_results = pd.DataFrame(scraping_results)\n",
    "print(df_scraping_results)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "# df_all_faculty.to_csv('faculty_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University</th>\n",
       "      <th>Faculty Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>economics.harvard.edu</td>\n",
       "      <td>Pol Antràs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>economics.harvard.edu</td>\n",
       "      <td>Mack Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>economics.harvard.edu</td>\n",
       "      <td>Robert Barro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>economics.harvard.edu</td>\n",
       "      <td>Augustin Bergeron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economics.harvard.edu</td>\n",
       "      <td>Emily Breza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>economics.ucdavis.edu</td>\n",
       "      <td>Jenna Stearns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>economics.ucdavis.edu</td>\n",
       "      <td>Derek Stimel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>economics.ucdavis.edu</td>\n",
       "      <td>Deborah Swenson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>economics.ucdavis.edu</td>\n",
       "      <td>Takuya Ura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>economics.ucdavis.edu</td>\n",
       "      <td>Janine Wilson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 University       Faculty Name\n",
       "0     economics.harvard.edu         Pol Antràs\n",
       "1     economics.harvard.edu       Mack Carroll\n",
       "2     economics.harvard.edu       Robert Barro\n",
       "3     economics.harvard.edu  Augustin Bergeron\n",
       "4     economics.harvard.edu        Emily Breza\n",
       "...                     ...                ...\n",
       "1043  economics.ucdavis.edu      Jenna Stearns\n",
       "1044  economics.ucdavis.edu       Derek Stimel\n",
       "1045  economics.ucdavis.edu    Deborah Swenson\n",
       "1046  economics.ucdavis.edu         Takuya Ura\n",
       "1047  economics.ucdavis.edu      Janine Wilson\n",
       "\n",
       "[1048 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_faculty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"/Users/hhadah/Documents/GiT/arabs-in-america/data/raw\"\n",
    "df_all_faculty.to_csv(f'{raw}/faculty_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/selenium/webdriver/common/driver_finder.py\u001b[0m in \u001b[0;36m_binary_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The path is not a valid file: {path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"driver_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The path is not a valid file: /Users/hhadah/Downloads/chromedriver_mac_arm64",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yk/l30p1rcd2fsfx74wfvs62cmh0000gn/T/ipykernel_96434/2518889558.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create the browser instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Open a webpage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mbrowser_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mvendor_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mfinder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDriverFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_browser_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_browser_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/selenium/webdriver/common/driver_finder.py\u001b[0m in \u001b[0;36mget_browser_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_browser_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_binary_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browser_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_driver_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/selenium/webdriver/common/driver_finder.py\u001b[0m in \u001b[0;36m_binary_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Unable to obtain driver for {browser}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDriverException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDriverException\u001b[0m: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your chromedriver\n",
    "chromedriver_path = '/Users/hhadah/Downloads/chromedriver_mac_arm64'\n",
    "\n",
    "# Create a Service object\n",
    "service = Service(executable_path=chromedriver_path)\n",
    "\n",
    "# Create the browser instance\n",
    "browser = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open a webpage\n",
    "browser.get('https://www.google.com')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
