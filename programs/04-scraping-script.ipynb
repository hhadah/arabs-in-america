{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# !pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_urls = [\n",
    "\"https://economics.harvard.edu/faculty\",\n",
    "\"https://economics.mit.edu/people/faculty\",\n",
    "\"https://econ.berkeley.edu/people/faculty\",\n",
    "\"https://economics.uchicago.edu/people/faculty\",\n",
    "\"https://economics.princeton.edu/people/\",\n",
    "\"https://economics.stanford.edu/people/faculty\",\n",
    "\"https://economics.yale.edu/people\",\n",
    "\"https://econ.columbia.edu/faculty/\",\n",
    "\"https://as.nyu.edu/departments/econ/faculty.html\",\n",
    "\"https://economics.sas.upenn.edu/people/faculty\",\n",
    "\"https://economics.brown.edu/people/faculty\",\n",
    "\"https://www.bu.edu/econ/people/faculty/\",\n",
    "\"https://dornsife.usc.edu/econ/faculty/\",\n",
    "\"https://lsa.umich.edu/econ/people/faculty.html\",\n",
    "\"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\",\n",
    "\"https://economics.dartmouth.edu/people\",\n",
    "\"https://economics.northwestern.edu/people/faculty/\",\n",
    "\"https://economics.ucla.edu/faculty/ladder\",\n",
    "\"https://business.columbia.edu/faculty/divisions/finance/faculty\",\n",
    "\"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\",\n",
    "\"https://econ.msu.edu/about/directory\",\n",
    "\"https://economics.cornell.edu/faculty\",\n",
    "\"https://econ.wisc.edu/faculty/\",\n",
    "\"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\",\n",
    "\"https://economics.ucdavis.edu/people/faculty\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the URL for Harvard's economics faculty page\n",
    "url = \"https://economics.harvard.edu/faculty\"\n",
    "\n",
    "def scrape_harvard_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Scrape the A to L tab content\n",
    "        tab_a_to_l = soup.find(\"div\", id=\"widget-1\")\n",
    "        if tab_a_to_l:\n",
    "            for name in tab_a_to_l.find_all(\"a\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                # Filter based on structure to capture only names\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "        \n",
    "        # Scrape the M to Z tab content\n",
    "        tab_m_to_z = soup.find(\"div\", id=\"widget-2\")\n",
    "        if tab_m_to_z:\n",
    "            for name in tab_m_to_z.find_all(\"a\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                # Filter based on structure to capture only names\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "        \n",
    "        print(f\"Harvard University - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, mostly capitalized\n",
    "    if re.match(r\"^[A-Z][a-z]+\\s[A-Z][a-z]+\", text):  # Simple check for capitalized first and last name\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\", \"Bureau\", \"Research\"]\n",
    "        if not any(keyword in text for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scrape the faculty names\n",
    "harvard_faculty = scrape_harvard_faculty(url)\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "unique_names = list(set(harvard_faculty))\n",
    "\n",
    "# Create a DataFrame with only the names\n",
    "df_harvard_faculty = pd.DataFrame(unique_names, columns=[\"Faculty Name\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_harvard_faculty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for MIT's economics faculty page\n",
    "url = \"https://economics.mit.edu/people/faculty\"\n",
    "\n",
    "def scrape_mit_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Target the specific container for faculty profiles\n",
    "        container = soup.find(\"div\", class_=\"view-content faculty-landing__items\")\n",
    "        if container:\n",
    "            # Find each name within <h3 class=\"profile-teaser__name\"> tags\n",
    "            for name in container.find_all(\"h3\", class_=\"profile-teaser__name\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                # Apply filter to ensure it's a valid name\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "        \n",
    "        print(f\"MIT - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Reuse the `is_valid_name` function from Harvard scraping script\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, mostly capitalized\n",
    "    if re.match(r\"^[A-Z][a-z]+\\s[A-Z][a-z]+\", text):  # Simple check for capitalized first and last name\n",
    "        excluded_keywords = [\"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\", \"Bureau\", \"Research\"]\n",
    "        if not any(keyword in text for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scrape the faculty names from MIT\n",
    "mit_faculty = scrape_mit_faculty(url)\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "unique_names = list(set(mit_faculty))\n",
    "\n",
    "# Create a DataFrame with only the names\n",
    "df_mit_faculty = pd.DataFrame(unique_names, columns=[\"Faculty Name\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_mit_faculty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for UC Berkeley's economics faculty page\n",
    "url = \"https://econ.berkeley.edu/people/faculty\"\n",
    "\n",
    "def scrape_berkeley_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Locate all faculty profiles\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"display-name\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UC Berkeley - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Scrape the faculty names from Berkeley\n",
    "berkeley_faculty = scrape_berkeley_faculty(url)\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "unique_names = list(set(berkeley_faculty))\n",
    "\n",
    "# Create a DataFrame with only the names\n",
    "df_berkeley_faculty = pd.DataFrame(unique_names, columns=[\"Faculty Name\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_berkeley_faculty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the University of Chicago's economics faculty directory\n",
    "base_url = \"https://economics.uchicago.edu/people/faculty\"\n",
    "faculty_names = []\n",
    "\n",
    "# Iterate through the pages\n",
    "page = 0\n",
    "while True:\n",
    "    # Modify the URL for each page\n",
    "    url = f\"{base_url}?title=all&name=all&page={page}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page}. Status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all faculty entries on the current page\n",
    "    rows = soup.select(\".views-row\")\n",
    "    if not rows:\n",
    "        print(\"No more faculty entries found.\")\n",
    "        break\n",
    "\n",
    "    # Extract faculty names from each entry\n",
    "    for row in rows:\n",
    "        name_tag = row.select_one(\"h2.no-tags a\")\n",
    "        if name_tag:\n",
    "            faculty_names.append(name_tag.text.strip())\n",
    "\n",
    "    # Go to the next page\n",
    "    page += 1\n",
    "\n",
    "# Convert the list of faculty names to a DataFrame\n",
    "df_faculty = pd.DataFrame(faculty_names, columns=[\"Faculty Name\"])\n",
    "print(f\"University of Chicago - Found {len(df_faculty)} faculty names.\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_faculty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print names in a formatted way\n",
    "def print_faculty_names(names):\n",
    "    print(f\"Total faculty members found: {len(names)}\")\n",
    "    print(\"\\nFaculty Names:\")\n",
    "    for i, name in enumerate(names, 1):\n",
    "        print(f\"{i}. {name}\")\n",
    "\n",
    "# Extract faculty names\n",
    "faculty_names = extract_faculty_names(html_content)\n",
    "print_faculty_names(faculty_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping faculty from: https://business.columbia.edu/faculty/divisions/finance/faculty\n",
      "Access to https://business.columbia.edu/faculty/divisions/finance/faculty is forbidden (403).\n",
      "Attempting to bypass with additional headers or methods...\n",
      "Successfully scraped 0 faculty names from business.columbia.edu.\n",
      "Warning: No faculty names found for business.columbia.edu.\n",
      "\n",
      "All Faculty Names Collected:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Scraping Summary Report:\n",
      "              University                                                URL  \\\n",
      "0  business.columbia.edu  https://business.columbia.edu/faculty/division...   \n",
      "\n",
      "           Status  \n",
      "0  No names found  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# List of faculty URLs\n",
    "faculty_urls = [\n",
    "    # \"https://economics.harvard.edu/faculty\",\n",
    "    # \"https://economics.mit.edu/people/faculty\",\n",
    "    # \"https://econ.berkeley.edu/people/faculty\",\n",
    "    # \"https://economics.uchicago.edu/people/faculty\",\n",
    "    # \"https://economics.princeton.edu/people/\",\n",
    "    # \"https://economics.stanford.edu/people/faculty\",\n",
    "    # \"https://economics.yale.edu/people\",\n",
    "    # \"https://econ.columbia.edu/faculty/\",\n",
    "    # \"https://as.nyu.edu/departments/econ/faculty.html\",\n",
    "    # \"https://economics.sas.upenn.edu/people/faculty\",\n",
    "    # \"https://economics.brown.edu/people/faculty\",\n",
    "    # \"https://www.bu.edu/econ/people/faculty/\",\n",
    "    # \"https://dornsife.usc.edu/econ/faculty/\",\n",
    "    # \"https://lsa.umich.edu/econ/people/faculty.html\",\n",
    "    # \"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\",\n",
    "    # \"https://economics.dartmouth.edu/people\",\n",
    "    # \"https://economics.northwestern.edu/people/faculty/\",\n",
    "    # \"https://economics.ucla.edu/faculty/ladder\",\n",
    "    # \"https://business.columbia.edu/faculty/divisions/finance/faculty\",\n",
    "    \"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\",\n",
    "    # \"https://econ.msu.edu/about/directory\",\n",
    "    # \"https://economics.cornell.edu/faculty\",\n",
    "    # \"https://econ.wisc.edu/faculty/\",\n",
    "    # \"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\",\n",
    "    # \"https://economics.ucdavis.edu/people/faculty\"\n",
    "]\n",
    "\n",
    "# Function to validate names\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, starting with uppercase letters\n",
    "    if re.match(r\"^[A-Z][a-zA-Z\\-\\'\\.]+\\s[A-Z][a-zA-Z\\-\\'\\.]+\", text):\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\n",
    "            \"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\",\n",
    "            \"Bureau\", \"Research\", \"Department\", \"Faculty\", \"Staff\",\n",
    "            \"Emeriti\", \"Adjunct\", \"Visiting\", \"Professor\", \"Lecturer\",\n",
    "            \"Assistant\", \"Associate\", \"Director\", \"Chair\", \"Affiliated\"\n",
    "        ]\n",
    "        if not any(keyword.lower() in text.lower() for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scraping functions for each university\n",
    "\n",
    "def scrape_harvard_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for widget_id in [\"widget-1\", \"widget-2\"]:\n",
    "            tab = soup.find(\"div\", id=widget_id)\n",
    "            if tab:\n",
    "                for name in tab.find_all(\"a\"):\n",
    "                    text = name.get_text(strip=True)\n",
    "                    if is_valid_name(text):\n",
    "                        faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_mit_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        container = soup.find(\"div\", class_=\"view-content faculty-landing__items\")\n",
    "        if container:\n",
    "            for name in container.find_all(\"h3\", class_=\"profile-teaser__name\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_berkeley_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Locate all faculty profiles\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"display-name\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UC Berkeley - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_uchicago_faculty(url):\n",
    "    # Same as before\n",
    "    faculty_names = []\n",
    "    page = 0\n",
    "    while True:\n",
    "        page_url = f\"{url}?title=all&name=all&page={page}\"\n",
    "        response = requests.get(page_url)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        rows = soup.select(\".views-row\")\n",
    "        if not rows:\n",
    "            break\n",
    "        for row in rows:\n",
    "            name_tag = row.select_one(\"h2.no-tags a\")\n",
    "            if name_tag:\n",
    "                name = name_tag.text.strip()\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "        page += 1\n",
    "        time.sleep(1)  # Respectful scraping\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_princeton_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all person divs\n",
    "        faculty_divs = soup.find_all('div', class_='person')\n",
    "        for faculty in faculty_divs:\n",
    "            name_tag = faculty.find('h3')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_stanford_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all faculty cards\n",
    "        faculty_cards = soup.find_all('div', class_='views-row')\n",
    "        for card in faculty_cards:\n",
    "            # Find the name element within each card\n",
    "            name_element = card.find('div', class_='views-field-title')\n",
    "            if name_element:\n",
    "                name_span = name_element.find('span', class_='field-content')\n",
    "                if name_span:\n",
    "                    name = name_span.find('a').get_text(strip=True)\n",
    "                    if is_valid_name(name):\n",
    "                        faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_yale_faculty(url):\n",
    "    response = requests.get(url + \"?person_type=2\")  # Add faculty filter parameter\n",
    "    faculty_names = []\n",
    "    page = 0\n",
    "    \n",
    "    while True:\n",
    "        page_url = f\"{url}?person_type=2&page={page}\"  # Add page parameter\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all person articles\n",
    "            faculty_articles = soup.find_all('article', class_='node-teaser--person')\n",
    "            \n",
    "            # If no more faculty articles found, break the loop\n",
    "            if not faculty_articles:\n",
    "                break\n",
    "                \n",
    "            for article in faculty_articles:\n",
    "                # Get the name from the heading\n",
    "                name_div = article.find('div', class_='node-teaser__heading')\n",
    "                if name_div:\n",
    "                    name_link = name_div.find('a')\n",
    "                    if name_link:\n",
    "                        name = name_link.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "            \n",
    "            # Look for \"Next\" link to determine if there are more pages\n",
    "            next_link = soup.find('li', class_='views-mini-pager__item--next')\n",
    "            if not next_link:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            time.sleep(1)  # Add a delay between requests to be polite\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_columbia_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty boxes\n",
    "        faculty_boxes = soup.find_all('div', class_='tshowcase-box')\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Get title/position to check if they're faculty\n",
    "            position_div = box.find('div', class_='tshowcase-single-position')\n",
    "            if position_div:\n",
    "                position = position_div.get_text(strip=True).lower()\n",
    "                \n",
    "                # Check if they're a professor (includes assistant, associate, full)\n",
    "                if 'professor' in position and not any(x in position for x in ['visiting', 'adjunct', 'emeritus']):\n",
    "                    # Get the name from the title element\n",
    "                    name_div = box.find('div', class_='tshowcase-box-title')\n",
    "                    if name_div:\n",
    "                        name = name_div.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "                            \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_nyu_faculty(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty containers in the undergraduate tab (main faculty)\n",
    "        faculty_boxes = soup.find_all('div', class_='book-container section')\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Get the role/title\n",
    "            author_div = box.find('div', class_='book-box__author')\n",
    "            if author_div:\n",
    "                role = author_div.get_text(strip=True).lower()\n",
    "                \n",
    "                # Filter for professors (including assistant/associate) but exclude clinical, visiting, emeritus\n",
    "                if ('professor' in role and \n",
    "                    'clinical' not in role and \n",
    "                    'visiting' not in role and \n",
    "                    'emeritus' not in role and\n",
    "                    'courtesy' not in role):\n",
    "                    \n",
    "                    # Get the faculty name\n",
    "                    name_div = box.find('h2', class_='book-box__title')\n",
    "                    if name_div:\n",
    "                        name = name_div.get_text(strip=True)\n",
    "                        # Remove \"Associated appointment\" prefix if present\n",
    "                        if \"Associated appointment\" in name:\n",
    "                            continue\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_penn_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty rows\n",
    "        faculty_rows = soup.find_all('li', class_='row')\n",
    "        \n",
    "        for row in faculty_rows:\n",
    "            # Get the name from h3 tag\n",
    "            name_tag = row.find('h3')\n",
    "            # Get the title/position from h4 tag\n",
    "            title_tag = row.find('h4')\n",
    "            \n",
    "            if name_tag and title_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                title = title_tag.get_text(strip=True).lower()\n",
    "                \n",
    "                # Exclude certain positions\n",
    "                excluded_titles = ['lecturer', 'adjunct', 'visiting', 'emeritus', 'scholar']\n",
    "                if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                    # Clean up the name by removing any additional text after the name\n",
    "                    name = name.split('(')[0].strip()  # Remove anything in parentheses\n",
    "                    if is_valid_name(name):\n",
    "                        faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_brown_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty list items in the main faculty section\n",
    "        # (Excluding the \"Affiliates\" section that comes after)\n",
    "        faculty_section = soup.find('div', class_='component_block_3999')\n",
    "        if faculty_section:\n",
    "            faculty_items = faculty_section.find_all('li', class_='people_item')\n",
    "            \n",
    "            for item in faculty_items:\n",
    "                # Get name and title\n",
    "                name_tag = item.find('h3', class_='people_item_name')\n",
    "                title_tag = item.find('div', class_='people_item_title')\n",
    "                \n",
    "                if name_tag and title_tag:\n",
    "                    name = name_tag.get_text(strip=True)\n",
    "                    title = title_tag.get_text(strip=True).lower()\n",
    "                    \n",
    "                    # Exclude non-faculty positions\n",
    "                    excluded_titles = ['visiting', 'adjunct', 'lecturer', 'affiliate', 'scholar', 'emeritus']\n",
    "                    if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                        # Clean up the name\n",
    "                        name = name.replace('\\n', ' ').strip()\n",
    "                        # Remove additional whitespace between words\n",
    "                        name = ' '.join(name.split())\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_bu_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the faculty list container\n",
    "        faculty_list = soup.find('ul', class_='profile-listing')\n",
    "        if faculty_list:\n",
    "            # Find all faculty items\n",
    "            faculty_items = faculty_list.find_all('li', class_='profile-item')\n",
    "            \n",
    "            for item in faculty_items:\n",
    "                # Get name and title\n",
    "                name_tag = item.find('h6', class_='profile-name')\n",
    "                title_tag = item.find('p', class_='profile-title')\n",
    "                \n",
    "                if name_tag and title_tag:\n",
    "                    name = name_tag.get_text(strip=True)\n",
    "                    title = title_tag.get_text(strip=True).lower()\n",
    "                    \n",
    "                    # Exclude non-faculty positions\n",
    "                    excluded_titles = ['lecturer', 'instructor', 'visiting', 'adjunct', 'emeritus']\n",
    "                    if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                        # Clean up any special characters or extra whitespace in name\n",
    "                        name = name.replace('\"', '').replace('\"', '')\n",
    "                        name = ' '.join(name.split())\n",
    "                        \n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_usc_faculty(base_url=\"https://dornsife.usc.edu/econ/faculty/\"):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    faculty_names = []\n",
    "    page = 1\n",
    "    more_pages = True\n",
    "    \n",
    "    while more_pages:\n",
    "        # Construct URL for current page\n",
    "        if page == 1:\n",
    "            current_url = base_url\n",
    "        else:\n",
    "            current_url = f\"{base_url}page/{page}/\"\n",
    "            \n",
    "        print(f\"Scraping page {page}: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find all the person cards on this page\n",
    "                faculty_cards = soup.find_all('div', class_='person-card')\n",
    "                \n",
    "                if not faculty_cards:  # If no faculty cards found, we've reached the end\n",
    "                    more_pages = False\n",
    "                    continue\n",
    "                \n",
    "                for card in faculty_cards:\n",
    "                    # Get name from the h3 within f--field f--cta-title div\n",
    "                    name_container = card.find('div', class_='f--field f--cta-title')\n",
    "                    if name_container:\n",
    "                        name_tag = name_container.find('h3')\n",
    "                        if name_tag and name_tag.find('a'):\n",
    "                            name = name_tag.find('a').get_text(strip=True)\n",
    "                            \n",
    "                            # Get title from the span with class person-title\n",
    "                            title_tag = card.find('span', class_='person-title')\n",
    "                            if title_tag:\n",
    "                                title = title_tag.get_text(strip=True).lower()\n",
    "                                \n",
    "                                # Filter faculty based on title\n",
    "                                excluded_titles = [\n",
    "                                    'lecturer',\n",
    "                                    'adjunct',\n",
    "                                    'visiting',\n",
    "                                    'emeritus',\n",
    "                                    'practice',\n",
    "                                    'teaching'\n",
    "                                ]\n",
    "                                \n",
    "                                # Include if contains 'professor' and not any excluded terms\n",
    "                                if ('professor' in title.lower() and \n",
    "                                    not any(exclude in title.lower() for exclude in excluded_titles)):\n",
    "                                    \n",
    "                                    # Clean up the name\n",
    "                                    name = ' '.join(name.split())\n",
    "                                    if is_valid_name(name) and name not in faculty_names:\n",
    "                                        faculty_names.append(name)\n",
    "                \n",
    "                print(f\"Found {len(faculty_names)} total faculty names after page {page}\")\n",
    "                \n",
    "                # Check if next page exists by looking for content\n",
    "                if not faculty_cards or len(faculty_cards) == 0:\n",
    "                    more_pages = False\n",
    "                else:\n",
    "                    page += 1\n",
    "                    \n",
    "            else:\n",
    "                # If we get a non-200 status code, we've likely reached the end\n",
    "                more_pages = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page}: {str(e)}\")\n",
    "            more_pages = False\n",
    "    \n",
    "    print(f\"\\nFound total of {len(faculty_names)} faculty members across {page-1} pages\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_michigan_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate the main div containing all faculty profiles\n",
    "        people_list = soup.find('div', class_='people row equalHeight clearfix', id='people-list')\n",
    "        if not people_list:\n",
    "            print(\"Could not find the people-list div.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        # Find all individual faculty containers\n",
    "        person_wraps = people_list.find_all('div', class_='person-wrap')\n",
    "        \n",
    "        for person in person_wraps:\n",
    "            info_div = person.find('div', class_='info')\n",
    "            if info_div:\n",
    "                name_h3 = info_div.find('h3', class_='name')\n",
    "                if name_h3:\n",
    "                    # Find the <a> tag with classes 'themeText themeLink' inside the <h3>\n",
    "                    name_a = name_h3.find('a', class_='themeText themeLink')\n",
    "                    if name_a and name_a.text:\n",
    "                        name = name_a.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"Michigan - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Michigan faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "# Function to validate names\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, starting with uppercase letters\n",
    "    if re.match(r\"^[A-Z][a-zA-Z\\-\\'\\.]+\\s[A-Z][a-zA-Z\\-\\'\\.]+\", text):\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\n",
    "            \"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\",\n",
    "            \"Bureau\", \"Research\", \"Department\", \"Faculty\", \"Staff\",\n",
    "            \"Emeriti\", \"Adjunct\", \"Visiting\", \"Professor\", \"Lecturer\",\n",
    "            \"Assistant\", \"Associate\", \"Director\", \"Chair\", \"Affiliated\"\n",
    "        ]\n",
    "        if not any(keyword.lower() in text.lower() for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def scrape_ucsd_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate the article containing the faculty listings\n",
    "        article = soup.find('article', class_='clearfix profile-listing')\n",
    "        if not article:\n",
    "            print(\"Could not find the faculty listing article.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        # Find all list items with class 'profile-listing-card'\n",
    "        profiles = article.find_all('li', class_='profile-listing-card')\n",
    "        \n",
    "        for profile in profiles:\n",
    "            # Locate the span containing the profile data\n",
    "            data_span = profile.find('span', class_='profile-listing-data')\n",
    "            if data_span:\n",
    "                # Find the h3 tag within the data span\n",
    "                h3_tag = data_span.find('h3')\n",
    "                if h3_tag:\n",
    "                    # Find the a tag within the h3 tag to get the faculty name\n",
    "                    a_tag = h3_tag.find('a')\n",
    "                    if a_tag and a_tag.text:\n",
    "                        name = a_tag.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UCSD - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve UCSD faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "\n",
    "def scrape_dartmouth_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate all <ul> elements with class 'people-list clearfix'\n",
    "        people_lists = soup.find_all('ul', class_='people-list clearfix')\n",
    "        if not people_lists:\n",
    "            print(\"Could not find any people-list elements.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        for people_list in people_lists:\n",
    "            # Find all <article> elements within the <ul>\n",
    "            articles = people_list.find_all('article', class_='node-person node-page-listing')\n",
    "            for article in articles:\n",
    "                # Find the <h3> tag within the <div class=\"content\">\n",
    "                content_div = article.find('div', class_='content')\n",
    "                if content_div:\n",
    "                    h3_tag = content_div.find('h3')\n",
    "                    if h3_tag:\n",
    "                        a_tag = h3_tag.find('a')\n",
    "                        if a_tag and a_tag.text:\n",
    "                            name = a_tag.get_text(strip=True)\n",
    "                            if is_valid_name(name):\n",
    "                                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"Dartmouth - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Dartmouth faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_northwestern_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate all <article> elements with class 'people'\n",
    "        articles = soup.find_all('article', class_='people')\n",
    "        if not articles:\n",
    "            print(\"Could not find any article elements with class 'people'.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        for article in articles:\n",
    "            # Find the <h3> tag within the <div class=\"people-content\">\n",
    "            people_content = article.find('div', class_='people-content')\n",
    "            if people_content:\n",
    "                h3_tag = people_content.find('h3')\n",
    "                if h3_tag:\n",
    "                    a_tag = h3_tag.find('a')\n",
    "                    if a_tag and a_tag.text:\n",
    "                        name = a_tag.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"Northwestern - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Northwestern faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucla_faculty(url):\n",
    "    \"\"\"\n",
    "    Scrapes faculty names from UCLA's Economics department page.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of UCLA's Economics faculty page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of faculty names.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate all <div> elements with class 'faculty-box'\n",
    "        faculty_boxes = soup.find_all('div', class_='faculty-box')\n",
    "        if not faculty_boxes:\n",
    "            print(\"Could not find any faculty-box elements.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Find the <h3> tag within the faculty-box\n",
    "            h3_tag = box.find('h3')\n",
    "            if h3_tag:\n",
    "                # Find all <a> tags within the <h3> tag\n",
    "                a_tags = h3_tag.find_all('a')\n",
    "                if a_tags:\n",
    "                    # Assume the second <a> tag contains the faculty name\n",
    "                    if len(a_tags) >= 2 and a_tags[1].text:\n",
    "                        name = a_tags[1].get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "                    elif len(a_tags) == 1 and a_tags[0].text:\n",
    "                        # Fallback in case there's only one <a> tag\n",
    "                        name = a_tags[0].get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UCLA - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve UCLA faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_columbiabusiness_faculty(url):\n",
    "    \"\"\"\n",
    "    Scrapes faculty names from Columbia Business School's Finance faculty page.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of Columbia Business School's Finance faculty page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of faculty names.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept\": (\n",
    "            \"text/html,application/xhtml+xml,application/xml;\"\n",
    "            \"q=0.9,image/webp,image/apng,*/*;q=0.8\"\n",
    "        ),\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error while accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    faculty_names = []\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Locate all <div> elements with class 'm-listing-faculty'\n",
    "        faculty_boxes = soup.find_all('div', class_='m-listing-faculty')\n",
    "        if not faculty_boxes:\n",
    "            print(\"Could not find any 'm-listing-faculty' elements.\")\n",
    "            return faculty_names\n",
    "\n",
    "        for box in faculty_boxes:\n",
    "            # Find the <h3> tag with class 'm-listing-faculty__title' within the faculty box\n",
    "            h3_tag = box.find('h3', class_='m-listing-faculty__title')\n",
    "            if h3_tag:\n",
    "                # Extract text from the <h3> tag, which contains the faculty name\n",
    "                # It may contain nested <a> tags, so get the text content\n",
    "                name = h3_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "\n",
    "        print(f\"Columbia Business School - Found {len(faculty_names)} faculty names.\")\n",
    "    elif response.status_code == 403:\n",
    "        print(f\"Access to {url} is forbidden (403).\")\n",
    "        print(\"Attempting to bypass with additional headers or methods...\")\n",
    "        # Optionally, implement further strategies here\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Columbia Business School faculty page. Status code: {response.status_code}\")\n",
    "\n",
    "    return faculty_names\n",
    "\n",
    "\n",
    "def scrape_bc_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h3', class_='person-card__name'):\n",
    "            name = person.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_msu_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h3', class_='staff__name'):\n",
    "            name = person.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_cornell_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"views-field-title\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_wisconsin_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for name_tag in soup.find_all(\"h3\", class_=\"person-name\"):\n",
    "            name = name_tag.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_duke_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"h4\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucdavis_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for tile in soup.find_all(\"div\", class_=\"tileItem\"):\n",
    "            name_tag = tile.find(\"h2\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_generic_faculty(url):\n",
    "    # Generic scraping function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            text = link.get_text(strip=True)\n",
    "            if is_valid_name(text):\n",
    "                faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "# Mapping URLs to their respective scraping functions\n",
    "scraping_functions = {\n",
    "    # \"https://economics.harvard.edu/faculty\": scrape_harvard_faculty,\n",
    "    # \"https://economics.mit.edu/people/faculty\": scrape_mit_faculty,\n",
    "    # \"https://econ.berkeley.edu/people/faculty\": scrape_berkeley_faculty,\n",
    "    # \"https://economics.uchicago.edu/people/faculty\": scrape_uchicago_faculty,\n",
    "    # \"https://economics.princeton.edu/people/\": scrape_princeton_faculty,\n",
    "    # \"https://economics.stanford.edu/people/faculty\": scrape_stanford_faculty,\n",
    "    # \"https://economics.yale.edu/people\": scrape_yale_faculty,\n",
    "    # \"https://econ.columbia.edu/faculty/\": scrape_columbia_faculty,\n",
    "    # \"https://as.nyu.edu/departments/econ/faculty.html\": scrape_nyu_faculty,\n",
    "    # \"https://economics.sas.upenn.edu/people/faculty\": scrape_penn_faculty,\n",
    "    # \"https://economics.brown.edu/people/faculty\": scrape_brown_faculty,\n",
    "    # \"https://www.bu.edu/econ/people/faculty/\": scrape_bu_faculty,\n",
    "    # \"https://dornsife.usc.edu/econ/faculty/\": scrape_usc_faculty,\n",
    "    # \"https://lsa.umich.edu/econ/people/faculty.html\": scrape_michigan_faculty,\n",
    "    # \"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\": scrape_ucsd_faculty,\n",
    "    # \"https://economics.dartmouth.edu/people\": scrape_dartmouth_faculty,\n",
    "    # \"https://economics.northwestern.edu/people/faculty/\": scrape_northwestern_faculty,\n",
    "    # \"https://economics.ucla.edu/faculty/ladder\": scrape_ucla_faculty,\n",
    "    # \"https://business.columbia.edu/faculty/divisions/finance/faculty\": scrape_columbiabusiness_faculty,\n",
    "    \"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\": scrape_bc_faculty,\n",
    "    # \"https://econ.msu.edu/about/directory\": scrape_msu_faculty,\n",
    "    # \"https://economics.cornell.edu/faculty\": scrape_cornell_faculty,\n",
    "    # \"https://econ.wisc.edu/faculty/\": scrape_wisconsin_faculty,\n",
    "    # \"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\": scrape_duke_faculty,\n",
    "    # \"https://economics.ucdavis.edu/people/faculty\": scrape_ucdavis_faculty\n",
    "}\n",
    "\n",
    "# Collect all faculty names in a list\n",
    "all_faculty = []\n",
    "scraping_results = []\n",
    "\n",
    "for url in faculty_urls:\n",
    "    university_match = re.findall(r'https?://(?:www\\.)?([^/]+)/', url)\n",
    "    university_name = university_match[0].replace('www.', '') if university_match else 'Unknown'\n",
    "    print(f\"\\nScraping faculty from: {url}\")\n",
    "    try:\n",
    "        if url in scraping_functions:\n",
    "            scrape_function = scraping_functions[url]\n",
    "        else:\n",
    "            scrape_function = scrape_generic_faculty\n",
    "            print(\"Using generic scraping function.\")\n",
    "        faculty_names = scrape_function(url)\n",
    "        num_names = len(faculty_names)\n",
    "        print(f\"Successfully scraped {num_names} faculty names from {university_name}.\")\n",
    "        if num_names == 0:\n",
    "            print(f\"Warning: No faculty names found for {university_name}.\")\n",
    "            scraping_results.append({'University': university_name, 'URL': url, 'Status': 'No names found'})\n",
    "        else:\n",
    "            scraping_results.append({'University': university_name, 'URL': url, 'Status': f'{num_names} names found'})\n",
    "            for name in faculty_names:\n",
    "                all_faculty.append({'University': university_name, 'Faculty Name': name})\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {university_name}: {e}\")\n",
    "        scraping_results.append({'University': university_name, 'URL': url, 'Status': f'Error: {e}'})\n",
    "    time.sleep(1)  # Be polite to the servers\n",
    "\n",
    "# Create a DataFrame with all faculty names\n",
    "df_all_faculty = pd.DataFrame(all_faculty)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nAll Faculty Names Collected:\")\n",
    "print(df_all_faculty)\n",
    "\n",
    "# Display a summary report\n",
    "print(\"\\nScraping Summary Report:\")\n",
    "df_scraping_results = pd.DataFrame(scraping_results)\n",
    "print(df_scraping_results)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "# df_all_faculty.to_csv('faculty_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_faculty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
