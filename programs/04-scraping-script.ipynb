{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting trio~=0.17\n",
      "  Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websocket-client~=1.8\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/anaconda3/lib/python3.9/site-packages (from selenium) (2022.9.24)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.11)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting typing_extensions~=4.9\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: idna in /usr/local/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Collecting attrs>=23.2.0\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting outcome\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting sniffio>=1.3.0\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/anaconda3/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: websocket-client, typing_extensions, sniffio, h11, exceptiongroup, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 0.58.0\n",
      "    Uninstalling websocket-client-0.58.0:\n",
      "      Successfully uninstalled websocket-client-0.58.0\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Uninstalling attrs-21.4.0:\n",
      "      Successfully uninstalled attrs-21.4.0\n",
      "Successfully installed attrs-24.2.0 exceptiongroup-1.2.2 h11-0.14.0 outcome-1.3.0.post0 selenium-4.26.1 sniffio-1.3.1 trio-0.27.0 trio-websocket-0.11.1 typing_extensions-4.12.2 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# !pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_urls = [\n",
    "\"https://economics.harvard.edu/faculty\",\n",
    "\"https://economics.mit.edu/people/faculty\",\n",
    "\"https://econ.berkeley.edu/people/faculty\",\n",
    "\"https://economics.uchicago.edu/people/faculty\",\n",
    "\"https://economics.princeton.edu/people/\",\n",
    "\"https://economics.stanford.edu/people/faculty\",\n",
    "\"https://economics.yale.edu/people\",\n",
    "\"https://econ.columbia.edu/faculty/\",\n",
    "\"https://as.nyu.edu/departments/econ/faculty.html\",\n",
    "\"https://economics.sas.upenn.edu/people/faculty\",\n",
    "\"https://economics.brown.edu/people/faculty\",\n",
    "\"https://www.bu.edu/econ/people/faculty/\",\n",
    "\"https://dornsife.usc.edu/econ/faculty/\",\n",
    "\"https://lsa.umich.edu/econ/people/faculty.html\",\n",
    "\"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\",\n",
    "\"https://economics.dartmouth.edu/people\",\n",
    "\"https://economics.northwestern.edu/people/faculty/\",\n",
    "\"https://economics.ucla.edu/faculty/ladder\",\n",
    "\"https://business.columbia.edu/faculty/divisions/finance/faculty\",\n",
    "\"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\",\n",
    "\"https://econ.msu.edu/about/directory\",\n",
    "\"https://economics.cornell.edu/faculty\",\n",
    "\"https://econ.wisc.edu/faculty/\",\n",
    "\"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\",\n",
    "\"https://economics.ucdavis.edu/people/faculty\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvard University - Found 58 faculty names.\n",
      "                        Faculty Name\n",
      "0                     Claudia Goldin\n",
      "1                         Shengwu Li\n",
      "2                      Benjamin Enke\n",
      "3                  Myrto Kalouptsidi\n",
      "4                  Gabriel Kreindler\n",
      "5                       Emily Palmer\n",
      "6                Stefanie Stantcheva\n",
      "7                       Jason Furman\n",
      "8                         Pol Antràs\n",
      "9                         Emily Sall\n",
      "10                      Melissa Dell\n",
      "11                        Elie Tamer\n",
      "12                    Eric Unverzagt\n",
      "13                    Kenneth Rogoff\n",
      "14                  Lawrence Summers\n",
      "15                 Augustin Bergeron\n",
      "16                    Marina Bisogno\n",
      "17                       James Stock\n",
      "18                       Amartya Sen\n",
      "19                      Jeremy Stein\n",
      "20                    Edward Glaeser\n",
      "21                 Christopher Foote\n",
      "22                  Personal Webpage\n",
      "23  Richard Freeman's NBER Home page\n",
      "24                      Jamie Murray\n",
      "25                       Oliver Hart\n",
      "26                       Marc Melitz\n",
      "27                   Eliza Rakaseder\n",
      "28                 Benjamin Friedman\n",
      "29                     Polina Barker\n",
      "30                       Jerry Green\n",
      "31                     Lawrence Katz\n",
      "32                    Amanda Pallais\n",
      "33                     Jesse Shapiro\n",
      "34                         Robin Lee\n",
      "35                 Tomasz Strzalecki\n",
      "36                       Karen Dynan\n",
      "37                     Oleg Itskhoki\n",
      "38                        David Yang\n",
      "39                     Jeffrey Miron\n",
      "40                     Neil Shephard\n",
      "41                     Ludwig Straub\n",
      "42            Gabriel Chodorow-Reich\n",
      "43                    Ursula Ferraro\n",
      "44                     David Laibson\n",
      "45                   Stephen Marglin\n",
      "46                     Matthew Rabin\n",
      "47                      Roland Fryer\n",
      "48                    Davide Viviano\n",
      "49                     Xavier Gabaix\n",
      "50                   Andrei Shleifer\n",
      "51                        Raj Chetty\n",
      "52                   Elhanan Helpman\n",
      "53                       Emily Breza\n",
      "54                      Robert Barro\n",
      "55                      Ann Richards\n",
      "56                      Mack Carroll\n",
      "57                       Ariel Pakes\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the URL for Harvard's economics faculty page\n",
    "url = \"https://economics.harvard.edu/faculty\"\n",
    "\n",
    "def scrape_harvard_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Scrape the A to L tab content\n",
    "        tab_a_to_l = soup.find(\"div\", id=\"widget-1\")\n",
    "        if tab_a_to_l:\n",
    "            for name in tab_a_to_l.find_all(\"a\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                # Filter based on structure to capture only names\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "        \n",
    "        # Scrape the M to Z tab content\n",
    "        tab_m_to_z = soup.find(\"div\", id=\"widget-2\")\n",
    "        if tab_m_to_z:\n",
    "            for name in tab_m_to_z.find_all(\"a\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                # Filter based on structure to capture only names\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "        \n",
    "        print(f\"Harvard University - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, mostly capitalized\n",
    "    if re.match(r\"^[A-Z][a-z]+\\s[A-Z][a-z]+\", text):  # Simple check for capitalized first and last name\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\", \"Bureau\", \"Research\"]\n",
    "        if not any(keyword in text for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scrape the faculty names\n",
    "harvard_faculty = scrape_harvard_faculty(url)\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "unique_names = list(set(harvard_faculty))\n",
    "\n",
    "# Create a DataFrame with only the names\n",
    "df_harvard_faculty = pd.DataFrame(unique_names, columns=[\"Faculty Name\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_harvard_faculty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIT - Found 46 faculty names.\n",
      "            Faculty Name\n",
      "0         Drew Fudenberg\n",
      "1       Abhijit Banerjee\n",
      "2        Jonathan Gruber\n",
      "3          Glenn Ellison\n",
      "4         Stephen Morris\n",
      "5           Esther Duflo\n",
      "6        Frank Schilbach\n",
      "7           Josh Angrist\n",
      "8           Parag Pathak\n",
      "9         Isaiah Andrews\n",
      "10   Richard Schmalensee\n",
      "11     Ricardo Caballero\n",
      "12         Martin Beraja\n",
      "13        Daron Acemoglu\n",
      "14         Drazen Prelec\n",
      "15        Jeffrey Harris\n",
      "16        Anna Mikusheva\n",
      "17    Alexander Wolitzky\n",
      "18           Tobias Salz\n",
      "19         Whitney Newey\n",
      "20        Benjamin Olken\n",
      "21   Victor Chernozhukov\n",
      "22         Peter Diamond\n",
      "23           Peter Temin\n",
      "24              Ian Ball\n",
      "25  Sendhil Mullainathan\n",
      "26        Dave Donaldson\n",
      "27       Stanley Fischer\n",
      "28        Nina Roussille\n",
      "29       Bengt Holmström\n",
      "30           David Atkin\n",
      "31   Sara Fisher Ellison\n",
      "32         Jacob Moscona\n",
      "33        Robert Gibbons\n",
      "34      Ashesh Rambachan\n",
      "35        Christian Wolf\n",
      "36         James Poterba\n",
      "37       Arnaud Costinot\n",
      "38      Michael Whinston\n",
      "39        Muhamet Yildiz\n",
      "40        Alberto Abadie\n",
      "41     Nathaniel Hendren\n",
      "42       Amy Finkelstein\n",
      "43         Jerry Hausman\n",
      "44        Nikhil Agarwal\n",
      "45     Olivier Blanchard\n"
     ]
    }
   ],
   "source": [
    "# Define the URL for MIT's economics faculty page\n",
    "url = \"https://economics.mit.edu/people/faculty\"\n",
    "\n",
    "def scrape_mit_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Target the specific container for faculty profiles\n",
    "        container = soup.find(\"div\", class_=\"view-content faculty-landing__items\")\n",
    "        if container:\n",
    "            # Find each name within <h3 class=\"profile-teaser__name\"> tags\n",
    "            for name in container.find_all(\"h3\", class_=\"profile-teaser__name\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                # Apply filter to ensure it's a valid name\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "        \n",
    "        print(f\"MIT - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Reuse the `is_valid_name` function from Harvard scraping script\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, mostly capitalized\n",
    "    if re.match(r\"^[A-Z][a-z]+\\s[A-Z][a-z]+\", text):  # Simple check for capitalized first and last name\n",
    "        excluded_keywords = [\"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\", \"Bureau\", \"Research\"]\n",
    "        if not any(keyword in text for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scrape the faculty names from MIT\n",
    "mit_faculty = scrape_mit_faculty(url)\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "unique_names = list(set(mit_faculty))\n",
    "\n",
    "# Create a DataFrame with only the names\n",
    "df_mit_faculty = pd.DataFrame(unique_names, columns=[\"Faculty Name\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_mit_faculty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC Berkeley - Found 53 faculty names.\n",
      "                  Faculty Name\n",
      "0   Valenzuela-Stookey, Quitzé\n",
      "1               Barahona, Nano\n",
      "2               Roland, Gérard\n",
      "3              Backus, Matthew\n",
      "4              Faber, Benjamin\n",
      "5               Miguel, Edward\n",
      "6             Rothstein, Jesse\n",
      "7               Kline, Patrick\n",
      "8               Goldberg, Lisa\n",
      "9              Munoz, Mathilde\n",
      "10           Auerbach, Alan J.\n",
      "11                Sraer, David\n",
      "12             Moretti, Enrico\n",
      "13          Schoefer, Benjamin\n",
      "14              Semenova, Vira\n",
      "15              Hoynes, Hilary\n",
      "16             Schaab, Andreas\n",
      "17        Walters, Christopher\n",
      "18                 Xu , Chenzi\n",
      "19            Caldwell, Sydnee\n",
      "20              Kariv, Shachar\n",
      "21              Shannon, Chris\n",
      "22             Zucman, Gabriel\n",
      "23                Walker, Reed\n",
      "24                  Kawai, Kei\n",
      "25            Jansson, Michael\n",
      "26       Hermalin, Benjamin E.\n",
      "27         DellaVigna, Stefano\n",
      "28                Ergin, Haluk\n",
      "29                Kolstad, Jon\n",
      "30                 Rao, Gautam\n",
      "31          Malmendier, Ulrike\n",
      "32            Graham, Bryan S.\n",
      "33            Finan, Frederico\n",
      "34             Shapiro, Joseph\n",
      "35               Nakamura, Emi\n",
      "36          DeLong, J.Bradford\n",
      "37          Eichengreen, Barry\n",
      "38  Gourinchas, Pierre-Olivier\n",
      "39                Edlin, Aaron\n",
      "40           Taubinsky, Dmitry\n",
      "41              Stein, Carolyn\n",
      "42             Gaubert, Cecile\n",
      "43               Kaur, Supreet\n",
      "44              Saez, Emmanuel\n",
      "45              Steinsson, Jón\n",
      "46                 Handel, Ben\n",
      "47         Echenique, Federico\n",
      "48     Rodríguez-Clare, Andrés\n",
      "49               Pouzo, Demian\n",
      "50                  Lian, Chen\n",
      "51                Yagan, Danny\n",
      "52        Gorodnichenko, Yuriy\n"
     ]
    }
   ],
   "source": [
    "# Define the URL for UC Berkeley's economics faculty page\n",
    "url = \"https://econ.berkeley.edu/people/faculty\"\n",
    "\n",
    "def scrape_berkeley_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Locate all faculty profiles\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"display-name\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UC Berkeley - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Scrape the faculty names from Berkeley\n",
    "berkeley_faculty = scrape_berkeley_faculty(url)\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "unique_names = list(set(berkeley_faculty))\n",
    "\n",
    "# Create a DataFrame with only the names\n",
    "df_berkeley_faculty = pd.DataFrame(unique_names, columns=[\"Faculty Name\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_berkeley_faculty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more faculty entries found.\n",
      "University of Chicago - Found 38 faculty names.\n",
      "              Faculty Name\n",
      "0             Ufuk Akcigit\n",
      "1        Fernando  Alvarez\n",
      "2        Stéphane Bonhomme\n",
      "3          Benjamin Brooks\n",
      "4          Christina Brown\n",
      "5       Leonardo  Bursztyn\n",
      "6        Manasi  Deshpande\n",
      "7           David Galenson\n",
      "8          Mikhail Golosov\n",
      "9       Michael Greenstone\n",
      "10       Lars Peter Hansen\n",
      "11     Arnold C. Harberger\n",
      "12       James J.  Heckman\n",
      "13            Ali Hortacsu\n",
      "14             Greg Kaplan\n",
      "15             Anne Karing\n",
      "16          Michael Kremer\n",
      "17         Thibaut Lamadon\n",
      "18            Steve Levitt\n",
      "19               John List\n",
      "20           Magne Mogstad\n",
      "21          Casey Mulligan\n",
      "22         Kevin M. Murphy\n",
      "23        Roger B. Myerson\n",
      "24          Derek A.  Neal\n",
      "25        Kirill Ponomarev\n",
      "26             Doron Ravid\n",
      "27          Philip J. Reny\n",
      "28            Eric Richert\n",
      "29             Joseph Root\n",
      "30               Evan Rose\n",
      "31  Esteban Rossi-Hansberg\n",
      "32            Azeem Shaikh\n",
      "33           Robert Shimer\n",
      "34         Nancy L. Stokey\n",
      "35       Max Tabord-Meehan\n",
      "36   Alexander Torgovitsky\n",
      "37            Harald Uhlig\n"
     ]
    }
   ],
   "source": [
    "# Base URL for the University of Chicago's economics faculty directory\n",
    "base_url = \"https://economics.uchicago.edu/people/faculty\"\n",
    "faculty_names = []\n",
    "\n",
    "# Iterate through the pages\n",
    "page = 0\n",
    "while True:\n",
    "    # Modify the URL for each page\n",
    "    url = f\"{base_url}?title=all&name=all&page={page}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page}. Status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all faculty entries on the current page\n",
    "    rows = soup.select(\".views-row\")\n",
    "    if not rows:\n",
    "        print(\"No more faculty entries found.\")\n",
    "        break\n",
    "\n",
    "    # Extract faculty names from each entry\n",
    "    for row in rows:\n",
    "        name_tag = row.select_one(\"h2.no-tags a\")\n",
    "        if name_tag:\n",
    "            faculty_names.append(name_tag.text.strip())\n",
    "\n",
    "    # Go to the next page\n",
    "    page += 1\n",
    "\n",
    "# Convert the list of faculty names to a DataFrame\n",
    "df_faculty = pd.DataFrame(faculty_names, columns=[\"Faculty Name\"])\n",
    "print(f\"University of Chicago - Found {len(df_faculty)} faculty names.\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_faculty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total faculty members found: 84\n",
      "\n",
      "Faculty Names:\n",
      "1. Alicia Adsera\n",
      "2. Mark A. Aguiar\n",
      "3. Yacine Ait-Sahalia\n",
      "4. Caio Almeida\n",
      "5. Roland Benabou\n",
      "6. Swati Bhatt\n",
      "7. Zachary Bleemer\n",
      "8. Alan S. Blinder\n",
      "9. Leah Platt Boustan\n",
      "10. Markus Brunnermeier\n",
      "11. Smita Brunnermeier\n",
      "12. Nicholas Buchholz\n",
      "13. Matias Cattaneo\n",
      "14. Sylvain Chassang\n",
      "15. Daniel Chen\n",
      "16. Natalie Cox\n",
      "17. Janet M. Currie\n",
      "18. Ellora Derenoncourt\n",
      "19. Pascaline Dupas\n",
      "20. Jianqing Fan\n",
      "21. Farid Farrokhi\n",
      "22. Mira Frick\n",
      "23. Thomas Fujiwara\n",
      "24. John R. Grigsby\n",
      "25. Gene M. Grossman\n",
      "26. Faruk R. Gul\n",
      "27. Kate Ho\n",
      "28. Bo E. Honoré\n",
      "29. Ryota Iijima\n",
      "30. Seema Jayachandran\n",
      "31. Adam Kapor\n",
      "32. Jakub Kastl\n",
      "33. Nobuhiro Kiyotaki\n",
      "34. Henrik J. Kleven\n",
      "35. Michal Kolesár\n",
      "36. Ilyana Kuziemko\n",
      "37. David Lee\n",
      "38. Moritz Lenel\n",
      "39. Thomas C. Leonard\n",
      "40. Ernest Liu\n",
      "41. Alessandro Lizzeri\n",
      "42. W. Bentley MacLeod\n",
      "43. Atif Mian\n",
      "44. Eduardo Morales\n",
      "45. Xiaosheng Mu\n",
      "46. Ulrich K. Müller\n",
      "47. Kelly Noonan\n",
      "48. Pietro Ortoleva\n",
      "49. Jonathan Payne\n",
      "50. Wolfgang Pesendorfer\n",
      "51. Mikkel Plagborg-Moller\n",
      "52. Stephen J. Redding\n",
      "53. Richard Rogerson\n",
      "54. Cecilia Rouse\n",
      "55. Ayşegül Sahin\n",
      "56. Batchimeg Sambalaibat\n",
      "57. Karthik Sastry\n",
      "58. Maria Micaela Sviatschi\n",
      "59. Can Urgun\n",
      "60. Giovanni (Gianluca) Violante\n",
      "61. Leonard Wantchekon\n",
      "62. Mark W. Watson\n",
      "63. Silvia Weyerbrock\n",
      "64. Wei Xiong\n",
      "65. Leeat Yariv\n",
      "66. Motohiro Yogo\n",
      "67. Iqbal Zaidi\n",
      "68. Owen Zidar\n",
      "69. Dilip Abreu\n",
      "70. Orley Ashenfelter\n",
      "71. Anne C. Case\n",
      "72. Gregory C. Chow\n",
      "73. Sir Angus S. Deaton\n",
      "74. Avinash K. Dixit\n",
      "75. Henry S. Farber\n",
      "76. Paul Krugman\n",
      "77. Burton G. Malkiel\n",
      "78. Richard E. Quandt\n",
      "79. Thomas Romer\n",
      "80. Harvey S. Rosen\n",
      "81. Michael Rothschild\n",
      "82. José A. Scheinkman\n",
      "83. Harold T. Shapiro\n",
      "84. Christopher A. Sims\n"
     ]
    }
   ],
   "source": [
    "# Function to print names in a formatted way\n",
    "def print_faculty_names(names):\n",
    "    print(f\"Total faculty members found: {len(names)}\")\n",
    "    print(\"\\nFaculty Names:\")\n",
    "    for i, name in enumerate(names, 1):\n",
    "        print(f\"{i}. {name}\")\n",
    "\n",
    "# Extract faculty names\n",
    "faculty_names = extract_faculty_names(html_content)\n",
    "print_faculty_names(faculty_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping faculty from: https://lsa.umich.edu/econ/people/faculty.html\n",
      "Successfully scraped 0 faculty names from lsa.umich.edu.\n",
      "Warning: No faculty names found for lsa.umich.edu.\n",
      "\n",
      "All Faculty Names Collected:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Scraping Summary Report:\n",
      "      University                                             URL  \\\n",
      "0  lsa.umich.edu  https://lsa.umich.edu/econ/people/faculty.html   \n",
      "\n",
      "           Status  \n",
      "0  No names found  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# List of faculty URLs\n",
    "faculty_urls = [\n",
    "    # \"https://economics.harvard.edu/faculty\",\n",
    "    # \"https://economics.mit.edu/people/faculty\",\n",
    "    # \"https://econ.berkeley.edu/people/faculty\",\n",
    "    # \"https://economics.uchicago.edu/people/faculty\",\n",
    "    # \"https://economics.princeton.edu/people/\",\n",
    "    # \"https://economics.stanford.edu/people/faculty\",\n",
    "    # \"https://economics.yale.edu/people\",\n",
    "    # \"https://econ.columbia.edu/faculty/\",\n",
    "    # \"https://as.nyu.edu/departments/econ/faculty.html\",\n",
    "    # \"https://economics.sas.upenn.edu/people/faculty\",\n",
    "    # \"https://economics.brown.edu/people/faculty\",\n",
    "    # \"https://www.bu.edu/econ/people/faculty/\",\n",
    "    # \"https://dornsife.usc.edu/econ/faculty/\",\n",
    "    \"https://lsa.umich.edu/econ/people/faculty.html\",\n",
    "    # \"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\",\n",
    "    # \"https://economics.dartmouth.edu/people\",\n",
    "    # \"https://economics.northwestern.edu/people/faculty/\",\n",
    "    # \"https://economics.ucla.edu/faculty/ladder\",\n",
    "    # \"https://business.columbia.edu/faculty/divisions/finance/faculty\",\n",
    "    # \"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\",\n",
    "    # \"https://econ.msu.edu/about/directory\",\n",
    "    # \"https://economics.cornell.edu/faculty\",\n",
    "    # \"https://econ.wisc.edu/faculty/\",\n",
    "    # \"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\",\n",
    "    # \"https://economics.ucdavis.edu/people/faculty\"\n",
    "]\n",
    "\n",
    "# Function to validate names\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, starting with uppercase letters\n",
    "    if re.match(r\"^[A-Z][a-zA-Z\\-\\'\\.]+\\s[A-Z][a-zA-Z\\-\\'\\.]+\", text):\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\n",
    "            \"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\",\n",
    "            \"Bureau\", \"Research\", \"Department\", \"Faculty\", \"Staff\",\n",
    "            \"Emeriti\", \"Adjunct\", \"Visiting\", \"Professor\", \"Lecturer\",\n",
    "            \"Assistant\", \"Associate\", \"Director\", \"Chair\", \"Affiliated\"\n",
    "        ]\n",
    "        if not any(keyword.lower() in text.lower() for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scraping functions for each university\n",
    "\n",
    "def scrape_harvard_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for widget_id in [\"widget-1\", \"widget-2\"]:\n",
    "            tab = soup.find(\"div\", id=widget_id)\n",
    "            if tab:\n",
    "                for name in tab.find_all(\"a\"):\n",
    "                    text = name.get_text(strip=True)\n",
    "                    if is_valid_name(text):\n",
    "                        faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_mit_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        container = soup.find(\"div\", class_=\"view-content faculty-landing__items\")\n",
    "        if container:\n",
    "            for name in container.find_all(\"h3\", class_=\"profile-teaser__name\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_berkeley_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Locate all faculty profiles\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"display-name\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UC Berkeley - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_uchicago_faculty(url):\n",
    "    # Same as before\n",
    "    faculty_names = []\n",
    "    page = 0\n",
    "    while True:\n",
    "        page_url = f\"{url}?title=all&name=all&page={page}\"\n",
    "        response = requests.get(page_url)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        rows = soup.select(\".views-row\")\n",
    "        if not rows:\n",
    "            break\n",
    "        for row in rows:\n",
    "            name_tag = row.select_one(\"h2.no-tags a\")\n",
    "            if name_tag:\n",
    "                name = name_tag.text.strip()\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "        page += 1\n",
    "        time.sleep(1)  # Respectful scraping\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_princeton_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all person divs\n",
    "        faculty_divs = soup.find_all('div', class_='person')\n",
    "        for faculty in faculty_divs:\n",
    "            name_tag = faculty.find('h3')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_stanford_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all faculty cards\n",
    "        faculty_cards = soup.find_all('div', class_='views-row')\n",
    "        for card in faculty_cards:\n",
    "            # Find the name element within each card\n",
    "            name_element = card.find('div', class_='views-field-title')\n",
    "            if name_element:\n",
    "                name_span = name_element.find('span', class_='field-content')\n",
    "                if name_span:\n",
    "                    name = name_span.find('a').get_text(strip=True)\n",
    "                    if is_valid_name(name):\n",
    "                        faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_yale_faculty(url):\n",
    "    response = requests.get(url + \"?person_type=2\")  # Add faculty filter parameter\n",
    "    faculty_names = []\n",
    "    page = 0\n",
    "    \n",
    "    while True:\n",
    "        page_url = f\"{url}?person_type=2&page={page}\"  # Add page parameter\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all person articles\n",
    "            faculty_articles = soup.find_all('article', class_='node-teaser--person')\n",
    "            \n",
    "            # If no more faculty articles found, break the loop\n",
    "            if not faculty_articles:\n",
    "                break\n",
    "                \n",
    "            for article in faculty_articles:\n",
    "                # Get the name from the heading\n",
    "                name_div = article.find('div', class_='node-teaser__heading')\n",
    "                if name_div:\n",
    "                    name_link = name_div.find('a')\n",
    "                    if name_link:\n",
    "                        name = name_link.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "            \n",
    "            # Look for \"Next\" link to determine if there are more pages\n",
    "            next_link = soup.find('li', class_='views-mini-pager__item--next')\n",
    "            if not next_link:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            time.sleep(1)  # Add a delay between requests to be polite\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_columbia_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty boxes\n",
    "        faculty_boxes = soup.find_all('div', class_='tshowcase-box')\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Get title/position to check if they're faculty\n",
    "            position_div = box.find('div', class_='tshowcase-single-position')\n",
    "            if position_div:\n",
    "                position = position_div.get_text(strip=True).lower()\n",
    "                \n",
    "                # Check if they're a professor (includes assistant, associate, full)\n",
    "                if 'professor' in position and not any(x in position for x in ['visiting', 'adjunct', 'emeritus']):\n",
    "                    # Get the name from the title element\n",
    "                    name_div = box.find('div', class_='tshowcase-box-title')\n",
    "                    if name_div:\n",
    "                        name = name_div.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "                            \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_nyu_faculty(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty containers in the undergraduate tab (main faculty)\n",
    "        faculty_boxes = soup.find_all('div', class_='book-container section')\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Get the role/title\n",
    "            author_div = box.find('div', class_='book-box__author')\n",
    "            if author_div:\n",
    "                role = author_div.get_text(strip=True).lower()\n",
    "                \n",
    "                # Filter for professors (including assistant/associate) but exclude clinical, visiting, emeritus\n",
    "                if ('professor' in role and \n",
    "                    'clinical' not in role and \n",
    "                    'visiting' not in role and \n",
    "                    'emeritus' not in role and\n",
    "                    'courtesy' not in role):\n",
    "                    \n",
    "                    # Get the faculty name\n",
    "                    name_div = box.find('h2', class_='book-box__title')\n",
    "                    if name_div:\n",
    "                        name = name_div.get_text(strip=True)\n",
    "                        # Remove \"Associated appointment\" prefix if present\n",
    "                        if \"Associated appointment\" in name:\n",
    "                            continue\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_penn_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty rows\n",
    "        faculty_rows = soup.find_all('li', class_='row')\n",
    "        \n",
    "        for row in faculty_rows:\n",
    "            # Get the name from h3 tag\n",
    "            name_tag = row.find('h3')\n",
    "            # Get the title/position from h4 tag\n",
    "            title_tag = row.find('h4')\n",
    "            \n",
    "            if name_tag and title_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                title = title_tag.get_text(strip=True).lower()\n",
    "                \n",
    "                # Exclude certain positions\n",
    "                excluded_titles = ['lecturer', 'adjunct', 'visiting', 'emeritus', 'scholar']\n",
    "                if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                    # Clean up the name by removing any additional text after the name\n",
    "                    name = name.split('(')[0].strip()  # Remove anything in parentheses\n",
    "                    if is_valid_name(name):\n",
    "                        faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_brown_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty list items in the main faculty section\n",
    "        # (Excluding the \"Affiliates\" section that comes after)\n",
    "        faculty_section = soup.find('div', class_='component_block_3999')\n",
    "        if faculty_section:\n",
    "            faculty_items = faculty_section.find_all('li', class_='people_item')\n",
    "            \n",
    "            for item in faculty_items:\n",
    "                # Get name and title\n",
    "                name_tag = item.find('h3', class_='people_item_name')\n",
    "                title_tag = item.find('div', class_='people_item_title')\n",
    "                \n",
    "                if name_tag and title_tag:\n",
    "                    name = name_tag.get_text(strip=True)\n",
    "                    title = title_tag.get_text(strip=True).lower()\n",
    "                    \n",
    "                    # Exclude non-faculty positions\n",
    "                    excluded_titles = ['visiting', 'adjunct', 'lecturer', 'affiliate', 'scholar', 'emeritus']\n",
    "                    if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                        # Clean up the name\n",
    "                        name = name.replace('\\n', ' ').strip()\n",
    "                        # Remove additional whitespace between words\n",
    "                        name = ' '.join(name.split())\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_bu_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the faculty list container\n",
    "        faculty_list = soup.find('ul', class_='profile-listing')\n",
    "        if faculty_list:\n",
    "            # Find all faculty items\n",
    "            faculty_items = faculty_list.find_all('li', class_='profile-item')\n",
    "            \n",
    "            for item in faculty_items:\n",
    "                # Get name and title\n",
    "                name_tag = item.find('h6', class_='profile-name')\n",
    "                title_tag = item.find('p', class_='profile-title')\n",
    "                \n",
    "                if name_tag and title_tag:\n",
    "                    name = name_tag.get_text(strip=True)\n",
    "                    title = title_tag.get_text(strip=True).lower()\n",
    "                    \n",
    "                    # Exclude non-faculty positions\n",
    "                    excluded_titles = ['lecturer', 'instructor', 'visiting', 'adjunct', 'emeritus']\n",
    "                    if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                        # Clean up any special characters or extra whitespace in name\n",
    "                        name = name.replace('\"', '').replace('\"', '')\n",
    "                        name = ' '.join(name.split())\n",
    "                        \n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_usc_faculty(base_url=\"https://dornsife.usc.edu/econ/faculty/\"):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    faculty_names = []\n",
    "    page = 1\n",
    "    more_pages = True\n",
    "    \n",
    "    while more_pages:\n",
    "        # Construct URL for current page\n",
    "        if page == 1:\n",
    "            current_url = base_url\n",
    "        else:\n",
    "            current_url = f\"{base_url}page/{page}/\"\n",
    "            \n",
    "        print(f\"Scraping page {page}: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find all the person cards on this page\n",
    "                faculty_cards = soup.find_all('div', class_='person-card')\n",
    "                \n",
    "                if not faculty_cards:  # If no faculty cards found, we've reached the end\n",
    "                    more_pages = False\n",
    "                    continue\n",
    "                \n",
    "                for card in faculty_cards:\n",
    "                    # Get name from the h3 within f--field f--cta-title div\n",
    "                    name_container = card.find('div', class_='f--field f--cta-title')\n",
    "                    if name_container:\n",
    "                        name_tag = name_container.find('h3')\n",
    "                        if name_tag and name_tag.find('a'):\n",
    "                            name = name_tag.find('a').get_text(strip=True)\n",
    "                            \n",
    "                            # Get title from the span with class person-title\n",
    "                            title_tag = card.find('span', class_='person-title')\n",
    "                            if title_tag:\n",
    "                                title = title_tag.get_text(strip=True).lower()\n",
    "                                \n",
    "                                # Filter faculty based on title\n",
    "                                excluded_titles = [\n",
    "                                    'lecturer',\n",
    "                                    'adjunct',\n",
    "                                    'visiting',\n",
    "                                    'emeritus',\n",
    "                                    'practice',\n",
    "                                    'teaching'\n",
    "                                ]\n",
    "                                \n",
    "                                # Include if contains 'professor' and not any excluded terms\n",
    "                                if ('professor' in title.lower() and \n",
    "                                    not any(exclude in title.lower() for exclude in excluded_titles)):\n",
    "                                    \n",
    "                                    # Clean up the name\n",
    "                                    name = ' '.join(name.split())\n",
    "                                    if is_valid_name(name) and name not in faculty_names:\n",
    "                                        faculty_names.append(name)\n",
    "                \n",
    "                print(f\"Found {len(faculty_names)} total faculty names after page {page}\")\n",
    "                \n",
    "                # Check if next page exists by looking for content\n",
    "                if not faculty_cards or len(faculty_cards) == 0:\n",
    "                    more_pages = False\n",
    "                else:\n",
    "                    page += 1\n",
    "                    \n",
    "            else:\n",
    "                # If we get a non-200 status code, we've likely reached the end\n",
    "                more_pages = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page}: {str(e)}\")\n",
    "            more_pages = False\n",
    "    \n",
    "    print(f\"\\nFound total of {len(faculty_names)} faculty members across {page-1} pages\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_michigan_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all `div` elements with class `person-wrap`, each containing a faculty profile\n",
    "        person_wraps = soup.find_all('div', class_='person-wrap')\n",
    "        \n",
    "        for person in person_wraps:\n",
    "            # Locate the `a` tag containing the name in the `title` attribute\n",
    "            profile_link = person.find('a')\n",
    "            if profile_link and 'title' in profile_link.attrs:\n",
    "                # Extract the name from the `title` attribute, removing ' profile'\n",
    "                name = profile_link['title'].replace(' profile', '').strip()\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "                    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "\n",
    "    return faculty_names\n",
    "\n",
    "\n",
    "def scrape_ucsd_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for profile in soup.find_all('div', class_='faculty-profile'):\n",
    "            name_tag = profile.find('h3')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_dartmouth_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('div', class_='views-row'):\n",
    "            name_tag = person.find('h2')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_northwestern_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h3', class_='people__name'):\n",
    "            name_tag = person.find('a')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucla_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h4', class_='people-list-item__name'):\n",
    "            name = person.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_columbiabusiness_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for profile in soup.find_all('div', class_='views-row'):\n",
    "            name_tag = profile.find('span', class_='field-content')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_bc_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h3', class_='person-card__name'):\n",
    "            name = person.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_msu_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h3', class_='staff__name'):\n",
    "            name = person.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_cornell_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"views-field-title\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_wisconsin_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for name_tag in soup.find_all(\"h3\", class_=\"person-name\"):\n",
    "            name = name_tag.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_duke_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"h4\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucdavis_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for tile in soup.find_all(\"div\", class_=\"tileItem\"):\n",
    "            name_tag = tile.find(\"h2\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_generic_faculty(url):\n",
    "    # Generic scraping function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            text = link.get_text(strip=True)\n",
    "            if is_valid_name(text):\n",
    "                faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "# Mapping URLs to their respective scraping functions\n",
    "scraping_functions = {\n",
    "    # \"https://economics.harvard.edu/faculty\": scrape_harvard_faculty,\n",
    "    # \"https://economics.mit.edu/people/faculty\": scrape_mit_faculty,\n",
    "    # \"https://econ.berkeley.edu/people/faculty\": scrape_berkeley_faculty,\n",
    "    # \"https://economics.uchicago.edu/people/faculty\": scrape_uchicago_faculty,\n",
    "    # \"https://economics.princeton.edu/people/\": scrape_princeton_faculty,\n",
    "    # \"https://economics.stanford.edu/people/faculty\": scrape_stanford_faculty,\n",
    "    # \"https://economics.yale.edu/people\": scrape_yale_faculty,\n",
    "    # \"https://econ.columbia.edu/faculty/\": scrape_columbia_faculty,\n",
    "    # \"https://as.nyu.edu/departments/econ/faculty.html\": scrape_nyu_faculty,\n",
    "    # \"https://economics.sas.upenn.edu/people/faculty\": scrape_penn_faculty,\n",
    "    # \"https://economics.brown.edu/people/faculty\": scrape_brown_faculty,\n",
    "    # \"https://www.bu.edu/econ/people/faculty/\": scrape_bu_faculty,\n",
    "    # \"https://dornsife.usc.edu/econ/faculty/\": scrape_usc_faculty,\n",
    "    \"https://lsa.umich.edu/econ/people/faculty.html\": scrape_michigan_faculty,\n",
    "    # \"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\": scrape_ucsd_faculty,\n",
    "    # \"https://economics.dartmouth.edu/people\": scrape_dartmouth_faculty,\n",
    "    # \"https://economics.northwestern.edu/people/faculty/\": scrape_northwestern_faculty,\n",
    "    # \"https://economics.ucla.edu/faculty/ladder\": scrape_ucla_faculty,\n",
    "    # \"https://business.columbia.edu/faculty/divisions/finance/faculty\": scrape_columbiabusiness_faculty,\n",
    "    # \"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\": scrape_bc_faculty,\n",
    "    # \"https://econ.msu.edu/about/directory\": scrape_msu_faculty,\n",
    "    # \"https://economics.cornell.edu/faculty\": scrape_cornell_faculty,\n",
    "    # \"https://econ.wisc.edu/faculty/\": scrape_wisconsin_faculty,\n",
    "    # \"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\": scrape_duke_faculty,\n",
    "    # \"https://economics.ucdavis.edu/people/faculty\": scrape_ucdavis_faculty\n",
    "}\n",
    "\n",
    "# Collect all faculty names in a list\n",
    "all_faculty = []\n",
    "scraping_results = []\n",
    "\n",
    "for url in faculty_urls:\n",
    "    university_match = re.findall(r'https?://(?:www\\.)?([^/]+)/', url)\n",
    "    university_name = university_match[0].replace('www.', '') if university_match else 'Unknown'\n",
    "    print(f\"\\nScraping faculty from: {url}\")\n",
    "    try:\n",
    "        if url in scraping_functions:\n",
    "            scrape_function = scraping_functions[url]\n",
    "        else:\n",
    "            scrape_function = scrape_generic_faculty\n",
    "            print(\"Using generic scraping function.\")\n",
    "        faculty_names = scrape_function(url)\n",
    "        num_names = len(faculty_names)\n",
    "        print(f\"Successfully scraped {num_names} faculty names from {university_name}.\")\n",
    "        if num_names == 0:\n",
    "            print(f\"Warning: No faculty names found for {university_name}.\")\n",
    "            scraping_results.append({'University': university_name, 'URL': url, 'Status': 'No names found'})\n",
    "        else:\n",
    "            scraping_results.append({'University': university_name, 'URL': url, 'Status': f'{num_names} names found'})\n",
    "            for name in faculty_names:\n",
    "                all_faculty.append({'University': university_name, 'Faculty Name': name})\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {university_name}: {e}\")\n",
    "        scraping_results.append({'University': university_name, 'URL': url, 'Status': f'Error: {e}'})\n",
    "    time.sleep(1)  # Be polite to the servers\n",
    "\n",
    "# Create a DataFrame with all faculty names\n",
    "df_all_faculty = pd.DataFrame(all_faculty)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nAll Faculty Names Collected:\")\n",
    "print(df_all_faculty)\n",
    "\n",
    "# Display a summary report\n",
    "print(\"\\nScraping Summary Report:\")\n",
    "df_scraping_results = pd.DataFrame(scraping_results)\n",
    "print(df_scraping_results)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "# df_all_faculty.to_csv('faculty_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_faculty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
