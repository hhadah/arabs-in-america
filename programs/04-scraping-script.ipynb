{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# !pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_urls = [\n",
    "\"https://economics.harvard.edu/faculty\",\n",
    "\"https://economics.mit.edu/people/faculty\",\n",
    "\"https://econ.berkeley.edu/people/faculty\",\n",
    "\"https://economics.uchicago.edu/people/faculty\",\n",
    "\"https://economics.princeton.edu/people/\",\n",
    "\"https://economics.stanford.edu/people/faculty\",\n",
    "\"https://economics.yale.edu/people\",\n",
    "\"https://econ.columbia.edu/faculty/\",\n",
    "\"https://as.nyu.edu/departments/econ/faculty.html\",\n",
    "\"https://economics.sas.upenn.edu/people/faculty\",\n",
    "\"https://economics.brown.edu/people/faculty\",\n",
    "\"https://www.bu.edu/econ/people/faculty/\",\n",
    "\"https://dornsife.usc.edu/econ/faculty/\",\n",
    "\"https://lsa.umich.edu/econ/people/faculty.html\",\n",
    "\"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\",\n",
    "\"https://economics.dartmouth.edu/people\",\n",
    "\"https://economics.northwestern.edu/people/faculty/\",\n",
    "\"https://economics.ucla.edu/faculty/ladder\",\n",
    "\"https://business.columbia.edu/faculty/divisions/finance/faculty\",\n",
    "\"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\",\n",
    "\"https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=\",\n",
    "\"https://economics.cornell.edu/faculty\",\n",
    "\"https://econ.wisc.edu/faculty/\",\n",
    "\"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\",\n",
    "\"https://economics.ucdavis.edu/people/faculty\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print names in a formatted way\n",
    "def print_faculty_names(names):\n",
    "    print(f\"Total faculty members found: {len(names)}\")\n",
    "    print(\"\\nFaculty Names:\")\n",
    "    for i, name in enumerate(names, 1):\n",
    "        print(f\"{i}. {name}\")\n",
    "\n",
    "# Extract faculty names\n",
    "faculty_names = extract_faculty_names(html_content)\n",
    "print_faculty_names(faculty_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping faculty from: https://econ.wisc.edu/faculty/\n",
      "Found faculty: Miguel Acosta\n",
      "Found faculty: Naoki Aizawa\n",
      "Found faculty: Simeon Alder\n",
      "Found faculty: Panle Barwick\n",
      "Found faculty: Benjamin Bernard\n",
      "Found faculty: Job Boerma\n",
      "Found faculty: John Brauer\n",
      "Found faculty: Yong Cai\n",
      "Found faculty: Matteo Camboni\n",
      "Found faculty: Stella Chan\n",
      "Found faculty: Harold Chiang\n",
      "Found faculty: Menzie Chinn\n",
      "Found faculty: Dean Corbae\n",
      "Found faculty: Louphou Coulibaly\n",
      "Found faculty: Lydia Cox\n",
      "Found faculty: Raymond Deneckere\n",
      "Found faculty: Charles Engel\n",
      "Found faculty: Gwen Eudey\n",
      "Found faculty: Fran Flanagan\n",
      "Found faculty: Matt Friedman\n",
      "Found faculty: Chao Fu\n",
      "Found faculty: Rebecca Glawtschew\n",
      "Found faculty: Jesse Gregory\n",
      "Found faculty: Agustin Gutierrez\n",
      "Found faculty: Bruce Hansen\n",
      "Found faculty: David Hansen\n",
      "Found faculty: Korinna Hansen\n",
      "Found faculty: Kenneth Hendricks\n",
      "Found faculty: Jean-Francois Houde\n",
      "Found faculty: David Johnson\n",
      "Found faculty: Karam Kang\n",
      "Found faculty: John Kennan\n",
      "Found faculty: Rishabh Kirpalani\n",
      "Found faculty: Rasmus Lentz\n",
      "Found faculty: Lorenzo Magnolfi\n",
      "Found faculty: Christopher McKelvey\n",
      "Found faculty: Corina Mommaerts\n",
      "Found faculty: Martin O'Connell\n",
      "Found faculty: Jack Porter\n",
      "Found faculty: Daniel Quint\n",
      "Found faculty: Steven Rick\n",
      "Found faculty: Fernanda Rojas-Ampuero\n",
      "Found faculty: Marzena Rostek\n",
      "Found faculty: Kim Ruhl\n",
      "Found faculty: Laura Schechter\n",
      "Found faculty: Ananth Seshadri\n",
      "Found faculty: Xiaoxia Shi\n",
      "Found faculty: Jeffrey Smith\n",
      "Found faculty: Lones Smith\n",
      "Found faculty: Alan Sorensen\n",
      "Found faculty: Christopher Sullivan\n",
      "Found faculty: Ashley Swanson\n",
      "Found faculty: Christopher Taber\n",
      "Found faculty: Steve Trost\n",
      "Found faculty: Marek Weretka\n",
      "Found faculty: Kenneth West\n",
      "Found faculty: Randall Wright\n",
      "Found faculty: Alice Wu\n",
      "Found faculty: Kohei Yata\n",
      "Wisconsin - Found 59 faculty names.\n",
      "Successfully scraped 59 faculty names from econ.wisc.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.duke.edu/people/other-faculty/regular-rank-faculty\n",
      "Duke - Found 0 faculty names.\n",
      "Successfully scraped 0 faculty names from econ.duke.edu.\n",
      "Warning: No faculty names found for econ.duke.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.ucdavis.edu/people/faculty\n",
      "Found faculty: Paul Bergin\n",
      "Found faculty: Marianne Bitler\n",
      "Found faculty: Giacomo Bonanno\n",
      "Found faculty: James Bushnell\n",
      "Found faculty: A. Colin Cameron\n",
      "Found faculty: Nicolas Caramp\n",
      "Found faculty: Andres Carvajal\n",
      "Found faculty: Anujit Chakraborty\n",
      "Found faculty: James Cloyne\n",
      "Found faculty: Katherine Eriksson\n",
      "Found faculty: Athanasios Geromichalos\n",
      "Found faculty: Xian Jiang\n",
      "Found faculty: Oscar Jorda\n",
      "Found faculty: Emile Marin\n",
      "Found faculty: Christopher Meissner\n",
      "Found faculty: Diana Moreira\n",
      "Found faculty: Erich Muehlegger\n",
      "Found faculty: Marianne Page\n",
      "Found faculty: Giovanni Peri\n",
      "Found faculty: Dave Rapson\n",
      "Found faculty: Arman Rezaee\n",
      "Found faculty: Katheryn Russ\n",
      "Found faculty: Burkhard Schipper\n",
      "Found faculty: Christoph Schlom\n",
      "Found faculty: Shu Shen\n",
      "Found faculty: Ina Simonovska\n",
      "Found faculty: Sanjay Singh\n",
      "Found faculty: Monica Singhal\n",
      "Found faculty: Jenna Stearns\n",
      "Found faculty: Derek Stimel\n",
      "Found faculty: Deborah Swenson\n",
      "Found faculty: Takuya Ura\n",
      "Found faculty: Janine Wilson\n",
      "UC Davis - Found 33 faculty names.\n",
      "Successfully scraped 33 faculty names from economics.ucdavis.edu.\n",
      "\n",
      "All Faculty Names Collected:\n",
      "               University      Faculty Name\n",
      "0           econ.wisc.edu     Miguel Acosta\n",
      "1           econ.wisc.edu      Naoki Aizawa\n",
      "2           econ.wisc.edu      Simeon Alder\n",
      "3           econ.wisc.edu     Panle Barwick\n",
      "4           econ.wisc.edu  Benjamin Bernard\n",
      "..                    ...               ...\n",
      "87  economics.ucdavis.edu     Jenna Stearns\n",
      "88  economics.ucdavis.edu      Derek Stimel\n",
      "89  economics.ucdavis.edu   Deborah Swenson\n",
      "90  economics.ucdavis.edu        Takuya Ura\n",
      "91  economics.ucdavis.edu     Janine Wilson\n",
      "\n",
      "[92 rows x 2 columns]\n",
      "\n",
      "Scraping Summary Report:\n",
      "              University                                                URL  \\\n",
      "0          econ.wisc.edu                     https://econ.wisc.edu/faculty/   \n",
      "1          econ.duke.edu  https://econ.duke.edu/people/other-faculty/reg...   \n",
      "2  economics.ucdavis.edu       https://economics.ucdavis.edu/people/faculty   \n",
      "\n",
      "           Status  \n",
      "0  59 names found  \n",
      "1  No names found  \n",
      "2  33 names found  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# List of faculty URLs\n",
    "faculty_urls = [\n",
    "    \"https://economics.harvard.edu/faculty\",\n",
    "    \"https://economics.mit.edu/people/faculty\",\n",
    "    \"https://econ.berkeley.edu/people/faculty\",\n",
    "    \"https://economics.uchicago.edu/people/faculty\",\n",
    "    \"https://economics.princeton.edu/people/\",\n",
    "    \"https://economics.stanford.edu/people/faculty\",\n",
    "    \"https://economics.yale.edu/people\",\n",
    "    \"https://econ.columbia.edu/faculty/\",\n",
    "    \"https://as.nyu.edu/departments/econ/faculty.html\",\n",
    "    \"https://economics.sas.upenn.edu/people/faculty\",\n",
    "    \"https://economics.brown.edu/people/faculty\",\n",
    "    \"https://www.bu.edu/econ/people/faculty/\",\n",
    "    \"https://dornsife.usc.edu/econ/faculty/\",\n",
    "    \"https://lsa.umich.edu/econ/people/faculty.html\",\n",
    "    \"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\",\n",
    "    \"https://economics.dartmouth.edu/people\",\n",
    "    \"https://economics.northwestern.edu/people/faculty/\",\n",
    "    \"https://economics.ucla.edu/faculty/ladder\",\n",
    "    \"https://business.columbia.edu/faculty/divisions/finance/faculty\",\n",
    "    \"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\",\n",
    "    \"https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=\",\n",
    "    \"https://economics.cornell.edu/faculty\",\n",
    "    \"https://econ.wisc.edu/faculty/\",\n",
    "    \"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\",\n",
    "    \"https://economics.ucdavis.edu/people/faculty\"\n",
    "]\n",
    "\n",
    "# Function to validate names\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, starting with uppercase letters\n",
    "    if re.match(r\"^[A-Z][a-zA-Z\\-\\'\\.]+\\s[A-Z][a-zA-Z\\-\\'\\.]+\", text):\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\n",
    "            \"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\",\n",
    "            \"Bureau\", \"Research\", \"Department\", \"Faculty\", \"Staff\",\n",
    "            \"Emeriti\", \"Adjunct\", \"Visiting\", \"Professor\", \"Lecturer\",\n",
    "            \"Assistant\", \"Associate\", \"Director\", \"Chair\", \"Affiliated\"\n",
    "        ]\n",
    "        if not any(keyword.lower() in text.lower() for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scraping functions for each university\n",
    "\n",
    "def scrape_harvard_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for widget_id in [\"widget-1\", \"widget-2\"]:\n",
    "            tab = soup.find(\"div\", id=widget_id)\n",
    "            if tab:\n",
    "                for name in tab.find_all(\"a\"):\n",
    "                    text = name.get_text(strip=True)\n",
    "                    if is_valid_name(text):\n",
    "                        faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_mit_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        container = soup.find(\"div\", class_=\"view-content faculty-landing__items\")\n",
    "        if container:\n",
    "            for name in container.find_all(\"h3\", class_=\"profile-teaser__name\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_berkeley_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Locate all faculty profiles\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"display-name\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UC Berkeley - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_uchicago_faculty(url):\n",
    "    # Same as before\n",
    "    faculty_names = []\n",
    "    page = 0\n",
    "    while True:\n",
    "        page_url = f\"{url}?title=all&name=all&page={page}\"\n",
    "        response = requests.get(page_url)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        rows = soup.select(\".views-row\")\n",
    "        if not rows:\n",
    "            break\n",
    "        for row in rows:\n",
    "            name_tag = row.select_one(\"h2.no-tags a\")\n",
    "            if name_tag:\n",
    "                name = name_tag.text.strip()\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "        page += 1\n",
    "        time.sleep(1)  # Respectful scraping\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_princeton_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all person divs\n",
    "        faculty_divs = soup.find_all('div', class_='person')\n",
    "        for faculty in faculty_divs:\n",
    "            name_tag = faculty.find('h3')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_stanford_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all faculty cards\n",
    "        faculty_cards = soup.find_all('div', class_='views-row')\n",
    "        for card in faculty_cards:\n",
    "            # Find the name element within each card\n",
    "            name_element = card.find('div', class_='views-field-title')\n",
    "            if name_element:\n",
    "                name_span = name_element.find('span', class_='field-content')\n",
    "                if name_span:\n",
    "                    name = name_span.find('a').get_text(strip=True)\n",
    "                    if is_valid_name(name):\n",
    "                        faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_yale_faculty(url):\n",
    "    response = requests.get(url + \"?person_type=2\")  # Add faculty filter parameter\n",
    "    faculty_names = []\n",
    "    page = 0\n",
    "    \n",
    "    while True:\n",
    "        page_url = f\"{url}?person_type=2&page={page}\"  # Add page parameter\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all person articles\n",
    "            faculty_articles = soup.find_all('article', class_='node-teaser--person')\n",
    "            \n",
    "            # If no more faculty articles found, break the loop\n",
    "            if not faculty_articles:\n",
    "                break\n",
    "                \n",
    "            for article in faculty_articles:\n",
    "                # Get the name from the heading\n",
    "                name_div = article.find('div', class_='node-teaser__heading')\n",
    "                if name_div:\n",
    "                    name_link = name_div.find('a')\n",
    "                    if name_link:\n",
    "                        name = name_link.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "            \n",
    "            # Look for \"Next\" link to determine if there are more pages\n",
    "            next_link = soup.find('li', class_='views-mini-pager__item--next')\n",
    "            if not next_link:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            time.sleep(1)  # Add a delay between requests to be polite\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_columbia_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty boxes\n",
    "        faculty_boxes = soup.find_all('div', class_='tshowcase-box')\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Get title/position to check if they're faculty\n",
    "            position_div = box.find('div', class_='tshowcase-single-position')\n",
    "            if position_div:\n",
    "                position = position_div.get_text(strip=True).lower()\n",
    "                \n",
    "                # Check if they're a professor (includes assistant, associate, full)\n",
    "                if 'professor' in position and not any(x in position for x in ['visiting', 'adjunct', 'emeritus']):\n",
    "                    # Get the name from the title element\n",
    "                    name_div = box.find('div', class_='tshowcase-box-title')\n",
    "                    if name_div:\n",
    "                        name = name_div.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "                            \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_nyu_faculty(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty containers in the undergraduate tab (main faculty)\n",
    "        faculty_boxes = soup.find_all('div', class_='book-container section')\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Get the role/title\n",
    "            author_div = box.find('div', class_='book-box__author')\n",
    "            if author_div:\n",
    "                role = author_div.get_text(strip=True).lower()\n",
    "                \n",
    "                # Filter for professors (including assistant/associate) but exclude clinical, visiting, emeritus\n",
    "                if ('professor' in role and \n",
    "                    'clinical' not in role and \n",
    "                    'visiting' not in role and \n",
    "                    'emeritus' not in role and\n",
    "                    'courtesy' not in role):\n",
    "                    \n",
    "                    # Get the faculty name\n",
    "                    name_div = box.find('h2', class_='book-box__title')\n",
    "                    if name_div:\n",
    "                        name = name_div.get_text(strip=True)\n",
    "                        # Remove \"Associated appointment\" prefix if present\n",
    "                        if \"Associated appointment\" in name:\n",
    "                            continue\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_penn_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty rows\n",
    "        faculty_rows = soup.find_all('li', class_='row')\n",
    "        \n",
    "        for row in faculty_rows:\n",
    "            # Get the name from h3 tag\n",
    "            name_tag = row.find('h3')\n",
    "            # Get the title/position from h4 tag\n",
    "            title_tag = row.find('h4')\n",
    "            \n",
    "            if name_tag and title_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                title = title_tag.get_text(strip=True).lower()\n",
    "                \n",
    "                # Exclude certain positions\n",
    "                excluded_titles = ['lecturer', 'adjunct', 'visiting', 'emeritus', 'scholar']\n",
    "                if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                    # Clean up the name by removing any additional text after the name\n",
    "                    name = name.split('(')[0].strip()  # Remove anything in parentheses\n",
    "                    if is_valid_name(name):\n",
    "                        faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_brown_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty list items in the main faculty section\n",
    "        # (Excluding the \"Affiliates\" section that comes after)\n",
    "        faculty_section = soup.find('div', class_='component_block_3999')\n",
    "        if faculty_section:\n",
    "            faculty_items = faculty_section.find_all('li', class_='people_item')\n",
    "            \n",
    "            for item in faculty_items:\n",
    "                # Get name and title\n",
    "                name_tag = item.find('h3', class_='people_item_name')\n",
    "                title_tag = item.find('div', class_='people_item_title')\n",
    "                \n",
    "                if name_tag and title_tag:\n",
    "                    name = name_tag.get_text(strip=True)\n",
    "                    title = title_tag.get_text(strip=True).lower()\n",
    "                    \n",
    "                    # Exclude non-faculty positions\n",
    "                    excluded_titles = ['visiting', 'adjunct', 'lecturer', 'affiliate', 'scholar', 'emeritus']\n",
    "                    if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                        # Clean up the name\n",
    "                        name = name.replace('\\n', ' ').strip()\n",
    "                        # Remove additional whitespace between words\n",
    "                        name = ' '.join(name.split())\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_bu_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the faculty list container\n",
    "        faculty_list = soup.find('ul', class_='profile-listing')\n",
    "        if faculty_list:\n",
    "            # Find all faculty items\n",
    "            faculty_items = faculty_list.find_all('li', class_='profile-item')\n",
    "            \n",
    "            for item in faculty_items:\n",
    "                # Get name and title\n",
    "                name_tag = item.find('h6', class_='profile-name')\n",
    "                title_tag = item.find('p', class_='profile-title')\n",
    "                \n",
    "                if name_tag and title_tag:\n",
    "                    name = name_tag.get_text(strip=True)\n",
    "                    title = title_tag.get_text(strip=True).lower()\n",
    "                    \n",
    "                    # Exclude non-faculty positions\n",
    "                    excluded_titles = ['lecturer', 'instructor', 'visiting', 'adjunct', 'emeritus']\n",
    "                    if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                        # Clean up any special characters or extra whitespace in name\n",
    "                        name = name.replace('\"', '').replace('\"', '')\n",
    "                        name = ' '.join(name.split())\n",
    "                        \n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_usc_faculty(url=\"https://dornsife.usc.edu/econ/faculty/\"):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    faculty_names = []\n",
    "    page = 1\n",
    "    more_pages = True\n",
    "    \n",
    "    while more_pages:\n",
    "        # Construct URL for current page\n",
    "        if page == 1:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = f\"{url}page/{page}/\"\n",
    "            \n",
    "        print(f\"Scraping page {page}: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find all the person cards on this page\n",
    "                faculty_cards = soup.find_all('div', class_='person-card')\n",
    "                \n",
    "                if not faculty_cards:  # If no faculty cards found, we've reached the end\n",
    "                    more_pages = False\n",
    "                    continue\n",
    "                \n",
    "                for card in faculty_cards:\n",
    "                    # Get name from the h3 within f--field f--cta-title div\n",
    "                    name_container = card.find('div', class_='f--field f--cta-title')\n",
    "                    if name_container:\n",
    "                        name_tag = name_container.find('h3')\n",
    "                        if name_tag and name_tag.find('a'):\n",
    "                            name = name_tag.find('a').get_text(strip=True)\n",
    "                            \n",
    "                            # Get title from the span with class person-title\n",
    "                            title_tag = card.find('span', class_='person-title')\n",
    "                            if title_tag:\n",
    "                                title = title_tag.get_text(strip=True).lower()\n",
    "                                \n",
    "                                # Filter faculty based on title\n",
    "                                excluded_titles = [\n",
    "                                    'lecturer',\n",
    "                                    'adjunct',\n",
    "                                    'visiting',\n",
    "                                    'emeritus',\n",
    "                                    'practice',\n",
    "                                    'teaching'\n",
    "                                ]\n",
    "                                \n",
    "                                # Include if contains 'professor' and not any excluded terms\n",
    "                                if ('professor' in title.lower() and \n",
    "                                    not any(exclude in title.lower() for exclude in excluded_titles)):\n",
    "                                    \n",
    "                                    # Clean up the name\n",
    "                                    name = ' '.join(name.split())\n",
    "                                    if is_valid_name(name) and name not in faculty_names:\n",
    "                                        faculty_names.append(name)\n",
    "                \n",
    "                print(f\"Found {len(faculty_names)} total faculty names after page {page}\")\n",
    "                \n",
    "                # Check if next page exists by looking for content\n",
    "                if not faculty_cards or len(faculty_cards) == 0:\n",
    "                    more_pages = False\n",
    "                else:\n",
    "                    page += 1\n",
    "                    \n",
    "            else:\n",
    "                # If we get a non-200 status code, we've likely reached the end\n",
    "                more_pages = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page}: {str(e)}\")\n",
    "            more_pages = False\n",
    "    \n",
    "    print(f\"\\nFound total of {len(faculty_names)} faculty members across {page-1} pages\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_michigan_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate the main div containing all faculty profiles\n",
    "        people_list = soup.find('div', class_='people row equalHeight clearfix', id='people-list')\n",
    "        if not people_list:\n",
    "            print(\"Could not find the people-list div.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        # Find all individual faculty containers\n",
    "        person_wraps = people_list.find_all('div', class_='person-wrap')\n",
    "        \n",
    "        for person in person_wraps:\n",
    "            info_div = person.find('div', class_='info')\n",
    "            if info_div:\n",
    "                name_h3 = info_div.find('h3', class_='name')\n",
    "                if name_h3:\n",
    "                    # Find the <a> tag with classes 'themeText themeLink' inside the <h3>\n",
    "                    name_a = name_h3.find('a', class_='themeText themeLink')\n",
    "                    if name_a and name_a.text:\n",
    "                        name = name_a.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"Michigan - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Michigan faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "# Function to validate names\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, starting with uppercase letters\n",
    "    if re.match(r\"^[A-Z][a-zA-Z\\-\\'\\.]+\\s[A-Z][a-zA-Z\\-\\'\\.]+\", text):\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\n",
    "            \"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\",\n",
    "            \"Bureau\", \"Research\", \"Department\", \"Faculty\", \"Staff\",\n",
    "            \"Emeriti\", \"Adjunct\", \"Visiting\", \"Professor\", \"Lecturer\",\n",
    "            \"Assistant\", \"Associate\", \"Director\", \"Chair\", \"Affiliated\"\n",
    "        ]\n",
    "        if not any(keyword.lower() in text.lower() for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def scrape_ucsd_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate the article containing the faculty listings\n",
    "        article = soup.find('article', class_='clearfix profile-listing')\n",
    "        if not article:\n",
    "            print(\"Could not find the faculty listing article.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        # Find all list items with class 'profile-listing-card'\n",
    "        profiles = article.find_all('li', class_='profile-listing-card')\n",
    "        \n",
    "        for profile in profiles:\n",
    "            # Locate the span containing the profile data\n",
    "            data_span = profile.find('span', class_='profile-listing-data')\n",
    "            if data_span:\n",
    "                # Find the h3 tag within the data span\n",
    "                h3_tag = data_span.find('h3')\n",
    "                if h3_tag:\n",
    "                    # Find the a tag within the h3 tag to get the faculty name\n",
    "                    a_tag = h3_tag.find('a')\n",
    "                    if a_tag and a_tag.text:\n",
    "                        name = a_tag.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UCSD - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve UCSD faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "\n",
    "def scrape_dartmouth_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate all <ul> elements with class 'people-list clearfix'\n",
    "        people_lists = soup.find_all('ul', class_='people-list clearfix')\n",
    "        if not people_lists:\n",
    "            print(\"Could not find any people-list elements.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        for people_list in people_lists:\n",
    "            # Find all <article> elements within the <ul>\n",
    "            articles = people_list.find_all('article', class_='node-person node-page-listing')\n",
    "            for article in articles:\n",
    "                # Find the <h3> tag within the <div class=\"content\">\n",
    "                content_div = article.find('div', class_='content')\n",
    "                if content_div:\n",
    "                    h3_tag = content_div.find('h3')\n",
    "                    if h3_tag:\n",
    "                        a_tag = h3_tag.find('a')\n",
    "                        if a_tag and a_tag.text:\n",
    "                            name = a_tag.get_text(strip=True)\n",
    "                            if is_valid_name(name):\n",
    "                                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"Dartmouth - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Dartmouth faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_northwestern_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate all <article> elements with class 'people'\n",
    "        articles = soup.find_all('article', class_='people')\n",
    "        if not articles:\n",
    "            print(\"Could not find any article elements with class 'people'.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        for article in articles:\n",
    "            # Find the <h3> tag within the <div class=\"people-content\">\n",
    "            people_content = article.find('div', class_='people-content')\n",
    "            if people_content:\n",
    "                h3_tag = people_content.find('h3')\n",
    "                if h3_tag:\n",
    "                    a_tag = h3_tag.find('a')\n",
    "                    if a_tag and a_tag.text:\n",
    "                        name = a_tag.get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"Northwestern - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Northwestern faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucla_faculty(url):\n",
    "    \"\"\"\n",
    "    Scrapes faculty names from UCLA's Economics department page.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of UCLA's Economics faculty page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of faculty names.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Locate all <div> elements with class 'faculty-box'\n",
    "        faculty_boxes = soup.find_all('div', class_='faculty-box')\n",
    "        if not faculty_boxes:\n",
    "            print(\"Could not find any faculty-box elements.\")\n",
    "            return faculty_names\n",
    "        \n",
    "        for box in faculty_boxes:\n",
    "            # Find the <h3> tag within the faculty-box\n",
    "            h3_tag = box.find('h3')\n",
    "            if h3_tag:\n",
    "                # Find all <a> tags within the <h3> tag\n",
    "                a_tags = h3_tag.find_all('a')\n",
    "                if a_tags:\n",
    "                    # Assume the second <a> tag contains the faculty name\n",
    "                    if len(a_tags) >= 2 and a_tags[1].text:\n",
    "                        name = a_tags[1].get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "                    elif len(a_tags) == 1 and a_tags[0].text:\n",
    "                        # Fallback in case there's only one <a> tag\n",
    "                        name = a_tags[0].get_text(strip=True)\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UCLA - Found {len(faculty_names)} faculty names.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve UCLA faculty page. Status code: {response.status_code}\")\n",
    "    \n",
    "    return faculty_names\n",
    "\n",
    "def scrape_columbiabusiness_faculty(url):\n",
    "    \"\"\"\n",
    "    Scrapes faculty names from Columbia Business School's Finance faculty page.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of Columbia Business School's Finance faculty page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of faculty names.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept\": (\n",
    "            \"text/html,application/xhtml+xml,application/xml;\"\n",
    "            \"q=0.9,image/webp,image/apng,*/*;q=0.8\"\n",
    "        ),\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error while accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    faculty_names = []\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Locate all <div> elements with class 'm-listing-faculty'\n",
    "        faculty_boxes = soup.find_all('div', class_='m-listing-faculty')\n",
    "        if not faculty_boxes:\n",
    "            print(\"Could not find any 'm-listing-faculty' elements.\")\n",
    "            return faculty_names\n",
    "\n",
    "        for box in faculty_boxes:\n",
    "            # Find the <h3> tag with class 'm-listing-faculty__title' within the faculty box\n",
    "            h3_tag = box.find('h3', class_='m-listing-faculty__title')\n",
    "            if h3_tag:\n",
    "                # Extract text from the <h3> tag, which contains the faculty name\n",
    "                # It may contain nested <a> tags, so get the text content\n",
    "                name = h3_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "\n",
    "        print(f\"Columbia Business School - Found {len(faculty_names)} faculty names.\")\n",
    "    elif response.status_code == 403:\n",
    "        print(f\"Access to {url} is forbidden (403).\")\n",
    "        print(\"Attempting to bypass with additional headers or methods...\")\n",
    "        # Optionally, implement further strategies here\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Columbia Business School faculty page. Status code: {response.status_code}\")\n",
    "\n",
    "    return faculty_names\n",
    "\n",
    "\n",
    "def scrape_bc_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h3', class_='person-card__name'):\n",
    "            name = person.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_msu_faculty(url):\n",
    "    faculty_names = []\n",
    "    \n",
    "    # Define URL pattern for each page\n",
    "    urls = [\n",
    "        f\"{url}\",  # Page 1\n",
    "        f\"{url}&page=2&letter=\",  # Page 2\n",
    "        f\"{url}&page=3&letter=\",  # Page 3\n",
    "        f\"{url}&page=4&letter=\"   # Page 4\n",
    "    ]\n",
    "    \n",
    "    for page_url in urls:\n",
    "        print(f\"Scraping page: {page_url}\")\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all faculty items on current page\n",
    "            faculty_items = soup.find_all('div', class_='faculty-item')\n",
    "            \n",
    "            for item in faculty_items:\n",
    "                # Get name and title\n",
    "                name_tag = item.find('h2', class_='line')\n",
    "                title_tag = item.find('p', class_='title')\n",
    "                \n",
    "                if name_tag and title_tag:\n",
    "                    name = name_tag.get_text(strip=True)\n",
    "                    title = title_tag.get_text(strip=True).lower()\n",
    "                    \n",
    "                    # Filter out non-faculty positions\n",
    "                    excluded_titles = ['student', 'emeritus', 'retired']\n",
    "                    if title and not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                        if is_valid_name(name):\n",
    "                            faculty_names.append(name)\n",
    "                            print(f\"Found faculty: {name} - {title}\")\n",
    "        \n",
    "        time.sleep(1)  # Be polite to the server\n",
    "    \n",
    "    print(f\"MSU - Found {len(faculty_names)} faculty names.\")\n",
    "    return faculty_names\n",
    "\n",
    "\n",
    "def scrape_cornell_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty articles in the main content\n",
    "        faculty_articles = soup.find_all('article', class_='person--card')\n",
    "        \n",
    "        for article in faculty_articles:\n",
    "            # Get name and title\n",
    "            name_tag = article.find('h1', class_='person__name')\n",
    "            title_tag = article.find('span', class_='person__title')\n",
    "            \n",
    "            if name_tag and title_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                title = title_tag.get_text(strip=True).lower()\n",
    "                \n",
    "                # Filter out non-faculty positions\n",
    "                excluded_titles = ['visiting', 'adjunct', 'emeritus', 'lecturer', 'research associate']\n",
    "                if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                    if is_valid_name(name):\n",
    "                        faculty_names.append(name)\n",
    "                        print(f\"Found faculty: {name} - {title}\")\n",
    "    \n",
    "    print(f\"Cornell - Found {len(faculty_names)} faculty names.\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_wisconsin_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty divs\n",
    "        faculty_divs = soup.find_all('div', class_='faculty-member')\n",
    "        \n",
    "        for div in faculty_divs:\n",
    "            # Get name from h3 with class faculty-name\n",
    "            name_tag = div.find('h3', class_='faculty-name')\n",
    "            if name_tag:\n",
    "                # The name is inside an <a> tag\n",
    "                name_link = name_tag.find('a')\n",
    "                if name_link:\n",
    "                    name = name_link.get_text(strip=True)\n",
    "                    # Get position title to filter non-faculty\n",
    "                    position_tag = div.find('p', class_='position-title')\n",
    "                    if position_tag:\n",
    "                        title = position_tag.get_text(strip=True).lower()\n",
    "                        # Filter out non-faculty positions\n",
    "                        excluded_titles = ['emeritus', 'visiting', 'adjunct']\n",
    "                        if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                            if is_valid_name(name):\n",
    "                                faculty_names.append(name)\n",
    "                                print(f\"Found faculty: {name}\")\n",
    "    \n",
    "    print(f\"Wisconsin - Found {len(faculty_names)} faculty names.\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_duke_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty items in the grid\n",
    "        faculty_items = soup.find_all('div', class_='grid list-group-item')\n",
    "        \n",
    "        for item in faculty_items:\n",
    "            # Get name from h4 element within the article section\n",
    "            article = item.find('article')\n",
    "            if article:\n",
    "                name_div = article.find('div', class_='h4')\n",
    "                if name_div:\n",
    "                    name_link = name_div.find('a') \n",
    "                    if name_link:\n",
    "                        name = name_link.get_text(strip=True)\n",
    "                        \n",
    "                        # Get title to filter non-faculty\n",
    "                        title_div = article.find('div', class_='h6')\n",
    "                        if title_div:\n",
    "                            title = title_div.get_text(strip=True).lower()\n",
    "                            \n",
    "                            # Filter out non-faculty positions\n",
    "                            excluded_titles = ['emeritus', 'visiting', 'adjunct', 'lecturer', 'scholar']\n",
    "                            if not any(exclude in title.lower() for exclude in excluded_titles):\n",
    "                                if is_valid_name(name):\n",
    "                                    faculty_names.append(name)\n",
    "                                    print(f\"Found faculty: {name}\")\n",
    "    \n",
    "    print(f\"Duke - Found {len(faculty_names)} faculty names.\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucdavis_faculty(url):\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all faculty articles\n",
    "        faculty_items = soup.find_all('article', class_='node--type-sf-person')\n",
    "        \n",
    "        for item in faculty_items:\n",
    "            # Get name from h3 within vm-teaser__title\n",
    "            name_tag = item.find('h3', class_='vm-teaser__title')\n",
    "            if name_tag:\n",
    "                name_link = name_tag.find('a')\n",
    "                if name_link:\n",
    "                    name = name_link.get_text(strip=True)\n",
    "                    # Get position to filter non-faculty\n",
    "                    position_list = item.find('ul', class_='vm-teaser__position')\n",
    "                    if position_list:\n",
    "                        position = position_list.get_text(strip=True).lower()\n",
    "                        # Filter out non-faculty positions\n",
    "                        excluded_titles = ['emeritus', 'visiting', 'adjunct', 'lecturer']\n",
    "                        if not any(exclude in position.lower() for exclude in excluded_titles):\n",
    "                            if is_valid_name(name):\n",
    "                                faculty_names.append(name)\n",
    "                                print(f\"Found faculty: {name}\")\n",
    "    \n",
    "    print(f\"UC Davis - Found {len(faculty_names)} faculty names.\")\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_generic_faculty(url):\n",
    "    # Generic scraping function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            text = link.get_text(strip=True)\n",
    "            if is_valid_name(text):\n",
    "                faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "# Mapping URLs to their respective scraping functions\n",
    "scraping_functions = {\n",
    "    \"https://economics.harvard.edu/faculty\": scrape_harvard_faculty,\n",
    "    \"https://economics.mit.edu/people/faculty\": scrape_mit_faculty,\n",
    "    \"https://econ.berkeley.edu/people/faculty\": scrape_berkeley_faculty,\n",
    "    \"https://economics.uchicago.edu/people/faculty\": scrape_uchicago_faculty,\n",
    "    \"https://economics.princeton.edu/people/\": scrape_princeton_faculty,\n",
    "    \"https://economics.stanford.edu/people/faculty\": scrape_stanford_faculty,\n",
    "    \"https://economics.yale.edu/people\": scrape_yale_faculty,\n",
    "    \"https://econ.columbia.edu/faculty/\": scrape_columbia_faculty,\n",
    "    \"https://as.nyu.edu/departments/econ/faculty.html\": scrape_nyu_faculty,\n",
    "    \"https://economics.sas.upenn.edu/people/faculty\": scrape_penn_faculty,\n",
    "    \"https://economics.brown.edu/people/faculty\": scrape_brown_faculty,\n",
    "    \"https://www.bu.edu/econ/people/faculty/\": scrape_bu_faculty,\n",
    "    \"https://dornsife.usc.edu/econ/faculty/\": scrape_usc_faculty,\n",
    "    \"https://lsa.umich.edu/econ/people/faculty.html\": scrape_michigan_faculty,\n",
    "    \"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\": scrape_ucsd_faculty,\n",
    "    \"https://economics.dartmouth.edu/people\": scrape_dartmouth_faculty,\n",
    "    \"https://economics.northwestern.edu/people/faculty/\": scrape_northwestern_faculty,\n",
    "    \"https://economics.ucla.edu/faculty/ladder\": scrape_ucla_faculty,\n",
    "    \"https://business.columbia.edu/faculty/divisions/finance/faculty\": scrape_columbiabusiness_faculty,\n",
    "    \"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\": scrape_bc_faculty,\n",
    "    \"https://econ.msu.edu/about/directory?employments=24a15d4c-1460-48c4-8714-e44066304ad0&letter=\": scrape_msu_faculty,\n",
    "    \"https://economics.cornell.edu/faculty\": scrape_cornell_faculty,\n",
    "    \"https://econ.wisc.edu/faculty/\": scrape_wisconsin_faculty,\n",
    "    \"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\": scrape_duke_faculty,\n",
    "    \"https://economics.ucdavis.edu/people/faculty\": scrape_ucdavis_faculty\n",
    "}\n",
    "\n",
    "# Collect all faculty names in a list\n",
    "all_faculty = []\n",
    "scraping_results = []\n",
    "\n",
    "for url in faculty_urls:\n",
    "    university_match = re.findall(r'https?://(?:www\\.)?([^/]+)/', url)\n",
    "    university_name = university_match[0].replace('www.', '') if university_match else 'Unknown'\n",
    "    print(f\"\\nScraping faculty from: {url}\")\n",
    "    try:\n",
    "        if url in scraping_functions:\n",
    "            scrape_function = scraping_functions[url]\n",
    "        else:\n",
    "            scrape_function = scrape_generic_faculty\n",
    "            print(\"Using generic scraping function.\")\n",
    "        faculty_names = scrape_function(url)\n",
    "        num_names = len(faculty_names)\n",
    "        print(f\"Successfully scraped {num_names} faculty names from {university_name}.\")\n",
    "        if num_names == 0:\n",
    "            print(f\"Warning: No faculty names found for {university_name}.\")\n",
    "            scraping_results.append({'University': university_name, 'URL': url, 'Status': 'No names found'})\n",
    "        else:\n",
    "            scraping_results.append({'University': university_name, 'URL': url, 'Status': f'{num_names} names found'})\n",
    "            for name in faculty_names:\n",
    "                all_faculty.append({'University': university_name, 'Faculty Name': name})\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {university_name}: {e}\")\n",
    "        scraping_results.append({'University': university_name, 'URL': url, 'Status': f'Error: {e}'})\n",
    "    time.sleep(1)  # Be polite to the servers\n",
    "\n",
    "# Create a DataFrame with all faculty names\n",
    "df_all_faculty = pd.DataFrame(all_faculty)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nAll Faculty Names Collected:\")\n",
    "print(df_all_faculty)\n",
    "\n",
    "# Display a summary report\n",
    "print(\"\\nScraping Summary Report:\")\n",
    "df_scraping_results = pd.DataFrame(scraping_results)\n",
    "print(df_scraping_results)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "# df_all_faculty.to_csv('faculty_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University</th>\n",
       "      <th>Faculty Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>econ.wisc.edu</td>\n",
       "      <td>Miguel Acosta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>econ.wisc.edu</td>\n",
       "      <td>Naoki Aizawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>econ.wisc.edu</td>\n",
       "      <td>Simeon Alder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>econ.wisc.edu</td>\n",
       "      <td>Panle Barwick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>econ.wisc.edu</td>\n",
       "      <td>Benjamin Bernard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>economics.ucdavis.edu</td>\n",
       "      <td>Jenna Stearns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>economics.ucdavis.edu</td>\n",
       "      <td>Derek Stimel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>economics.ucdavis.edu</td>\n",
       "      <td>Deborah Swenson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>economics.ucdavis.edu</td>\n",
       "      <td>Takuya Ura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>economics.ucdavis.edu</td>\n",
       "      <td>Janine Wilson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               University      Faculty Name\n",
       "0           econ.wisc.edu     Miguel Acosta\n",
       "1           econ.wisc.edu      Naoki Aizawa\n",
       "2           econ.wisc.edu      Simeon Alder\n",
       "3           econ.wisc.edu     Panle Barwick\n",
       "4           econ.wisc.edu  Benjamin Bernard\n",
       "..                    ...               ...\n",
       "87  economics.ucdavis.edu     Jenna Stearns\n",
       "88  economics.ucdavis.edu      Derek Stimel\n",
       "89  economics.ucdavis.edu   Deborah Swenson\n",
       "90  economics.ucdavis.edu        Takuya Ura\n",
       "91  economics.ucdavis.edu     Janine Wilson\n",
       "\n",
       "[92 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_faculty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
