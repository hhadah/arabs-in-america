{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting trio~=0.17\n",
      "  Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websocket-client~=1.8\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/anaconda3/lib/python3.9/site-packages (from selenium) (2022.9.24)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.11)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting typing_extensions~=4.9\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: idna in /usr/local/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Collecting attrs>=23.2.0\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting outcome\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting sniffio>=1.3.0\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/anaconda3/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: websocket-client, typing_extensions, sniffio, h11, exceptiongroup, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 0.58.0\n",
      "    Uninstalling websocket-client-0.58.0:\n",
      "      Successfully uninstalled websocket-client-0.58.0\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Uninstalling attrs-21.4.0:\n",
      "      Successfully uninstalled attrs-21.4.0\n",
      "Successfully installed attrs-24.2.0 exceptiongroup-1.2.2 h11-0.14.0 outcome-1.3.0.post0 selenium-4.26.1 sniffio-1.3.1 trio-0.27.0 trio-websocket-0.11.1 typing_extensions-4.12.2 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# !pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_urls = [\n",
    "\"https://economics.harvard.edu/faculty\",\n",
    "\"https://economics.mit.edu/people/faculty\",\n",
    "\"https://econ.berkeley.edu/people/faculty\",\n",
    "\"https://economics.uchicago.edu/people/faculty\",\n",
    "\"https://economics.princeton.edu/people/\",\n",
    "\"https://economics.stanford.edu/people/faculty\",\n",
    "\"https://economics.yale.edu/people\",\n",
    "\"https://econ.columbia.edu/faculty/\",\n",
    "\"https://as.nyu.edu/departments/econ/faculty.html\",\n",
    "\"https://economics.sas.upenn.edu/people/faculty\",\n",
    "\"https://economics.brown.edu/people/faculty\",\n",
    "\"https://www.bu.edu/econ/people/faculty/\",\n",
    "\"https://dornsife.usc.edu/econ/faculty/\",\n",
    "\"https://lsa.umich.edu/econ/people/faculty.html\",\n",
    "\"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\",\n",
    "\"https://economics.dartmouth.edu/people\",\n",
    "\"https://economics.northwestern.edu/people/faculty/\",\n",
    "\"https://economics.ucla.edu/faculty/ladder\",\n",
    "\"https://business.columbia.edu/faculty/divisions/finance/faculty\",\n",
    "\"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\",\n",
    "\"https://econ.msu.edu/about/directory\",\n",
    "\"https://economics.cornell.edu/faculty\",\n",
    "\"https://econ.wisc.edu/faculty/\",\n",
    "\"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\",\n",
    "\"https://economics.ucdavis.edu/people/faculty\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvard University - Found 58 faculty names.\n",
      "                        Faculty Name\n",
      "0                     Claudia Goldin\n",
      "1                         Shengwu Li\n",
      "2                      Benjamin Enke\n",
      "3                  Myrto Kalouptsidi\n",
      "4                  Gabriel Kreindler\n",
      "5                       Emily Palmer\n",
      "6                Stefanie Stantcheva\n",
      "7                       Jason Furman\n",
      "8                         Pol Antràs\n",
      "9                         Emily Sall\n",
      "10                      Melissa Dell\n",
      "11                        Elie Tamer\n",
      "12                    Eric Unverzagt\n",
      "13                    Kenneth Rogoff\n",
      "14                  Lawrence Summers\n",
      "15                 Augustin Bergeron\n",
      "16                    Marina Bisogno\n",
      "17                       James Stock\n",
      "18                       Amartya Sen\n",
      "19                      Jeremy Stein\n",
      "20                    Edward Glaeser\n",
      "21                 Christopher Foote\n",
      "22                  Personal Webpage\n",
      "23  Richard Freeman's NBER Home page\n",
      "24                      Jamie Murray\n",
      "25                       Oliver Hart\n",
      "26                       Marc Melitz\n",
      "27                   Eliza Rakaseder\n",
      "28                 Benjamin Friedman\n",
      "29                     Polina Barker\n",
      "30                       Jerry Green\n",
      "31                     Lawrence Katz\n",
      "32                    Amanda Pallais\n",
      "33                     Jesse Shapiro\n",
      "34                         Robin Lee\n",
      "35                 Tomasz Strzalecki\n",
      "36                       Karen Dynan\n",
      "37                     Oleg Itskhoki\n",
      "38                        David Yang\n",
      "39                     Jeffrey Miron\n",
      "40                     Neil Shephard\n",
      "41                     Ludwig Straub\n",
      "42            Gabriel Chodorow-Reich\n",
      "43                    Ursula Ferraro\n",
      "44                     David Laibson\n",
      "45                   Stephen Marglin\n",
      "46                     Matthew Rabin\n",
      "47                      Roland Fryer\n",
      "48                    Davide Viviano\n",
      "49                     Xavier Gabaix\n",
      "50                   Andrei Shleifer\n",
      "51                        Raj Chetty\n",
      "52                   Elhanan Helpman\n",
      "53                       Emily Breza\n",
      "54                      Robert Barro\n",
      "55                      Ann Richards\n",
      "56                      Mack Carroll\n",
      "57                       Ariel Pakes\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the URL for Harvard's economics faculty page\n",
    "url = \"https://economics.harvard.edu/faculty\"\n",
    "\n",
    "def scrape_harvard_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Scrape the A to L tab content\n",
    "        tab_a_to_l = soup.find(\"div\", id=\"widget-1\")\n",
    "        if tab_a_to_l:\n",
    "            for name in tab_a_to_l.find_all(\"a\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                # Filter based on structure to capture only names\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "        \n",
    "        # Scrape the M to Z tab content\n",
    "        tab_m_to_z = soup.find(\"div\", id=\"widget-2\")\n",
    "        if tab_m_to_z:\n",
    "            for name in tab_m_to_z.find_all(\"a\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                # Filter based on structure to capture only names\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "        \n",
    "        print(f\"Harvard University - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, mostly capitalized\n",
    "    if re.match(r\"^[A-Z][a-z]+\\s[A-Z][a-z]+\", text):  # Simple check for capitalized first and last name\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\", \"Bureau\", \"Research\"]\n",
    "        if not any(keyword in text for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scrape the faculty names\n",
    "harvard_faculty = scrape_harvard_faculty(url)\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "unique_names = list(set(harvard_faculty))\n",
    "\n",
    "# Create a DataFrame with only the names\n",
    "df_harvard_faculty = pd.DataFrame(unique_names, columns=[\"Faculty Name\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_harvard_faculty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIT - Found 46 faculty names.\n",
      "            Faculty Name\n",
      "0         Drew Fudenberg\n",
      "1       Abhijit Banerjee\n",
      "2        Jonathan Gruber\n",
      "3          Glenn Ellison\n",
      "4         Stephen Morris\n",
      "5           Esther Duflo\n",
      "6        Frank Schilbach\n",
      "7           Josh Angrist\n",
      "8           Parag Pathak\n",
      "9         Isaiah Andrews\n",
      "10   Richard Schmalensee\n",
      "11     Ricardo Caballero\n",
      "12         Martin Beraja\n",
      "13        Daron Acemoglu\n",
      "14         Drazen Prelec\n",
      "15        Jeffrey Harris\n",
      "16        Anna Mikusheva\n",
      "17    Alexander Wolitzky\n",
      "18           Tobias Salz\n",
      "19         Whitney Newey\n",
      "20        Benjamin Olken\n",
      "21   Victor Chernozhukov\n",
      "22         Peter Diamond\n",
      "23           Peter Temin\n",
      "24              Ian Ball\n",
      "25  Sendhil Mullainathan\n",
      "26        Dave Donaldson\n",
      "27       Stanley Fischer\n",
      "28        Nina Roussille\n",
      "29       Bengt Holmström\n",
      "30           David Atkin\n",
      "31   Sara Fisher Ellison\n",
      "32         Jacob Moscona\n",
      "33        Robert Gibbons\n",
      "34      Ashesh Rambachan\n",
      "35        Christian Wolf\n",
      "36         James Poterba\n",
      "37       Arnaud Costinot\n",
      "38      Michael Whinston\n",
      "39        Muhamet Yildiz\n",
      "40        Alberto Abadie\n",
      "41     Nathaniel Hendren\n",
      "42       Amy Finkelstein\n",
      "43         Jerry Hausman\n",
      "44        Nikhil Agarwal\n",
      "45     Olivier Blanchard\n"
     ]
    }
   ],
   "source": [
    "# Define the URL for MIT's economics faculty page\n",
    "url = \"https://economics.mit.edu/people/faculty\"\n",
    "\n",
    "def scrape_mit_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Target the specific container for faculty profiles\n",
    "        container = soup.find(\"div\", class_=\"view-content faculty-landing__items\")\n",
    "        if container:\n",
    "            # Find each name within <h3 class=\"profile-teaser__name\"> tags\n",
    "            for name in container.find_all(\"h3\", class_=\"profile-teaser__name\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                # Apply filter to ensure it's a valid name\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "        \n",
    "        print(f\"MIT - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Reuse the `is_valid_name` function from Harvard scraping script\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, mostly capitalized\n",
    "    if re.match(r\"^[A-Z][a-z]+\\s[A-Z][a-z]+\", text):  # Simple check for capitalized first and last name\n",
    "        excluded_keywords = [\"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\", \"Bureau\", \"Research\"]\n",
    "        if not any(keyword in text for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scrape the faculty names from MIT\n",
    "mit_faculty = scrape_mit_faculty(url)\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "unique_names = list(set(mit_faculty))\n",
    "\n",
    "# Create a DataFrame with only the names\n",
    "df_mit_faculty = pd.DataFrame(unique_names, columns=[\"Faculty Name\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_mit_faculty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC Berkeley - Found 53 faculty names.\n",
      "                  Faculty Name\n",
      "0   Valenzuela-Stookey, Quitzé\n",
      "1               Barahona, Nano\n",
      "2               Roland, Gérard\n",
      "3              Backus, Matthew\n",
      "4              Faber, Benjamin\n",
      "5               Miguel, Edward\n",
      "6             Rothstein, Jesse\n",
      "7               Kline, Patrick\n",
      "8               Goldberg, Lisa\n",
      "9              Munoz, Mathilde\n",
      "10           Auerbach, Alan J.\n",
      "11                Sraer, David\n",
      "12             Moretti, Enrico\n",
      "13          Schoefer, Benjamin\n",
      "14              Semenova, Vira\n",
      "15              Hoynes, Hilary\n",
      "16             Schaab, Andreas\n",
      "17        Walters, Christopher\n",
      "18                 Xu , Chenzi\n",
      "19            Caldwell, Sydnee\n",
      "20              Kariv, Shachar\n",
      "21              Shannon, Chris\n",
      "22             Zucman, Gabriel\n",
      "23                Walker, Reed\n",
      "24                  Kawai, Kei\n",
      "25            Jansson, Michael\n",
      "26       Hermalin, Benjamin E.\n",
      "27         DellaVigna, Stefano\n",
      "28                Ergin, Haluk\n",
      "29                Kolstad, Jon\n",
      "30                 Rao, Gautam\n",
      "31          Malmendier, Ulrike\n",
      "32            Graham, Bryan S.\n",
      "33            Finan, Frederico\n",
      "34             Shapiro, Joseph\n",
      "35               Nakamura, Emi\n",
      "36          DeLong, J.Bradford\n",
      "37          Eichengreen, Barry\n",
      "38  Gourinchas, Pierre-Olivier\n",
      "39                Edlin, Aaron\n",
      "40           Taubinsky, Dmitry\n",
      "41              Stein, Carolyn\n",
      "42             Gaubert, Cecile\n",
      "43               Kaur, Supreet\n",
      "44              Saez, Emmanuel\n",
      "45              Steinsson, Jón\n",
      "46                 Handel, Ben\n",
      "47         Echenique, Federico\n",
      "48     Rodríguez-Clare, Andrés\n",
      "49               Pouzo, Demian\n",
      "50                  Lian, Chen\n",
      "51                Yagan, Danny\n",
      "52        Gorodnichenko, Yuriy\n"
     ]
    }
   ],
   "source": [
    "# Define the URL for UC Berkeley's economics faculty page\n",
    "url = \"https://econ.berkeley.edu/people/faculty\"\n",
    "\n",
    "def scrape_berkeley_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Locate all faculty profiles\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"display-name\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UC Berkeley - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Scrape the faculty names from Berkeley\n",
    "berkeley_faculty = scrape_berkeley_faculty(url)\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "unique_names = list(set(berkeley_faculty))\n",
    "\n",
    "# Create a DataFrame with only the names\n",
    "df_berkeley_faculty = pd.DataFrame(unique_names, columns=[\"Faculty Name\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_berkeley_faculty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more faculty entries found.\n",
      "University of Chicago - Found 38 faculty names.\n",
      "              Faculty Name\n",
      "0             Ufuk Akcigit\n",
      "1        Fernando  Alvarez\n",
      "2        Stéphane Bonhomme\n",
      "3          Benjamin Brooks\n",
      "4          Christina Brown\n",
      "5       Leonardo  Bursztyn\n",
      "6        Manasi  Deshpande\n",
      "7           David Galenson\n",
      "8          Mikhail Golosov\n",
      "9       Michael Greenstone\n",
      "10       Lars Peter Hansen\n",
      "11     Arnold C. Harberger\n",
      "12       James J.  Heckman\n",
      "13            Ali Hortacsu\n",
      "14             Greg Kaplan\n",
      "15             Anne Karing\n",
      "16          Michael Kremer\n",
      "17         Thibaut Lamadon\n",
      "18            Steve Levitt\n",
      "19               John List\n",
      "20           Magne Mogstad\n",
      "21          Casey Mulligan\n",
      "22         Kevin M. Murphy\n",
      "23        Roger B. Myerson\n",
      "24          Derek A.  Neal\n",
      "25        Kirill Ponomarev\n",
      "26             Doron Ravid\n",
      "27          Philip J. Reny\n",
      "28            Eric Richert\n",
      "29             Joseph Root\n",
      "30               Evan Rose\n",
      "31  Esteban Rossi-Hansberg\n",
      "32            Azeem Shaikh\n",
      "33           Robert Shimer\n",
      "34         Nancy L. Stokey\n",
      "35       Max Tabord-Meehan\n",
      "36   Alexander Torgovitsky\n",
      "37            Harald Uhlig\n"
     ]
    }
   ],
   "source": [
    "# Base URL for the University of Chicago's economics faculty directory\n",
    "base_url = \"https://economics.uchicago.edu/people/faculty\"\n",
    "faculty_names = []\n",
    "\n",
    "# Iterate through the pages\n",
    "page = 0\n",
    "while True:\n",
    "    # Modify the URL for each page\n",
    "    url = f\"{base_url}?title=all&name=all&page={page}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page}. Status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all faculty entries on the current page\n",
    "    rows = soup.select(\".views-row\")\n",
    "    if not rows:\n",
    "        print(\"No more faculty entries found.\")\n",
    "        break\n",
    "\n",
    "    # Extract faculty names from each entry\n",
    "    for row in rows:\n",
    "        name_tag = row.select_one(\"h2.no-tags a\")\n",
    "        if name_tag:\n",
    "            faculty_names.append(name_tag.text.strip())\n",
    "\n",
    "    # Go to the next page\n",
    "    page += 1\n",
    "\n",
    "# Convert the list of faculty names to a DataFrame\n",
    "df_faculty = pd.DataFrame(faculty_names, columns=[\"Faculty Name\"])\n",
    "print(f\"University of Chicago - Found {len(df_faculty)} faculty names.\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_faculty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total faculty members found: 84\n",
      "\n",
      "Faculty Names:\n",
      "1. Alicia Adsera\n",
      "2. Mark A. Aguiar\n",
      "3. Yacine Ait-Sahalia\n",
      "4. Caio Almeida\n",
      "5. Roland Benabou\n",
      "6. Swati Bhatt\n",
      "7. Zachary Bleemer\n",
      "8. Alan S. Blinder\n",
      "9. Leah Platt Boustan\n",
      "10. Markus Brunnermeier\n",
      "11. Smita Brunnermeier\n",
      "12. Nicholas Buchholz\n",
      "13. Matias Cattaneo\n",
      "14. Sylvain Chassang\n",
      "15. Daniel Chen\n",
      "16. Natalie Cox\n",
      "17. Janet M. Currie\n",
      "18. Ellora Derenoncourt\n",
      "19. Pascaline Dupas\n",
      "20. Jianqing Fan\n",
      "21. Farid Farrokhi\n",
      "22. Mira Frick\n",
      "23. Thomas Fujiwara\n",
      "24. John R. Grigsby\n",
      "25. Gene M. Grossman\n",
      "26. Faruk R. Gul\n",
      "27. Kate Ho\n",
      "28. Bo E. Honoré\n",
      "29. Ryota Iijima\n",
      "30. Seema Jayachandran\n",
      "31. Adam Kapor\n",
      "32. Jakub Kastl\n",
      "33. Nobuhiro Kiyotaki\n",
      "34. Henrik J. Kleven\n",
      "35. Michal Kolesár\n",
      "36. Ilyana Kuziemko\n",
      "37. David Lee\n",
      "38. Moritz Lenel\n",
      "39. Thomas C. Leonard\n",
      "40. Ernest Liu\n",
      "41. Alessandro Lizzeri\n",
      "42. W. Bentley MacLeod\n",
      "43. Atif Mian\n",
      "44. Eduardo Morales\n",
      "45. Xiaosheng Mu\n",
      "46. Ulrich K. Müller\n",
      "47. Kelly Noonan\n",
      "48. Pietro Ortoleva\n",
      "49. Jonathan Payne\n",
      "50. Wolfgang Pesendorfer\n",
      "51. Mikkel Plagborg-Moller\n",
      "52. Stephen J. Redding\n",
      "53. Richard Rogerson\n",
      "54. Cecilia Rouse\n",
      "55. Ayşegül Sahin\n",
      "56. Batchimeg Sambalaibat\n",
      "57. Karthik Sastry\n",
      "58. Maria Micaela Sviatschi\n",
      "59. Can Urgun\n",
      "60. Giovanni (Gianluca) Violante\n",
      "61. Leonard Wantchekon\n",
      "62. Mark W. Watson\n",
      "63. Silvia Weyerbrock\n",
      "64. Wei Xiong\n",
      "65. Leeat Yariv\n",
      "66. Motohiro Yogo\n",
      "67. Iqbal Zaidi\n",
      "68. Owen Zidar\n",
      "69. Dilip Abreu\n",
      "70. Orley Ashenfelter\n",
      "71. Anne C. Case\n",
      "72. Gregory C. Chow\n",
      "73. Sir Angus S. Deaton\n",
      "74. Avinash K. Dixit\n",
      "75. Henry S. Farber\n",
      "76. Paul Krugman\n",
      "77. Burton G. Malkiel\n",
      "78. Richard E. Quandt\n",
      "79. Thomas Romer\n",
      "80. Harvey S. Rosen\n",
      "81. Michael Rothschild\n",
      "82. José A. Scheinkman\n",
      "83. Harold T. Shapiro\n",
      "84. Christopher A. Sims\n"
     ]
    }
   ],
   "source": [
    "# Function to print names in a formatted way\n",
    "def print_faculty_names(names):\n",
    "    print(f\"Total faculty members found: {len(names)}\")\n",
    "    print(\"\\nFaculty Names:\")\n",
    "    for i, name in enumerate(names, 1):\n",
    "        print(f\"{i}. {name}\")\n",
    "\n",
    "# Extract faculty names\n",
    "faculty_names = extract_faculty_names(html_content)\n",
    "print_faculty_names(faculty_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping faculty from: https://economics.harvard.edu/faculty\n",
      "Successfully scraped 64 faculty names from economics.harvard.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.mit.edu/people/faculty\n",
      "Successfully scraped 52 faculty names from economics.mit.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.berkeley.edu/people/faculty\n",
      "UC Berkeley - Found 53 faculty names.\n",
      "Successfully scraped 53 faculty names from econ.berkeley.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.uchicago.edu/people/faculty\n",
      "Successfully scraped 34 faculty names from economics.uchicago.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.princeton.edu/people/\n",
      "Total faculty members found: 84\n",
      "\n",
      "Faculty Names:\n",
      "1. Alicia Adsera\n",
      "2. Mark A. Aguiar\n",
      "3. Yacine Ait-Sahalia\n",
      "4. Caio Almeida\n",
      "5. Roland Benabou\n",
      "6. Swati Bhatt\n",
      "7. Zachary Bleemer\n",
      "8. Alan S. Blinder\n",
      "9. Leah Platt Boustan\n",
      "10. Markus Brunnermeier\n",
      "11. Smita Brunnermeier\n",
      "12. Nicholas Buchholz\n",
      "13. Matias Cattaneo\n",
      "14. Sylvain Chassang\n",
      "15. Daniel Chen\n",
      "16. Natalie Cox\n",
      "17. Janet M. Currie\n",
      "18. Ellora Derenoncourt\n",
      "19. Pascaline Dupas\n",
      "20. Jianqing Fan\n",
      "21. Farid Farrokhi\n",
      "22. Mira Frick\n",
      "23. Thomas Fujiwara\n",
      "24. John R. Grigsby\n",
      "25. Gene M. Grossman\n",
      "26. Faruk R. Gul\n",
      "27. Kate Ho\n",
      "28. Bo E. Honoré\n",
      "29. Ryota Iijima\n",
      "30. Seema Jayachandran\n",
      "31. Adam Kapor\n",
      "32. Jakub Kastl\n",
      "33. Nobuhiro Kiyotaki\n",
      "34. Henrik J. Kleven\n",
      "35. Michal Kolesár\n",
      "36. Ilyana Kuziemko\n",
      "37. David Lee\n",
      "38. Moritz Lenel\n",
      "39. Thomas C. Leonard\n",
      "40. Ernest Liu\n",
      "41. Alessandro Lizzeri\n",
      "42. W. Bentley MacLeod\n",
      "43. Atif Mian\n",
      "44. Eduardo Morales\n",
      "45. Xiaosheng Mu\n",
      "46. Ulrich K. Müller\n",
      "47. Kelly Noonan\n",
      "48. Pietro Ortoleva\n",
      "49. Jonathan Payne\n",
      "50. Wolfgang Pesendorfer\n",
      "51. Mikkel Plagborg-Moller\n",
      "52. Stephen J. Redding\n",
      "53. Richard Rogerson\n",
      "54. Cecilia Rouse\n",
      "55. Ayşegül Sahin\n",
      "56. Batchimeg Sambalaibat\n",
      "57. Karthik Sastry\n",
      "58. Maria Micaela Sviatschi\n",
      "59. Can Urgun\n",
      "60. Giovanni (Gianluca) Violante\n",
      "61. Leonard Wantchekon\n",
      "62. Mark W. Watson\n",
      "63. Silvia Weyerbrock\n",
      "64. Wei Xiong\n",
      "65. Leeat Yariv\n",
      "66. Motohiro Yogo\n",
      "67. Iqbal Zaidi\n",
      "68. Owen Zidar\n",
      "69. Dilip Abreu\n",
      "70. Orley Ashenfelter\n",
      "71. Anne C. Case\n",
      "72. Gregory C. Chow\n",
      "73. Sir Angus S. Deaton\n",
      "74. Avinash K. Dixit\n",
      "75. Henry S. Farber\n",
      "76. Paul Krugman\n",
      "77. Burton G. Malkiel\n",
      "78. Richard E. Quandt\n",
      "79. Thomas Romer\n",
      "80. Harvey S. Rosen\n",
      "81. Michael Rothschild\n",
      "82. José A. Scheinkman\n",
      "83. Harold T. Shapiro\n",
      "84. Christopher A. Sims\n",
      "Error scraping economics.princeton.edu: object of type 'NoneType' has no len()\n",
      "\n",
      "Scraping faculty from: https://economics.stanford.edu/people/faculty\n",
      "Successfully scraped 39 faculty names from economics.stanford.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.yale.edu/people\n",
      "Successfully scraped 0 faculty names from economics.yale.edu.\n",
      "Warning: No faculty names found for economics.yale.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.columbia.edu/faculty/\n",
      "Successfully scraped 0 faculty names from econ.columbia.edu.\n",
      "Warning: No faculty names found for econ.columbia.edu.\n",
      "\n",
      "Scraping faculty from: https://as.nyu.edu/departments/econ/faculty.html\n",
      "Successfully scraped 0 faculty names from as.nyu.edu.\n",
      "Warning: No faculty names found for as.nyu.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.sas.upenn.edu/people/faculty\n",
      "Successfully scraped 0 faculty names from economics.sas.upenn.edu.\n",
      "Warning: No faculty names found for economics.sas.upenn.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.brown.edu/people/faculty\n",
      "Successfully scraped 0 faculty names from economics.brown.edu.\n",
      "Warning: No faculty names found for economics.brown.edu.\n",
      "\n",
      "Scraping faculty from: https://www.bu.edu/econ/people/faculty/\n",
      "Successfully scraped 0 faculty names from bu.edu.\n",
      "Warning: No faculty names found for bu.edu.\n",
      "\n",
      "Scraping faculty from: https://dornsife.usc.edu/econ/faculty/\n",
      "Successfully scraped 0 faculty names from dornsife.usc.edu.\n",
      "Warning: No faculty names found for dornsife.usc.edu.\n",
      "\n",
      "Scraping faculty from: https://lsa.umich.edu/econ/people/faculty.html\n",
      "Successfully scraped 0 faculty names from lsa.umich.edu.\n",
      "Warning: No faculty names found for lsa.umich.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\n",
      "Successfully scraped 0 faculty names from economics.ucsd.edu.\n",
      "Warning: No faculty names found for economics.ucsd.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.dartmouth.edu/people\n",
      "Successfully scraped 0 faculty names from economics.dartmouth.edu.\n",
      "Warning: No faculty names found for economics.dartmouth.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.northwestern.edu/people/faculty/\n",
      "Successfully scraped 0 faculty names from economics.northwestern.edu.\n",
      "Warning: No faculty names found for economics.northwestern.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.ucla.edu/faculty/ladder\n",
      "Successfully scraped 0 faculty names from economics.ucla.edu.\n",
      "Warning: No faculty names found for economics.ucla.edu.\n",
      "\n",
      "Scraping faculty from: https://business.columbia.edu/faculty/divisions/finance/faculty\n",
      "Successfully scraped 0 faculty names from business.columbia.edu.\n",
      "Warning: No faculty names found for business.columbia.edu.\n",
      "\n",
      "Scraping faculty from: https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\n",
      "Successfully scraped 0 faculty names from bc.edu.\n",
      "Warning: No faculty names found for bc.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.msu.edu/about/directory\n",
      "Successfully scraped 0 faculty names from econ.msu.edu.\n",
      "Warning: No faculty names found for econ.msu.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.cornell.edu/faculty\n",
      "Successfully scraped 0 faculty names from economics.cornell.edu.\n",
      "Warning: No faculty names found for economics.cornell.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.wisc.edu/faculty/\n",
      "Successfully scraped 0 faculty names from econ.wisc.edu.\n",
      "Warning: No faculty names found for econ.wisc.edu.\n",
      "\n",
      "Scraping faculty from: https://econ.duke.edu/people/other-faculty/regular-rank-faculty\n",
      "Successfully scraped 0 faculty names from econ.duke.edu.\n",
      "Warning: No faculty names found for econ.duke.edu.\n",
      "\n",
      "Scraping faculty from: https://economics.ucdavis.edu/people/faculty\n",
      "Successfully scraped 0 faculty names from economics.ucdavis.edu.\n",
      "Warning: No faculty names found for economics.ucdavis.edu.\n",
      "\n",
      "All Faculty Names Collected:\n",
      "                 University       Faculty Name\n",
      "0     economics.harvard.edu         Pol Antràs\n",
      "1     economics.harvard.edu       Mack Carroll\n",
      "2     economics.harvard.edu       Robert Barro\n",
      "3     economics.harvard.edu  Augustin Bergeron\n",
      "4     economics.harvard.edu        Emily Breza\n",
      "..                      ...                ...\n",
      "237  economics.stanford.edu         Ilya Segal\n",
      "238  economics.stanford.edu       Isaac Sorkin\n",
      "239  economics.stanford.edu     John B. Taylor\n",
      "240  economics.stanford.edu   Alessandra Voena\n",
      "241  economics.stanford.edu        Frank Wolak\n",
      "\n",
      "[242 rows x 2 columns]\n",
      "\n",
      "Scraping Summary Report:\n",
      "                    University  \\\n",
      "0        economics.harvard.edu   \n",
      "1            economics.mit.edu   \n",
      "2            econ.berkeley.edu   \n",
      "3       economics.uchicago.edu   \n",
      "4      economics.princeton.edu   \n",
      "5       economics.stanford.edu   \n",
      "6           economics.yale.edu   \n",
      "7            econ.columbia.edu   \n",
      "8                   as.nyu.edu   \n",
      "9      economics.sas.upenn.edu   \n",
      "10         economics.brown.edu   \n",
      "11                      bu.edu   \n",
      "12            dornsife.usc.edu   \n",
      "13               lsa.umich.edu   \n",
      "14          economics.ucsd.edu   \n",
      "15     economics.dartmouth.edu   \n",
      "16  economics.northwestern.edu   \n",
      "17          economics.ucla.edu   \n",
      "18       business.columbia.edu   \n",
      "19                      bc.edu   \n",
      "20                econ.msu.edu   \n",
      "21       economics.cornell.edu   \n",
      "22               econ.wisc.edu   \n",
      "23               econ.duke.edu   \n",
      "24       economics.ucdavis.edu   \n",
      "\n",
      "                                                  URL  \\\n",
      "0               https://economics.harvard.edu/faculty   \n",
      "1            https://economics.mit.edu/people/faculty   \n",
      "2            https://econ.berkeley.edu/people/faculty   \n",
      "3       https://economics.uchicago.edu/people/faculty   \n",
      "4             https://economics.princeton.edu/people/   \n",
      "5       https://economics.stanford.edu/people/faculty   \n",
      "6                   https://economics.yale.edu/people   \n",
      "7                  https://econ.columbia.edu/faculty/   \n",
      "8    https://as.nyu.edu/departments/econ/faculty.html   \n",
      "9      https://economics.sas.upenn.edu/people/faculty   \n",
      "10         https://economics.brown.edu/people/faculty   \n",
      "11            https://www.bu.edu/econ/people/faculty/   \n",
      "12             https://dornsife.usc.edu/econ/faculty/   \n",
      "13     https://lsa.umich.edu/econ/people/faculty.html   \n",
      "14  https://economics.ucsd.edu/faculty-and-researc...   \n",
      "15             https://economics.dartmouth.edu/people   \n",
      "16  https://economics.northwestern.edu/people/facu...   \n",
      "17          https://economics.ucla.edu/faculty/ladder   \n",
      "18  https://business.columbia.edu/faculty/division...   \n",
      "19  https://www.bc.edu/content/bc-web/schools/morr...   \n",
      "20               https://econ.msu.edu/about/directory   \n",
      "21              https://economics.cornell.edu/faculty   \n",
      "22                     https://econ.wisc.edu/faculty/   \n",
      "23  https://econ.duke.edu/people/other-faculty/reg...   \n",
      "24       https://economics.ucdavis.edu/people/faculty   \n",
      "\n",
      "                                           Status  \n",
      "0                                  64 names found  \n",
      "1                                  52 names found  \n",
      "2                                  53 names found  \n",
      "3                                  34 names found  \n",
      "4   Error: object of type 'NoneType' has no len()  \n",
      "5                                  39 names found  \n",
      "6                                  No names found  \n",
      "7                                  No names found  \n",
      "8                                  No names found  \n",
      "9                                  No names found  \n",
      "10                                 No names found  \n",
      "11                                 No names found  \n",
      "12                                 No names found  \n",
      "13                                 No names found  \n",
      "14                                 No names found  \n",
      "15                                 No names found  \n",
      "16                                 No names found  \n",
      "17                                 No names found  \n",
      "18                                 No names found  \n",
      "19                                 No names found  \n",
      "20                                 No names found  \n",
      "21                                 No names found  \n",
      "22                                 No names found  \n",
      "23                                 No names found  \n",
      "24                                 No names found  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# List of faculty URLs\n",
    "faculty_urls = [\n",
    "    \"https://economics.harvard.edu/faculty\",\n",
    "    \"https://economics.mit.edu/people/faculty\",\n",
    "    \"https://econ.berkeley.edu/people/faculty\",\n",
    "    \"https://economics.uchicago.edu/people/faculty\",\n",
    "    \"https://economics.princeton.edu/people/\",\n",
    "    \"https://economics.stanford.edu/people/faculty\",\n",
    "    \"https://economics.yale.edu/people\",\n",
    "    \"https://econ.columbia.edu/faculty/\",\n",
    "    \"https://as.nyu.edu/departments/econ/faculty.html\",\n",
    "    \"https://economics.sas.upenn.edu/people/faculty\",\n",
    "    \"https://economics.brown.edu/people/faculty\",\n",
    "    \"https://www.bu.edu/econ/people/faculty/\",\n",
    "    \"https://dornsife.usc.edu/econ/faculty/\",\n",
    "    \"https://lsa.umich.edu/econ/people/faculty.html\",\n",
    "    \"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\",\n",
    "    \"https://economics.dartmouth.edu/people\",\n",
    "    \"https://economics.northwestern.edu/people/faculty/\",\n",
    "    \"https://economics.ucla.edu/faculty/ladder\",\n",
    "    \"https://business.columbia.edu/faculty/divisions/finance/faculty\",\n",
    "    \"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\",\n",
    "    \"https://econ.msu.edu/about/directory\",\n",
    "    \"https://economics.cornell.edu/faculty\",\n",
    "    \"https://econ.wisc.edu/faculty/\",\n",
    "    \"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\",\n",
    "    \"https://economics.ucdavis.edu/people/faculty\"\n",
    "]\n",
    "\n",
    "# Function to validate names\n",
    "def is_valid_name(text):\n",
    "    # Check for typical structure of a faculty name: at least two words, starting with uppercase letters\n",
    "    if re.match(r\"^[A-Z][a-zA-Z\\-\\'\\.]+\\s[A-Z][a-zA-Z\\-\\'\\.]+\", text):\n",
    "        # Exclude keywords that are unlikely to be part of a name\n",
    "        excluded_keywords = [\n",
    "            \"Institute\", \"Economy\", \"Website\", \"Center\", \"Program\",\n",
    "            \"Bureau\", \"Research\", \"Department\", \"Faculty\", \"Staff\",\n",
    "            \"Emeriti\", \"Adjunct\", \"Visiting\", \"Professor\", \"Lecturer\",\n",
    "            \"Assistant\", \"Associate\", \"Director\", \"Chair\", \"Affiliated\"\n",
    "        ]\n",
    "        if not any(keyword.lower() in text.lower() for keyword in excluded_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Scraping functions for each university\n",
    "\n",
    "def scrape_harvard_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for widget_id in [\"widget-1\", \"widget-2\"]:\n",
    "            tab = soup.find(\"div\", id=widget_id)\n",
    "            if tab:\n",
    "                for name in tab.find_all(\"a\"):\n",
    "                    text = name.get_text(strip=True)\n",
    "                    if is_valid_name(text):\n",
    "                        faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_mit_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        container = soup.find(\"div\", class_=\"view-content faculty-landing__items\")\n",
    "        if container:\n",
    "            for name in container.find_all(\"h3\", class_=\"profile-teaser__name\"):\n",
    "                text = name.get_text(strip=True)\n",
    "                if is_valid_name(text):\n",
    "                    faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_berkeley_faculty(url):\n",
    "    # Send a request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculty_names = []\n",
    "        \n",
    "        # Locate all faculty profiles\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"display-name\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                faculty_names.append(name)\n",
    "        \n",
    "        print(f\"UC Berkeley - Found {len(faculty_names)} faculty names.\")\n",
    "        return faculty_names\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_uchicago_faculty(url):\n",
    "    # Same as before\n",
    "    faculty_names = []\n",
    "    page = 0\n",
    "    while True:\n",
    "        page_url = f\"{url}?title=all&name=all&page={page}\"\n",
    "        response = requests.get(page_url)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        rows = soup.select(\".views-row\")\n",
    "        if not rows:\n",
    "            break\n",
    "        for row in rows:\n",
    "            name_tag = row.select_one(\"h2.no-tags a\")\n",
    "            if name_tag:\n",
    "                name = name_tag.text.strip()\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "        page += 1\n",
    "        time.sleep(1)  # Respectful scraping\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_princeton_faculty(url):\n",
    "    # Create BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all person divs\n",
    "    person_divs = soup.find_all('div', class_='person')\n",
    "    \n",
    "    # Extract names from h3 tags within person divs\n",
    "    faculty_names = []\n",
    "    for person in person_divs:\n",
    "        name = person.find('h3')\n",
    "        if name:\n",
    "            faculty_names.append(name.text.strip())\n",
    "    faculty_names= print_faculty_names(faculty_names)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_stanford_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('div', class_='views-row'):\n",
    "            name_tag = person.find('span', class_='field-content')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_yale_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"h3\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_columbia_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for faculty in soup.find_all('div', class_='person'):\n",
    "            name_tag = faculty.find('h3')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_nyu_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('div', class_='profile person'):\n",
    "            name_tag = person.find('h3', class_='profile__name')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_penn_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('div', class_='views-row'):\n",
    "            name_tag = person.find('div', class_='field-content')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_brown_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('div', class_='people__list-item'):\n",
    "            name_tag = person.find('h3', class_='people__name')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_bu_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('div', class_='person'):\n",
    "            name_tag = person.find('div', class_='person-name')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_usc_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for faculty in soup.find_all('div', class_='faculty-listing'):\n",
    "            name_tag = faculty.find('h3')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_michigan_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('div', class_='person-view'):\n",
    "            name_tag = person.find('h3')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucsd_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for profile in soup.find_all('div', class_='faculty-profile'):\n",
    "            name_tag = profile.find('h3')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_dartmouth_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('div', class_='views-row'):\n",
    "            name_tag = person.find('h2')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_northwestern_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h3', class_='people__name'):\n",
    "            name_tag = person.find('a')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucla_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h4', class_='people-list-item__name'):\n",
    "            name = person.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_columbiabusiness_faculty(url):\n",
    "    # Same as before\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for profile in soup.find_all('div', class_='views-row'):\n",
    "            name_tag = profile.find('span', class_='field-content')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_bc_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h3', class_='person-card__name'):\n",
    "            name = person.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_msu_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for person in soup.find_all('h3', class_='staff__name'):\n",
    "            name = person.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_cornell_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"div\", class_=\"views-field-title\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_wisconsin_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for name_tag in soup.find_all(\"h3\", class_=\"person-name\"):\n",
    "            name = name_tag.get_text(strip=True)\n",
    "            if is_valid_name(name):\n",
    "                faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_duke_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for row in soup.find_all(\"div\", class_=\"views-row\"):\n",
    "            name_tag = row.find(\"h4\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_ucdavis_faculty(url):\n",
    "    # Updated function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for tile in soup.find_all(\"div\", class_=\"tileItem\"):\n",
    "            name_tag = tile.find(\"h2\")\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text(strip=True)\n",
    "                if is_valid_name(name):\n",
    "                    faculty_names.append(name)\n",
    "    return faculty_names\n",
    "\n",
    "def scrape_generic_faculty(url):\n",
    "    # Generic scraping function\n",
    "    response = requests.get(url)\n",
    "    faculty_names = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            text = link.get_text(strip=True)\n",
    "            if is_valid_name(text):\n",
    "                faculty_names.append(text)\n",
    "    return faculty_names\n",
    "\n",
    "# Mapping URLs to their respective scraping functions\n",
    "scraping_functions = {\n",
    "    \"https://economics.harvard.edu/faculty\": scrape_harvard_faculty,\n",
    "    \"https://economics.mit.edu/people/faculty\": scrape_mit_faculty,\n",
    "    \"https://econ.berkeley.edu/people/faculty\": scrape_berkeley_faculty,\n",
    "    \"https://economics.uchicago.edu/people/faculty\": scrape_uchicago_faculty,\n",
    "    \"https://economics.princeton.edu/people/\": scrape_princeton_faculty,\n",
    "    \"https://economics.stanford.edu/people/faculty\": scrape_stanford_faculty,\n",
    "    \"https://economics.yale.edu/people\": scrape_yale_faculty,\n",
    "    \"https://econ.columbia.edu/faculty/\": scrape_columbia_faculty,\n",
    "    \"https://as.nyu.edu/departments/econ/faculty.html\": scrape_nyu_faculty,\n",
    "    \"https://economics.sas.upenn.edu/people/faculty\": scrape_penn_faculty,\n",
    "    \"https://economics.brown.edu/people/faculty\": scrape_brown_faculty,\n",
    "    \"https://www.bu.edu/econ/people/faculty/\": scrape_bu_faculty,\n",
    "    \"https://dornsife.usc.edu/econ/faculty/\": scrape_usc_faculty,\n",
    "    \"https://lsa.umich.edu/econ/people/faculty.html\": scrape_michigan_faculty,\n",
    "    \"https://economics.ucsd.edu/faculty-and-research/faculty-profiles/index.html#Faculty\": scrape_ucsd_faculty,\n",
    "    \"https://economics.dartmouth.edu/people\": scrape_dartmouth_faculty,\n",
    "    \"https://economics.northwestern.edu/people/faculty/\": scrape_northwestern_faculty,\n",
    "    \"https://economics.ucla.edu/faculty/ladder\": scrape_ucla_faculty,\n",
    "    \"https://business.columbia.edu/faculty/divisions/finance/faculty\": scrape_columbiabusiness_faculty,\n",
    "    \"https://www.bc.edu/content/bc-web/schools/morrissey/departments/economics/people.html\": scrape_bc_faculty,\n",
    "    \"https://econ.msu.edu/about/directory\": scrape_msu_faculty,\n",
    "    \"https://economics.cornell.edu/faculty\": scrape_cornell_faculty,\n",
    "    \"https://econ.wisc.edu/faculty/\": scrape_wisconsin_faculty,\n",
    "    \"https://econ.duke.edu/people/other-faculty/regular-rank-faculty\": scrape_duke_faculty,\n",
    "    \"https://economics.ucdavis.edu/people/faculty\": scrape_ucdavis_faculty\n",
    "}\n",
    "\n",
    "# Collect all faculty names in a list\n",
    "all_faculty = []\n",
    "scraping_results = []\n",
    "\n",
    "for url in faculty_urls:\n",
    "    university_match = re.findall(r'https?://(?:www\\.)?([^/]+)/', url)\n",
    "    university_name = university_match[0].replace('www.', '') if university_match else 'Unknown'\n",
    "    print(f\"\\nScraping faculty from: {url}\")\n",
    "    try:\n",
    "        if url in scraping_functions:\n",
    "            scrape_function = scraping_functions[url]\n",
    "        else:\n",
    "            scrape_function = scrape_generic_faculty\n",
    "            print(\"Using generic scraping function.\")\n",
    "        faculty_names = scrape_function(url)\n",
    "        num_names = len(faculty_names)\n",
    "        print(f\"Successfully scraped {num_names} faculty names from {university_name}.\")\n",
    "        if num_names == 0:\n",
    "            print(f\"Warning: No faculty names found for {university_name}.\")\n",
    "            scraping_results.append({'University': university_name, 'URL': url, 'Status': 'No names found'})\n",
    "        else:\n",
    "            scraping_results.append({'University': university_name, 'URL': url, 'Status': f'{num_names} names found'})\n",
    "            for name in faculty_names:\n",
    "                all_faculty.append({'University': university_name, 'Faculty Name': name})\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {university_name}: {e}\")\n",
    "        scraping_results.append({'University': university_name, 'URL': url, 'Status': f'Error: {e}'})\n",
    "    time.sleep(1)  # Be polite to the servers\n",
    "\n",
    "# Create a DataFrame with all faculty names\n",
    "df_all_faculty = pd.DataFrame(all_faculty)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nAll Faculty Names Collected:\")\n",
    "print(df_all_faculty)\n",
    "\n",
    "# Display a summary report\n",
    "print(\"\\nScraping Summary Report:\")\n",
    "df_scraping_results = pd.DataFrame(scraping_results)\n",
    "print(df_scraping_results)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "# df_all_faculty.to_csv('faculty_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
